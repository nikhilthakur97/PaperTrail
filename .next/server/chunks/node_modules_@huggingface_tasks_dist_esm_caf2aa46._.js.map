{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 4, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/library-to-tasks.js"],"sourcesContent":["/**\n * Mapping from library name to its supported tasks.\n * HF-Inference API (serverless) should be disabled for all other (library, task) pairs beyond this mapping.\n * This mapping is partially generated automatically by \"python-api-export-tasks\" action in\n * huggingface/api-inference-community repo upon merge. For transformers, the mapping is manually\n * based on api-inference (hf_types.rs).\n */\nexport const LIBRARY_TASK_MAPPING = {\n    \"adapter-transformers\": [\"question-answering\", \"text-classification\", \"token-classification\"],\n    allennlp: [\"question-answering\"],\n    asteroid: [\n        // \"audio-source-separation\",\n        \"audio-to-audio\",\n    ],\n    bertopic: [\"text-classification\"],\n    diffusers: [\"image-to-image\", \"text-to-image\"],\n    doctr: [\"object-detection\"],\n    espnet: [\"text-to-speech\", \"automatic-speech-recognition\"],\n    fairseq: [\"text-to-speech\", \"audio-to-audio\"],\n    fastai: [\"image-classification\"],\n    fasttext: [\"feature-extraction\", \"text-classification\"],\n    flair: [\"token-classification\"],\n    k2: [\"automatic-speech-recognition\"],\n    keras: [\"image-classification\"],\n    nemo: [\"automatic-speech-recognition\"],\n    open_clip: [\"zero-shot-classification\", \"zero-shot-image-classification\"],\n    paddlenlp: [\"fill-mask\", \"summarization\", \"zero-shot-classification\"],\n    peft: [\"text-generation\"],\n    \"pyannote-audio\": [\"automatic-speech-recognition\"],\n    \"sentence-transformers\": [\"feature-extraction\", \"sentence-similarity\"],\n    setfit: [\"text-classification\"],\n    sklearn: [\"tabular-classification\", \"tabular-regression\", \"text-classification\"],\n    spacy: [\"token-classification\", \"text-classification\", \"sentence-similarity\"],\n    \"span-marker\": [\"token-classification\"],\n    speechbrain: [\"audio-classification\", \"audio-to-audio\", \"automatic-speech-recognition\", \"text-to-speech\"],\n    stanza: [\"token-classification\"],\n    timm: [\"image-classification\", \"image-feature-extraction\"],\n    transformers: [\n        \"audio-classification\",\n        \"automatic-speech-recognition\",\n        \"depth-estimation\",\n        \"document-question-answering\",\n        \"feature-extraction\",\n        \"fill-mask\",\n        \"image-classification\",\n        \"image-feature-extraction\",\n        \"image-segmentation\",\n        \"image-to-image\",\n        \"image-to-text\",\n        \"image-text-to-text\",\n        \"mask-generation\",\n        \"object-detection\",\n        \"question-answering\",\n        \"summarization\",\n        \"table-question-answering\",\n        \"text-classification\",\n        \"text-generation\",\n        \"text-to-audio\",\n        \"text-to-speech\",\n        \"token-classification\",\n        \"translation\",\n        \"video-classification\",\n        \"visual-question-answering\",\n        \"zero-shot-classification\",\n        \"zero-shot-image-classification\",\n        \"zero-shot-object-detection\",\n    ],\n    mindspore: [\"image-classification\"],\n};\n"],"names":[],"mappings":"AAAA;;;;;;CAMC;;;;AACM,MAAM,uBAAuB;IAChC,wBAAwB;QAAC;QAAsB;QAAuB;KAAuB;IAC7F,UAAU;QAAC;KAAqB;IAChC,UAAU;QACN,6BAA6B;QAC7B;KACH;IACD,UAAU;QAAC;KAAsB;IACjC,WAAW;QAAC;QAAkB;KAAgB;IAC9C,OAAO;QAAC;KAAmB;IAC3B,QAAQ;QAAC;QAAkB;KAA+B;IAC1D,SAAS;QAAC;QAAkB;KAAiB;IAC7C,QAAQ;QAAC;KAAuB;IAChC,UAAU;QAAC;QAAsB;KAAsB;IACvD,OAAO;QAAC;KAAuB;IAC/B,IAAI;QAAC;KAA+B;IACpC,OAAO;QAAC;KAAuB;IAC/B,MAAM;QAAC;KAA+B;IACtC,WAAW;QAAC;QAA4B;KAAiC;IACzE,WAAW;QAAC;QAAa;QAAiB;KAA2B;IACrE,MAAM;QAAC;KAAkB;IACzB,kBAAkB;QAAC;KAA+B;IAClD,yBAAyB;QAAC;QAAsB;KAAsB;IACtE,QAAQ;QAAC;KAAsB;IAC/B,SAAS;QAAC;QAA0B;QAAsB;KAAsB;IAChF,OAAO;QAAC;QAAwB;QAAuB;KAAsB;IAC7E,eAAe;QAAC;KAAuB;IACvC,aAAa;QAAC;QAAwB;QAAkB;QAAgC;KAAiB;IACzG,QAAQ;QAAC;KAAuB;IAChC,MAAM;QAAC;QAAwB;KAA2B;IAC1D,cAAc;QACV;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;KACH;IACD,WAAW;QAAC;KAAuB;AACvC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 150, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/default-widget-inputs.js"],"sourcesContent":["/// NOTE TO CONTRIBUTORS:\n///\n/// When adding sample inputs for a new language, you don't\n/// necessarily have to translate the inputs from existing languages.\n/// (which were quite random to begin with)\n///\n/// i.e. Feel free to be creative and provide better samples.\n//\n/// The <mask> placeholder will be replaced by the correct mask token\n/// in the following examples, depending on the model type\n///\n/// see [INTERNAL] github.com/huggingface/moon-landing/blob/c5c3d45fe0ab27347b3ab27bdad646ef20732351/server/lib/App.ts#L254\n//\nconst MAPPING_EN = new Map([\n    [\"text-classification\", [`I like you. I love you`]],\n    [\n        \"token-classification\",\n        [\n            `My name is Wolfgang and I live in Berlin`,\n            `My name is Sarah and I live in London`,\n            `My name is Clara and I live in Berkeley, California.`,\n        ],\n    ],\n    [\n        \"table-question-answering\",\n        [\n            {\n                text: `How many stars does the transformers repository have?`,\n                table: {\n                    Repository: [\"Transformers\", \"Datasets\", \"Tokenizers\"],\n                    Stars: [36542, 4512, 3934],\n                    Contributors: [651, 77, 34],\n                    \"Programming language\": [\"Python\", \"Python\", \"Rust, Python and NodeJS\"],\n                },\n            },\n        ],\n    ],\n    [\n        \"question-answering\",\n        [\n            {\n                text: `Where do I live?`,\n                context: `My name is Wolfgang and I live in Berlin`,\n            },\n            {\n                text: `Where do I live?`,\n                context: `My name is Sarah and I live in London`,\n            },\n            {\n                text: `What's my name?`,\n                context: `My name is Clara and I live in Berkeley.`,\n            },\n            {\n                text: `Which name is also used to describe the Amazon rainforest in English?`,\n                context: `The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.`,\n            },\n        ],\n    ],\n    [\n        \"zero-shot-classification\",\n        [\n            {\n                text: \"I have a problem with my iphone that needs to be resolved asap!\",\n                candidate_labels: \"urgent, not urgent, phone, tablet, computer\",\n                multi_class: true,\n            },\n            {\n                text: \"Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.\",\n                candidate_labels: \"mobile, website, billing, account access\",\n                multi_class: false,\n            },\n            {\n                text: \"A new model offers an explanation for how the Galilean satellites formed around the solar system’s largest world. Konstantin Batygin did not set out to solve one of the solar system’s most puzzling mysteries when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his contributions to the search for the solar system’s missing “Planet Nine,” spotted a beer bottle. At a steep, 20 degree grade, he wondered why it wasn’t rolling down the hill. He realized there was a breeze at his back holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical astrophysicist: “Oh! This is how Europa formed.” Europa is one of Jupiter’s four large Galilean moons. And in a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a planetary scientist at the Côte d’Azur Observatory in France, present a theory explaining how some moons form around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the solar system’s formation became trapped around these massive worlds, taking shape one at a time into the potentially habitable moons we know today.\",\n                candidate_labels: \"space & cosmos, scientific discovery, microbiology, robots, archeology\",\n                multi_class: true,\n            },\n        ],\n    ],\n    [\"translation\", [`My name is Wolfgang and I live in Berlin`, `My name is Sarah and I live in London`]],\n    [\n        \"summarization\",\n        [\n            `The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.`,\n        ],\n    ],\n    [\n        \"conversational\",\n        [\n            `Hi, what can you help me with?`,\n            `What is 84 * 3 / 2?`,\n            `Tell me an interesting fact about the universe!`,\n            `Explain quantum computing in simple terms.`,\n        ],\n    ],\n    [\n        \"text-generation\",\n        [\n            `My name is Julien and I like to`,\n            `I like traveling by train because`,\n            `Paris is an amazing place to visit,`,\n            `Once upon a time,`,\n        ],\n    ],\n    [\"fill-mask\", [`Paris is the <mask> of France.`, `The goal of life is <mask>.`]],\n    [\n        \"sentence-similarity\",\n        [\n            {\n                source_sentence: \"That is a happy person\",\n                sentences: [\"That is a happy dog\", \"That is a very happy person\", \"Today is a sunny day\"],\n            },\n        ],\n    ],\n]);\nconst MAPPING_ZH = new Map([\n    [\"text-classification\", [`我喜欢你。 我爱你`]],\n    [\"token-classification\", [`我叫沃尔夫冈，我住在柏林。`, `我叫萨拉，我住在伦敦。`, `我叫克拉拉，我住在加州伯克利。`]],\n    [\n        \"question-answering\",\n        [\n            {\n                text: `我住在哪里？`,\n                context: `我叫沃尔夫冈，我住在柏林。`,\n            },\n            {\n                text: `我住在哪里？`,\n                context: `我叫萨拉，我住在伦敦。`,\n            },\n            {\n                text: `我的名字是什么？`,\n                context: `我叫克拉拉，我住在伯克利。`,\n            },\n        ],\n    ],\n    [\"translation\", [`我叫沃尔夫冈，我住在柏林。`, `我叫萨拉，我住在伦敦。`]],\n    [\n        \"zero-shot-classification\",\n        [\n            {\n                text: \"房间干净明亮，非常不错\",\n                candidate_labels: \"这是一条差评, 这是一条好评\",\n            },\n        ],\n    ],\n    [\n        \"summarization\",\n        [\n            `该塔高324米（1063英尺），与一幢81层的建筑物一样高，是巴黎最高的建筑物。 它的底座是方形的，每边长125米（410英尺）。 在建造过程中，艾菲尔铁塔超过了华盛顿纪念碑，成为世界上最高的人造结构，它保持了41年的头衔，直到1930年纽约市的克莱斯勒大楼竣工。这是第一个到达300米高度的结构。 由于1957年在塔顶增加了广播天线，因此它现在比克莱斯勒大厦高5.2米（17英尺）。 除发射器外，艾菲尔铁塔是法国第二高的独立式建筑，仅次于米劳高架桥。`,\n        ],\n    ],\n    [\n        \"text-generation\",\n        [`我叫朱利安，我喜欢`, `我叫托马斯，我的主要`, `我叫玛丽亚，我最喜欢的`, `我叫克拉拉，我是`, `从前，`],\n    ],\n    [\"fill-mask\", [`巴黎是<mask>国的首都。`, `生活的真谛是<mask>。`]],\n    [\n        \"sentence-similarity\",\n        [\n            {\n                source_sentence: \"那是 個快樂的人\",\n                sentences: [\"那是 條快樂的狗\", \"那是 個非常幸福的人\", \"今天是晴天\"],\n            },\n        ],\n    ],\n]);\nconst MAPPING_FR = new Map([\n    [\"text-classification\", [`Je t'apprécie beaucoup. Je t'aime.`]],\n    [\"token-classification\", [`Mon nom est Wolfgang et je vis à Berlin`]],\n    [\n        \"question-answering\",\n        [\n            {\n                text: `Où est-ce que je vis?`,\n                context: `Mon nom est Wolfgang et je vis à Berlin`,\n            },\n        ],\n    ],\n    [\"translation\", [`Mon nom est Wolfgang et je vis à Berlin`]],\n    [\n        \"summarization\",\n        [\n            `La tour fait 324 mètres (1,063 pieds) de haut, environ la même hauteur qu'un immeuble de 81 étages, et est la plus haute structure de Paris. Sa base est carrée, mesurant 125 mètres (410 pieds) sur chaque côté. Durant sa construction, la tour Eiffel surpassa le Washington Monument pour devenir la plus haute structure construite par l'homme dans le monde, un titre qu'elle conserva pendant 41 ans jusqu'à l'achèvement du Chrysler Building à New-York City en 1930. Ce fut la première structure à atteindre une hauteur de 300 mètres. Avec l'ajout d'une antenne de radiodiffusion au sommet de la tour Eiffel en 1957, celle-ci redevint plus haute que le Chrysler Building de 5,2 mètres (17 pieds). En excluant les transmetteurs, elle est la seconde plus haute structure autoportante de France après le viaduc de Millau.`,\n        ],\n    ],\n    [\"text-generation\", [`Mon nom est Julien et j'aime`, `Mon nom est Thomas et mon principal`, `Il était une fois`]],\n    [\"fill-mask\", [`Paris est la <mask> de la France.`]],\n    [\n        \"sentence-similarity\",\n        [\n            {\n                source_sentence: \"C'est une personne heureuse\",\n                sentences: [\n                    \"C'est un chien heureux\",\n                    \"C'est une personne très heureuse\",\n                    \"Aujourd'hui est une journée ensoleillée\",\n                ],\n            },\n        ],\n    ],\n]);\nconst MAPPING_ES = new Map([\n    [\"text-classification\", [`Te quiero. Te amo.`]],\n    [\"token-classification\", [`Me llamo Wolfgang y vivo en Berlin`]],\n    [\n        \"question-answering\",\n        [\n            {\n                text: `¿Dónde vivo?`,\n                context: `Me llamo Wolfgang y vivo en Berlin`,\n            },\n            {\n                text: `¿Quién inventó el submarino?`,\n                context: `Isaac Peral fue un murciano que inventó el submarino`,\n            },\n            {\n                text: `¿Cuántas personas hablan español?`,\n                context: `El español es el segundo idioma más hablado del mundo con más de 442 millones de hablantes`,\n            },\n        ],\n    ],\n    [\n        \"translation\",\n        [\n            `Me llamo Wolfgang y vivo en Berlin`,\n            `Los ingredientes de una tortilla de patatas son: huevos, patatas y cebolla`,\n        ],\n    ],\n    [\n        \"summarization\",\n        [\n            `La torre tiene 324 metros (1.063 pies) de altura, aproximadamente la misma altura que un edificio de 81 pisos y la estructura más alta de París. Su base es cuadrada, mide 125 metros (410 pies) a cada lado. Durante su construcción, la Torre Eiffel superó al Washington Monument para convertirse en la estructura artificial más alta del mundo, un título que mantuvo durante 41 años hasta que el Chrysler Building en la ciudad de Nueva York se terminó en 1930. Fue la primera estructura en llegar Una altura de 300 metros. Debido a la adición de una antena de transmisión en la parte superior de la torre en 1957, ahora es más alta que el Chrysler Building en 5,2 metros (17 pies). Excluyendo los transmisores, la Torre Eiffel es la segunda estructura independiente más alta de Francia después del Viaducto de Millau.`,\n        ],\n    ],\n    [\n        \"text-generation\",\n        [\n            `Me llamo Julien y me gusta`,\n            `Me llamo Thomas y mi principal`,\n            `Me llamo Manuel y trabajo en`,\n            `Érase una vez,`,\n            `Si tú me dices ven, `,\n        ],\n    ],\n    [\"fill-mask\", [`Mi nombre es <mask> y vivo en Nueva York.`, `El español es un idioma muy <mask> en el mundo.`]],\n    [\n        \"sentence-similarity\",\n        [\n            {\n                source_sentence: \"Esa es una persona feliz\",\n                sentences: [\"Ese es un perro feliz\", \"Esa es una persona muy feliz\", \"Hoy es un día soleado\"],\n            },\n        ],\n    ],\n]);\nconst MAPPING_RU = new Map([\n    [\"text-classification\", [`Ты мне нравишься. Я тебя люблю`]],\n    [\"token-classification\", [`Меня зовут Вольфганг и я живу в Берлине`]],\n    [\n        \"question-answering\",\n        [\n            {\n                text: `Где живу?`,\n                context: `Меня зовут Вольфганг и я живу в Берлине`,\n            },\n        ],\n    ],\n    [\"translation\", [`Меня зовут Вольфганг и я живу в Берлине`]],\n    [\n        \"summarization\",\n        [\n            `Высота башни составляет 324 метра (1063 фута), примерно такая же высота, как у 81-этажного здания, и самое высокое сооружение в Париже. Его основание квадратно, размером 125 метров (410 футов) с любой стороны. Во время строительства Эйфелева башня превзошла монумент Вашингтона, став самым высоким искусственным сооружением в мире, и этот титул она удерживала в течение 41 года до завершения строительство здания Крайслер в Нью-Йорке в 1930 году. Это первое сооружение которое достигло высоты 300 метров. Из-за добавления вещательной антенны на вершине башни в 1957 году она сейчас выше здания Крайслер на 5,2 метра (17 футов). За исключением передатчиков, Эйфелева башня является второй самой высокой отдельно стоящей структурой во Франции после виадука Мийо.`,\n        ],\n    ],\n    [\"text-generation\", [`Меня зовут Жюльен и`, `Меня зовут Томас и мой основной`, `Однажды`]],\n    [\"fill-mask\", [`Меня зовут <mask> и я инженер живущий в Нью-Йорке.`]],\n    [\n        \"sentence-similarity\",\n        [\n            {\n                source_sentence: \"Это счастливый человек\",\n                sentences: [\"Это счастливая собака\", \"Это очень счастливый человек\", \"Сегодня солнечный день\"],\n            },\n        ],\n    ],\n]);\nconst MAPPING_UK = new Map([\n    [\"translation\", [`Мене звати Вольфґанґ і я живу в Берліні.`]],\n    [\"fill-mask\", [`Мене звати <mask>.`]],\n]);\nconst MAPPING_IT = new Map([\n    [\"text-classification\", [`Mi piaci. Ti amo`]],\n    [\n        \"token-classification\",\n        [\n            `Mi chiamo Wolfgang e vivo a Berlino`,\n            `Mi chiamo Sarah e vivo a Londra`,\n            `Mi chiamo Clara e vivo a Berkeley in California.`,\n        ],\n    ],\n    [\n        \"question-answering\",\n        [\n            {\n                text: `Dove vivo?`,\n                context: `Mi chiamo Wolfgang e vivo a Berlino`,\n            },\n            {\n                text: `Dove vivo?`,\n                context: `Mi chiamo Sarah e vivo a Londra`,\n            },\n            {\n                text: `Come mio chiamo?`,\n                context: `Mi chiamo Clara e vivo a Berkeley.`,\n            },\n        ],\n    ],\n    [\"translation\", [`Mi chiamo Wolfgang e vivo a Berlino`, `Mi chiamo Sarah e vivo a Londra`]],\n    [\n        \"summarization\",\n        [\n            `La torre degli Asinelli è una delle cosiddette due torri di Bologna, simbolo della città, situate in piazza di porta Ravegnana, all'incrocio tra le antiche strade San Donato (ora via Zamboni), San Vitale, Maggiore e Castiglione. Eretta, secondo la tradizione, fra il 1109 e il 1119 dal nobile Gherardo Asinelli, la torre è alta 97,20 metri, pende verso ovest per 2,23 metri e presenta all'interno una scalinata composta da 498 gradini. Ancora non si può dire con certezza quando e da chi fu costruita la torre degli Asinelli. Si presume che la torre debba il proprio nome a Gherardo Asinelli, il nobile cavaliere di fazione ghibellina al quale se ne attribuisce la costruzione, iniziata secondo una consolidata tradizione l'11 ottobre 1109 e terminata dieci anni dopo, nel 1119.`,\n        ],\n    ],\n    [\n        \"text-generation\",\n        [\n            `Mi chiamo Loreto e mi piace`,\n            `Mi chiamo Thomas e il mio principale`,\n            `Mi chiamo Marianna, la mia cosa preferita`,\n            `Mi chiamo Clara e sono`,\n            `C'era una volta`,\n        ],\n    ],\n    [\"fill-mask\", [`Roma è la <mask> d'Italia.`, `Lo scopo della vita è <mask>.`]],\n    [\n        \"sentence-similarity\",\n        [\n            {\n                source_sentence: \"Questa è una persona felice\",\n                sentences: [\"Questo è un cane felice\", \"Questa è una persona molto felice\", \"Oggi è una giornata di sole\"],\n            },\n        ],\n    ],\n]);\nconst MAPPING_FA = new Map([\n    [\n        \"text-classification\",\n        [`پروژه به موقع تحویل شد و همه چیز خوب بود.`, `سیب‌زمینی بی‌کیفیت بود.`, `قیمت و کیفیت عالی`, `خوب نبود اصلا`],\n    ],\n    [\n        \"token-classification\",\n        [\n            `این سریال به صورت رسمی در تاریخ دهم می ۲۰۱۱ توسط شبکه فاکس برای پخش رزرو شد.`,\n            `دفتر مرکزی شرکت پارس‌مینو در شهر اراک در استان مرکزی قرار دارد.`,\n            `وی در سال ۲۰۱۳ درگذشت و مسئول خاکسپاری و اقوامش برای او مراسم یادبود گرفتند.`,\n        ],\n    ],\n    [\n        \"question-answering\",\n        [\n            {\n                text: `من کجا زندگی میکنم؟`,\n                context: `نام من پژمان است و در گرگان زندگی میکنم.`,\n            },\n            {\n                text: `نامم چیست و کجا زندگی می‌کنم؟`,\n                context: `اسمم سارا است و در آفریقای جنوبی زندگی میکنم.`,\n            },\n            {\n                text: `نام من چیست؟`,\n                context: `من مریم هستم و در تبریز زندگی می‌کنم.`,\n            },\n            {\n                text: `بیشترین مساحت جنگل آمازون در کدام کشور است؟`,\n                context: [\n                    \"آمازون نام بزرگ‌ترین جنگل بارانی جهان است که در شمال آمریکای جنوبی قرار گرفته و بیشتر آن در خاک برزیل و پرو\",\n                    \"جای دارد. بیش از نیمی از همه جنگل‌های بارانی باقی‌مانده در جهان در آمازون قرار دارد.\",\n                    \"مساحت جنگل‌های آمازون ۵٫۵ میلیون کیلومتر مربع است که بین ۹ کشور تقسیم شده‌است.\",\n                ].join(\"\\n\"),\n            },\n        ],\n    ],\n    [\n        \"translation\",\n        [\n            \"بیشتر مساحت جنگل‌های آمازون در حوضه آبریز رود آمازون و ۱۱۰۰ شاخه آن واقع شده‌است.\",\n            \"مردمان نَبَطی از هزاره‌های یکم و دوم پیش از میلاد در این منطقه زندگی می‌کردند.\",\n        ],\n    ],\n    [\n        \"summarization\",\n        [\n            [\n                \"شاهنامه اثر حکیم ابوالقاسم فردوسی توسی، حماسه‌ای منظوم، بر حسب دست نوشته‌های \",\n                \"موجود دربرگیرنده نزدیک به ۵۰٬۰۰۰ بیت تا نزدیک به ۶۱٬۰۰۰ بیت و یکی از \",\n                \"بزرگ‌ترین و برجسته‌ترین سروده‌های حماسی جهان است که سرایش آن دست‌آوردِ \",\n                \"دست‌کم سی سال کارِ پیوستهٔ این سخن‌سرای نامدار ایرانی است. موضوع این شاهکار ادبی،\",\n                \" افسانه‌ها و تاریخ ایران از آغاز تا حملهٔ عرب‌ها به ایران در سدهٔ هفتم میلادی است\",\n                \"  (شاهنامه از سه بخش اسطوره، پهلوانی و تاریخی تشکیل شده‌است) که در چهار\",\n                \"   دودمان پادشاهیِ پیشدادیان، کیانیان، اشکانیان و ساسانیان گنجانده می‌شود.\",\n                \"    شاهنامه بر وزن «فَعولُن فعولن فعولن فَعَلْ»، در بحرِ مُتَقارِبِ مثمَّنِ محذوف نگاشته شده‌است.\",\n                \"هنگامی که زبان دانش و ادبیات در ایران زبان عربی بود، فردوسی، با سرودن شاهنامه\",\n                \" با ویژگی‌های هدف‌مندی که داشت، زبان پارسی را زنده و پایدار کرد. یکی از \",\n                \" بن‌مایه‌های مهمی که فردوسی برای سرودن شاهنامه از آن استفاده کرد،\",\n                \"  شاهنامهٔ ابومنصوری بود. شاهنامه نفوذ بسیاری در جهت‌گیری \",\n                \"  فرهنگ فارسی و نیز بازتاب‌های شکوه‌مندی در ادبیات جهان داشته‌است و شاعران \",\n                \"  بزرگی مانند گوته و ویکتور هوگو از آن به نیکی یاد کرده‌اند.\",\n            ].join(\"\\n\"),\n        ],\n    ],\n    [\"text-generation\", [\"اسم من نازنین است و من\", \"روزی روزگاری\"]],\n    [\n        \"fill-mask\",\n        [\n            `زندگی یک سوال است و این که چگونه <mask> کنیم پاسخ این سوال!`,\n            `زندگی از مرگ پرسید: چرا همه من را <mask> دارند اما از تو متنفرند؟`,\n        ],\n    ],\n]);\nconst MAPPING_AR = new Map([\n    [\"text-classification\", [`أحبك. أهواك`]],\n    [\n        \"token-classification\",\n        [`إسمي محمد وأسكن في برلين`, `إسمي ساره وأسكن في لندن`, `إسمي سامي وأسكن في القدس في فلسطين.`],\n    ],\n    [\n        \"question-answering\",\n        [\n            {\n                text: `أين أسكن؟`,\n                context: `إسمي محمد وأسكن في بيروت`,\n            },\n            {\n                text: `أين أسكن؟`,\n                context: `إسمي ساره وأسكن في لندن`,\n            },\n            {\n                text: `ما اسمي؟`,\n                context: `اسمي سعيد وأسكن في حيفا.`,\n            },\n            {\n                text: `ما لقب خالد بن الوليد بالعربية؟`,\n                context: `خالد بن الوليد من أبطال وقادة الفتح الإسلامي وقد تحدثت عنه اللغات الإنجليزية والفرنسية والإسبانية ولقب بسيف الله المسلول.`,\n            },\n        ],\n    ],\n    [\"translation\", [`إسمي محمد وأسكن في برلين`, `إسمي ساره وأسكن في لندن`]],\n    [\n        \"summarization\",\n        [\n            `تقع الأهرامات في الجيزة قرب القاهرة في مصر وقد بنيت منذ عدة قرون، وقيل إنها كانت قبورا للفراعنة وتم بناؤها بعملية هندسية رائعة واستقدمت حجارتها من جبل المقطم وتم نقلها بالسفن أو على الرمل، وما تزال شامخة ويقصدها السياح من كافة أرجاء المعمورة.`,\n        ],\n    ],\n    [\n        \"text-generation\",\n        [\n            `إسمي محمد وأحب أن`,\n            `دع المكارم لا ترحل لبغيتها - واقعد فإنك أنت الطاعم الكاسي.`,\n            `لماذا نحن هنا؟`,\n            `القدس مدينة تاريخية، بناها الكنعانيون في`,\n            `كان يا ما كان في قديم الزمان`,\n        ],\n    ],\n    [\"fill-mask\", [`باريس <mask> فرنسا.`, `فلسفة الحياة هي <mask>.`]],\n    [\n        \"sentence-similarity\",\n        [\n            {\n                source_sentence: \"هذا شخص سعيد\",\n                sentences: [\"هذا كلب سعيد\", \"هذا شخص سعيد جدا\", \"اليوم هو يوم مشمس\"],\n            },\n        ],\n    ],\n]);\nconst MAPPING_BN = new Map([\n    [\"text-classification\", [`বাঙালির ঘরে ঘরে আজ নবান্ন উৎসব।`]],\n    [\n        \"token-classification\",\n        [`আমার নাম জাহিদ এবং আমি ঢাকায় বাস করি।`, `তিনি গুগলে চাকরী করেন।`, `আমার নাম সুস্মিতা এবং আমি কলকাতায় বাস করি।`],\n    ],\n    [\"translation\", [`আমার নাম জাহিদ, আমি রংপুরে বাস করি।`, `আপনি কী আজকে বাসায় আসবেন?`]],\n    [\n        \"summarization\",\n        [\n            `‘ইকোনমিস্ট’ লিখেছে, অ্যান্টিবডির চার মাস স্থায়ী হওয়ার খবরটি দুই কারণে আনন্দের। অ্যান্টিবডি যত দিন পর্যন্ত শরীরে টিকবে, তত দিন সংক্রমণ থেকে সুরক্ষিত থাকা সম্ভব। অর্থাৎ, এমন এক টিকার প্রয়োজন হবে, যা অ্যান্টিবডির উত্পাদনকে প্ররোচিত করতে পারে এবং দীর্ঘস্থায়ী সুরক্ষা দিতে পারে। এগুলো খুঁজে বের করাও সহজ। এটি আভাস দেয়, ব্যাপক হারে অ্যান্টিবডি শনাক্তকরণ ফলাফল মোটামুটি নির্ভুল হওয়া উচিত। দ্বিতীয় আরেকটি গবেষণার নেতৃত্ব দিয়েছেন যুক্তরাজ্যের মেডিকেল রিসার্চ কাউন্সিলের (এমআরসি) ইমিউনোলজিস্ট তাও দং। তিনি টি-সেল শনাক্তকরণে কাজ করেছেন। টি-সেল শনাক্তকরণের প্রক্রিয়া অবশ্য অ্যান্টিবডির মতো এত আলোচিত নয়। তবে সংক্রমণের বিরুদ্ধে লড়াই এবং দীর্ঘমেয়াদি সুরক্ষায় সমান গুরুত্বপূর্ণ ভূমিকা পালন করে। গবেষণাসংক্রান্ত নিবন্ধ প্রকাশিত হয়েছে ‘নেচার ইমিউনোলজি’ সাময়িকীতে। তাঁরা বলছেন, গবেষণার ক্ষেত্রে কোভিড-১৯ মৃদু সংক্রমণের শিকার ২৮ ব্যক্তির রক্তের নমুনা, ১৪ জন গুরুতর অসুস্থ ও ১৬ জন সুস্থ ব্যক্তির রক্তের নমুনা পরীক্ষা করেছেন। গবেষণা নিবন্ধে বলা হয়, সংক্রমিত ব্যক্তিদের ক্ষেত্রে টি-সেলের তীব্র প্রতিক্রিয়া তাঁরা দেখেছেন। এ ক্ষেত্রে মৃদু ও গুরুতর অসুস্থ ব্যক্তিদের ক্ষেত্রে প্রতিক্রিয়ার ভিন্নতা পাওয়া গেছে।`,\n        ],\n    ],\n    [\"text-generation\", [`আমি রতন এবং আমি`, `তুমি যদি চাও তবে`, `মিথিলা আজকে বড্ড`]],\n    [\"fill-mask\", [`আমি বাংলায় <mask> গাই।`, `আমি <mask> খুব ভালোবাসি। `]],\n    [\n        \"question-answering\",\n        [\n            {\n                text: `প্রথম এশিয়া কাপ ক্রিকেট টুর্নামেন্ট কোথায় অনুষ্ঠিত হয় ?`,\n                context: `প্রথম টুর্নামেন্ট অনুষ্ঠিত হয় ১৯৮৪ সালে সংযুক্ত আরব আমিরাত এর শারজাহ তে যেখানে কাউন্সিলের মূল অফিস ছিল (১৯৯৫ পর্যন্ত)। ভারত শ্রীলঙ্কার সাথে আন্তরিকতাহীন ক্রিকেট সম্পর্কের কারণে ১৯৮৬ সালের টুর্নামেন্ট বর্জন করে। ১৯৯৩ সালে ভারত ও পাকিস্তান এর মধ্যে রাজনৈতিক অস্থিরতার কারণে এটি বাতিল হয়ে যায়। শ্রীলঙ্কা এশিয়া কাপ শুরু থেকে অংশ গ্রহণ করে আসছে। আন্তর্জাতিক ক্রিকেট কাউন্সিল নিয়ম করে দিয়েছে যে এশিয়া কাপের সকল খেলা অনুষ্ঠিত হবে অফিসিয়াল একদিনের আন্তর্জাতিক ক্রিকেট হিসেবে। এসিসি ঘোষনা অনুযায়ী প্রতি দুই বছর পর পর টুর্নামেন্ট অনুষ্ঠিত হয় ২০০৮ সাল থেকে।`,\n            },\n            {\n                text: `ভারতীয় বাঙালি কথাসাহিত্যিক মহাশ্বেতা দেবীর মৃত্যু কবে হয় ?`,\n                context: `২০১৬ সালের ২৩ জুলাই হৃদরোগে আক্রান্ত হয়ে মহাশ্বেতা দেবী কলকাতার বেল ভিউ ক্লিনিকে ভর্তি হন। সেই বছরই ২৮ জুলাই একাধিক অঙ্গ বিকল হয়ে তাঁর মৃত্যু ঘটে। তিনি মধুমেহ, সেপ্টিসেমিয়া ও মূত্র সংক্রমণ রোগেও ভুগছিলেন।`,\n            },\n            {\n                text: `মাস্টারদা সূর্যকুমার সেনের বাবার নাম কী ছিল ?`,\n                context: `সূর্য সেন ১৮৯৪ সালের ২২ মার্চ চট্টগ্রামের রাউজান থানার নোয়াপাড়ায় অর্থনৈতিক ভাবে অস্বচ্ছল পরিবারে জন্মগ্রহণ করেন। তাঁর পিতার নাম রাজমনি সেন এবং মাতার নাম শশী বালা সেন। রাজমনি সেনের দুই ছেলে আর চার মেয়ে। সূর্য সেন তাঁদের পরিবারের চতুর্থ সন্তান। দুই ছেলের নাম সূর্য ও কমল। চার মেয়ের নাম বরদাসুন্দরী, সাবিত্রী, ভানুমতী ও প্রমিলা। শৈশবে পিতা মাতাকে হারানো সূর্য সেন কাকা গৌরমনি সেনের কাছে মানুষ হয়েছেন। সূর্য সেন ছেলেবেলা থেকেই খুব মনোযোগী ভাল ছাত্র ছিলেন এবং ধর্মভাবাপন্ন গম্ভীর প্রকৃতির ছিলেন।`,\n            },\n        ],\n    ],\n    [\n        \"sentence-similarity\",\n        [\n            {\n                source_sentence: \"সে একজন সুখী ব্যক্তি\",\n                sentences: [\"সে হ্যাপি কুকুর\", \"সে খুব সুখী মানুষ\", \"আজ একটি রৌদ্রোজ্জ্বল দিন\"],\n            },\n        ],\n    ],\n]);\nconst MAPPING_MN = new Map([\n    [\"text-classification\", [`Би чамд хайртай`]],\n    [\n        \"token-classification\",\n        [\n            `Намайг Дорж гэдэг. Би Улаанбаатарт амьдардаг.`,\n            `Намайг Ганбат гэдэг. Би Увс аймагт төрсөн.`,\n            `Манай улс таван хошуу малтай.`,\n        ],\n    ],\n    [\n        \"question-answering\",\n        [\n            {\n                text: `Та хаана амьдардаг вэ?`,\n                context: `Намайг Дорж гэдэг. Би Улаанбаатарт амьдардаг.`,\n            },\n            {\n                text: `Таныг хэн гэдэг вэ?`,\n                context: `Намайг Дорж гэдэг. Би Улаанбаатарт амьдардаг.`,\n            },\n            {\n                text: `Миний нэрийг хэн гэдэг вэ?`,\n                context: `Намайг Ганбат гэдэг. Би Увс аймагт төрсөн.`,\n            },\n        ],\n    ],\n    [\"translation\", [`Намайг Дорж гэдэг. Би Улаанбаатарт амьдардаг.`, `Намайг Ганбат гэдэг. Би Увс аймагт төрсөн.`]],\n    [\n        \"summarization\",\n        [\n            `Монгол Улс (1992 оноос хойш) — дорно болон төв Азид оршдог бүрэн эрхт улс. Хойд талаараа Орос, бусад талаараа Хятад улстай хиллэдэг далайд гарцгүй орон. Нийслэл — Улаанбаатар хот. Алтайн нуруунаас Хянган, Соёноос Говь хүрсэн 1 сая 566 мянган км2 уудам нутагтай, дэлхийд нутаг дэвсгэрийн хэмжээгээр 19-рт жагсдаг. 2015 оны эхэнд Монгол Улсын хүн ам 3 сая хүрсэн (135-р олон). Үндсэндээ монгол үндэстэн (95 хувь), мөн хасаг, тува хүн байна. 16-р зуунаас хойш буддын шашин, 20-р зуунаас шашингүй байдал дэлгэрсэн ба албан хэрэгт монгол хэлээр харилцана.`,\n        ],\n    ],\n    [\n        \"text-generation\",\n        [`Намайг Дорж гэдэг. Би`, `Хамгийн сайн дуучин бол`, `Миний дуртай хамтлаг бол`, `Эрт урьдын цагт`],\n    ],\n    [\"fill-mask\", [`Монгол улсын <mask> Улаанбаатар хотоос ярьж байна.`, `Миний амьдралын зорилго бол <mask>.`]],\n    [\n        \"automatic-speech-recognition\",\n        [\n            {\n                label: `Common Voice Train Example`,\n                src: `https://cdn-media.huggingface.co/common_voice/train/common_voice_mn_18577472.wav`,\n            },\n            {\n                label: `Common Voice Test Example`,\n                src: `https://cdn-media.huggingface.co/common_voice/test/common_voice_mn_18577346.wav`,\n            },\n        ],\n    ],\n    [\n        \"text-to-speech\",\n        [\n            `Би Монгол улсын иргэн.`,\n            `Энэхүү жишээ нь цаанаа ямар ч утга агуулаагүй болно`,\n            `Сар шинэдээ сайхан шинэлэж байна уу?`,\n        ],\n    ],\n    [\n        \"sentence-similarity\",\n        [\n            {\n                source_sentence: \"Энэ бол аз жаргалтай хүн юм\",\n                sentences: [\"Энэ бол аз жаргалтай нохой юм\", \"Энэ бол маш их аз жаргалтай хүн юм\", \"Өнөөдөр нарлаг өдөр байна\"],\n            },\n        ],\n    ],\n]);\nconst MAPPING_SI = new Map([\n    [\"translation\", [`සිංහල ඉතා අලංකාර භාෂාවකි.`, `මෙම තාක්ෂණය භාවිතා කරන ඔබට ස්තූතියි.`]],\n    [\"fill-mask\", [`මම ගෙදර <mask>.`, `<mask> ඉගෙනීමට ගියාය.`]],\n]);\nconst MAPPING_DE = new Map([\n    [\n        \"question-answering\",\n        [\n            {\n                text: `Wo wohne ich?`,\n                context: `Mein Name ist Wolfgang und ich lebe in Berlin`,\n            },\n            {\n                text: `Welcher Name wird auch verwendet, um den Amazonas-Regenwald auf Englisch zu beschreiben?`,\n                context: `Der Amazonas-Regenwald, auf Englisch auch als Amazonien oder Amazonas-Dschungel bekannt, ist ein feuchter Laubwald, der den größten Teil des Amazonas-Beckens Südamerikas bedeckt. Dieses Becken umfasst 7.000.000 Quadratkilometer (2.700.000 Quadratmeilen), von denen 5.500.000 Quadratkilometer (2.100.000 Quadratmeilen) vom Regenwald bedeckt sind. Diese Region umfasst Gebiete von neun Nationen. Der größte Teil des Waldes befindet sich in Brasilien mit 60% des Regenwaldes, gefolgt von Peru mit 13%, Kolumbien mit 10% und geringen Mengen in Venezuela, Ecuador, Bolivien, Guyana, Suriname und Französisch-Guayana. Staaten oder Abteilungen in vier Nationen enthalten \"Amazonas\" in ihren Namen. Der Amazonas repräsentiert mehr als die Hälfte der verbleibenden Regenwälder des Planeten und umfasst den größten und artenreichsten tropischen Regenwald der Welt mit geschätzten 390 Milliarden Einzelbäumen, die in 16.000 Arten unterteilt sind.`,\n            },\n        ],\n    ],\n    [\n        \"sentence-similarity\",\n        [\n            {\n                source_sentence: \"Das ist eine glückliche Person\",\n                sentences: [\n                    \"Das ist ein glücklicher Hund\",\n                    \"Das ist eine sehr glückliche Person\",\n                    \"Heute ist ein sonniger Tag\",\n                ],\n            },\n        ],\n    ],\n]);\nconst MAPPING_DV = new Map([\n    [\"text-classification\", [`އަހަރެން ގަޔާވޭ. އަހަރެން ލޯބިވޭ`]],\n    [\n        \"token-classification\",\n        [\n            `އަހަރެންގެ ނަމަކީ އަހުމަދު އަދި އަހަރެން ދިރިއުޅެނީ މާލޭގަ`,\n            `އަހަރެންގެ ނަމަކީ ސާރާ އަދި އަހަރެން ދިރިއުޅެނީ އުތީމުގަ`,\n            `އަހަރެންގެ ނަމަކީ އައިޝާ އަދި އަހަރެން ދިރިއުޅެނީ ފޭދޫ، އައްޑޫގަ`,\n        ],\n    ],\n    [\n        \"question-answering\",\n        [\n            {\n                text: `އަހަރެން ދިރިއުޅެނީ ކޮންތާކު؟`,\n                context: `އަހަރެންގެ ނަމަކީ އަހުމަދު އަދި އަހަރެން ދިރިއުޅެނީ މާލޭގަ`,\n            },\n            {\n                text: `އަހަރެން ދިރިއުޅެނީ ކޮންތާކު؟`,\n                context: `އަހަރެންގެ ނަމަކީ ސާރާ އަދި އަހަރެން ދިރިއުޅެނީ އުތީމުގަ`,\n            },\n            {\n                text: `އަހަރެންގެ ނަމަކީ ކޮބާ؟`,\n                context: `އަހަރެންގެ ނަމަކީ އައިޝާ އަދި އަހަރެން ދިރިއުޅެނީ ފޭދޫގަ`,\n            },\n            {\n                text: `އެމޭޒަން ރެއިންފޮރެސްޓް ސިފަކޮށްދިނުމަށް އިނގިރޭސި ބަހުން ބޭނުންކުރާނީ ކޮންނަމެއް؟`,\n                context: `އެމޭޒަން ރެއިންފޮރެސްޓް (ޕޯޗުޖީޒް: ފްލޮރެސްޓާ އެމަސޮނިކާ ނުވަތަ އެމަސޮނިއާ؛ ސްޕެނިޝް: ސެލްވާ އެމަސޮނިކާ, އެމަސޮނިއާ ނޫނީ އާންމުކޮށް އެމަޒޯނިއާ؛ ފްރެންޗް: ފޮރޭ އެމެޒޮނިއެން؛ ޑަޗް: އެމެޒޯންރޭގެވައުޑް)، އިގިރޭސި ބަހުން ބުނާ އެމެޒޯނިއާ ނުވަތަ ދަ އެމޭޒަން ޖަންގަލް އަކީ, ސައުތު އެމެރިކާގެ އެމޭޒަން ބޭސިން ސަރަހައްދުގެ ބޮޑުބައެއްގައި ހިމެނޭ މޮއިސްޓް ބޮރޯޑްލީފް ފޮރެސްޓެއެކެވެ. އެމޭޒަން ބޭސިން ސަރަހައްދުގެ ބޮޑު މިނަކީ 7 މިލިއަން އަކަ ކިލޯމީޓަރ (2.7 މިލިއަން އަކަ މައިލް(. މީގެ ތެރެއިން 5.5 މިލިއަން އަކަ ކިލޯމީޓަރ (2.1 މިލިއަން އަކަ މައިލް) އަކީ މި ފޮރެސްޓެވެ. މި ސަރަހައްދުގައި 9 ގައުމަކަށް ނިސްބަތްވާ ޓެރިޓަރީ ހިމެނެއެވެ.  60% އާއިއެކެ އެންމެ ބޮޑު ބައެއް ނިސްބަތްވަނީ ބްރެޒިލްއަށެވެ. އޭގެ ފަހުތުން 13% އާއެކު ޕެރޫ އާއި 10% އާއެކު ކޮލަމްބިއާ އަދި ކުޑަ ބައެއް ހިމެނޭ ގޮތުން ވެނެޒުއެލާ, އެކްއަޑޯ, ބޮލިވިއާ, ގުޔާނާ, ސުރިނާމް އަދި ފްރެންޗް ގްއާނާ އަށް ވެސް ނިސްބަތްވެއެވެ. މީގެ ތެރެއިން 4 ގައުމެއްގައި \"އެމެޒޮނާސް\" ހިމަނައިގެން ސްޓޭޓް ނުވަތަ ޑިޕާޓްމަންޓް އަކަށް ނަންދީފައިވެއެވެ. މުޅި ދުނިޔޭގައި ބާކީ ހުރި ރެއިންފޮރެސްޓްގެ ތެރެއިން ދެބައިކުޅަ އެއްބަޔަށްވުރެބޮޑުވަރެއް އެމޭޒޮން ރެއިންފޮރެސްޓް ހިއްސާކުރެއެވެ. މިއީ މުޅި ދުނިޔެއިން އެންމޮ ބޮޑު އަދި އެންމެ ބައޮޑައިވަރސް ރެއިންފޮރެސްޓް ޓްރެކްޓެވެ. ލަފާކުރެވޭ ގޮތުން 16 ހާސް ސްޕީޝީސްއަށް ބެހިގެންވާ 390 މިލިއަން ވައްތަރުގެ ގަސް މިތާގައި ހިމެނެއެވެ`,\n            },\n        ],\n    ],\n    [\n        \"translation\",\n        [\n            `އަހަރެންގެ ނަމަކީ އަހުމަދު އަދި އަހަރެން ދިރިއުޅެނީ މާލޭގަ`,\n            `އަހަރެންގެ ނަމަކީ ސާރާ އަދި އަހަރެން ދިރިއުޅެނީ އުތީމުގަ`,\n        ],\n    ],\n    [\n        \"summarization\",\n        [\n            `ޓަވަރުގެ އުސްމިނަކީ 324 މީޓަރު، އެއީ ގާތްގަނޑަކަށް 81 ބުރީގެ އިމާރާތަކާއި އެއްވަރެވެ. އެއީ ޕެރިސްގައި ހުރި އެންމެ އުސް އިމާރާތެވެ. އޭގެ ހަތަރެސްކަނަށް ހުރި ބުޑުގެ ދިގުމިނަކީ ކޮންމެ ފަރާތަކުން 125 މީޓަރެވެ. (410 ފޫޓު) އައިފިލް ޓަވަރު ބިނާކުރި އިރު، ވޮޝިންގްޓަން މޮނިއުމެންޓްގެ އުސްމިން ފަހަނައަޅާ ގޮސް، ދުނިޔޭގައި މީހުން އުފެއްދި ތަންތަނުގެ ތެރެއިން އެންމެ އުސް ތަނުގެ ލަގަބު ލިބުނެވެ. އަދި 1930 ގައި ނިއު ޔޯކްގެ ކްރައިސްލަރ ބިލްޑިންގް ބިނާކުރުމާއި ހަމައަށް 41 އަހަރު ވަންދެން މިލަގަބު ހިފެހެއްޓިއެވެ. މިއީ 300 މީޓަރަށް ވުރެ އުސްކޮށް އިމާރާތްކުރެވުނު ފުރަތަމަ ތަނެވެ. 1957 ގައި ޓަވަރުގެ އެންމެ މަތީގައި ހަރުކުރެވުނު ބްރޯޑްކާސްޓިންގ އޭރިއަލްގެ ސަބަބުން މިހާރު މި ޓަވަރު ކްރައިސްލަރ ބިލްޑިންގއަށް ވުރެ 5.2 މީޓަރ (17 ފޫޓު) އުހެވެ. މި ޓްރާންސްމިޓަރު ނުލާ، އައިފިލް ޓަވަރަކީ، މިލާއު ވިއާޑަކްޓަށް ފަހު ފްރާންސްގައި ހުރި 2 ވަނައަށް އެންމެ އުސް ފްރީސްޓޭންޑިންގ އިމާރާތެވެ`,\n        ],\n    ],\n    [\n        \"text-generation\",\n        [\n            `އަހަރެންގެ ނަމަކީ ޔޫސުފް އަދި އަހަރެންގެ މައިގަނޑު`,\n            `އަހަރެންގެ ނަމަކީ މަރިއަމް، އަހަރެން އެންމެ ގަޔާވާ`,\n            `އަހަރެންގެ ނަމަކީ ފާތުމަތު އަދި އަހަރެން`,\n            `،އެއް ޒަމާނެއްގައި`,\n        ],\n    ],\n    [\"fill-mask\", [`.<mask> މާލެ އަކީ ދިވެހިރާއްޖޭގެ`, `ގަރުދިޔައަކީ ދިވެހިންގެ މެދުގައި <mask> ކެއުމެއް.`]],\n]);\nexport const MAPPING_DEFAULT_WIDGET = new Map([\n    [\"en\", MAPPING_EN],\n    [\"zh\", MAPPING_ZH],\n    [\"fr\", MAPPING_FR],\n    [\"es\", MAPPING_ES],\n    [\"ru\", MAPPING_RU],\n    [\"uk\", MAPPING_UK],\n    [\"it\", MAPPING_IT],\n    [\"fa\", MAPPING_FA],\n    [\"ar\", MAPPING_AR],\n    [\"bn\", MAPPING_BN],\n    [\"mn\", MAPPING_MN],\n    [\"si\", MAPPING_SI],\n    [\"de\", MAPPING_DE],\n    [\"dv\", MAPPING_DV],\n]);\n"],"names":[],"mappings":"AAAA,yBAAyB;AACzB,GAAG;AACH,2DAA2D;AAC3D,qEAAqE;AACrE,2CAA2C;AAC3C,GAAG;AACH,6DAA6D;AAC7D,EAAE;AACF,qEAAqE;AACrE,0DAA0D;AAC1D,GAAG;AACH,2HAA2H;AAC3H,EAAE;;;;;AACF,MAAM,aAAa,IAAI,IAAI;IACvB;QAAC;QAAuB;YAAC,CAAC,sBAAsB,CAAC;SAAC;KAAC;IACnD;QACI;QACA;YACI,CAAC,wCAAwC,CAAC;YAC1C,CAAC,qCAAqC,CAAC;YACvC,CAAC,oDAAoD,CAAC;SACzD;KACJ;IACD;QACI;QACA;YACI;gBACI,MAAM,CAAC,qDAAqD,CAAC;gBAC7D,OAAO;oBACH,YAAY;wBAAC;wBAAgB;wBAAY;qBAAa;oBACtD,OAAO;wBAAC;wBAAO;wBAAM;qBAAK;oBAC1B,cAAc;wBAAC;wBAAK;wBAAI;qBAAG;oBAC3B,wBAAwB;wBAAC;wBAAU;wBAAU;qBAA0B;gBAC3E;YACJ;SACH;KACJ;IACD;QACI;QACA;YACI;gBACI,MAAM,CAAC,gBAAgB,CAAC;gBACxB,SAAS,CAAC,wCAAwC,CAAC;YACvD;YACA;gBACI,MAAM,CAAC,gBAAgB,CAAC;gBACxB,SAAS,CAAC,qCAAqC,CAAC;YACpD;YACA;gBACI,MAAM,CAAC,eAAe,CAAC;gBACvB,SAAS,CAAC,wCAAwC,CAAC;YACvD;YACA;gBACI,MAAM,CAAC,qEAAqE,CAAC;gBAC7E,SAAS,CAAC,iiCAAiiC,CAAC;YAChjC;SACH;KACJ;IACD;QACI;QACA;YACI;gBACI,MAAM;gBACN,kBAAkB;gBAClB,aAAa;YACjB;YACA;gBACI,MAAM;gBACN,kBAAkB;gBAClB,aAAa;YACjB;YACA;gBACI,MAAM;gBACN,kBAAkB;gBAClB,aAAa;YACjB;SACH;KACJ;IACD;QAAC;QAAe;YAAC,CAAC,wCAAwC,CAAC;YAAE,CAAC,qCAAqC,CAAC;SAAC;KAAC;IACtG;QACI;QACA;YACI,CAAC,uuBAAuuB,CAAC;SAC5uB;KACJ;IACD;QACI;QACA;YACI,CAAC,8BAA8B,CAAC;YAChC,CAAC,mBAAmB,CAAC;YACrB,CAAC,+CAA+C,CAAC;YACjD,CAAC,0CAA0C,CAAC;SAC/C;KACJ;IACD;QACI;QACA;YACI,CAAC,+BAA+B,CAAC;YACjC,CAAC,iCAAiC,CAAC;YACnC,CAAC,mCAAmC,CAAC;YACrC,CAAC,iBAAiB,CAAC;SACtB;KACJ;IACD;QAAC;QAAa;YAAC,CAAC,8BAA8B,CAAC;YAAE,CAAC,2BAA2B,CAAC;SAAC;KAAC;IAChF;QACI;QACA;YACI;gBACI,iBAAiB;gBACjB,WAAW;oBAAC;oBAAuB;oBAA+B;iBAAuB;YAC7F;SACH;KACJ;CACJ;AACD,MAAM,aAAa,IAAI,IAAI;IACvB;QAAC;QAAuB;YAAC,CAAC,SAAS,CAAC;SAAC;KAAC;IACtC;QAAC;QAAwB;YAAC,CAAC,aAAa,CAAC;YAAE,CAAC,WAAW,CAAC;YAAE,CAAC,eAAe,CAAC;SAAC;KAAC;IAC7E;QACI;QACA;YACI;gBACI,MAAM,CAAC,MAAM,CAAC;gBACd,SAAS,CAAC,aAAa,CAAC;YAC5B;YACA;gBACI,MAAM,CAAC,MAAM,CAAC;gBACd,SAAS,CAAC,WAAW,CAAC;YAC1B;YACA;gBACI,MAAM,CAAC,QAAQ,CAAC;gBAChB,SAAS,CAAC,aAAa,CAAC;YAC5B;SACH;KACJ;IACD;QAAC;QAAe;YAAC,CAAC,aAAa,CAAC;YAAE,CAAC,WAAW,CAAC;SAAC;KAAC;IACjD;QACI;QACA;YACI;gBACI,MAAM;gBACN,kBAAkB;YACtB;SACH;KACJ;IACD;QACI;QACA;YACI,CAAC,kOAAkO,CAAC;SACvO;KACJ;IACD;QACI;QACA;YAAC,CAAC,SAAS,CAAC;YAAE,CAAC,UAAU,CAAC;YAAE,CAAC,WAAW,CAAC;YAAE,CAAC,QAAQ,CAAC;YAAE,CAAC,GAAG,CAAC;SAAC;KAChE;IACD;QAAC;QAAa;YAAC,CAAC,cAAc,CAAC;YAAE,CAAC,aAAa,CAAC;SAAC;KAAC;IAClD;QACI;QACA;YACI;gBACI,iBAAiB;gBACjB,WAAW;oBAAC;oBAAY;oBAAc;iBAAQ;YAClD;SACH;KACJ;CACJ;AACD,MAAM,aAAa,IAAI,IAAI;IACvB;QAAC;QAAuB;YAAC,CAAC,kCAAkC,CAAC;SAAC;KAAC;IAC/D;QAAC;QAAwB;YAAC,CAAC,uCAAuC,CAAC;SAAC;KAAC;IACrE;QACI;QACA;YACI;gBACI,MAAM,CAAC,qBAAqB,CAAC;gBAC7B,SAAS,CAAC,uCAAuC,CAAC;YACtD;SACH;KACJ;IACD;QAAC;QAAe;YAAC,CAAC,uCAAuC,CAAC;SAAC;KAAC;IAC5D;QACI;QACA;YACI,CAAC,+yBAA+yB,CAAC;SACpzB;KACJ;IACD;QAAC;QAAmB;YAAC,CAAC,4BAA4B,CAAC;YAAE,CAAC,mCAAmC,CAAC;YAAE,CAAC,iBAAiB,CAAC;SAAC;KAAC;IACjH;QAAC;QAAa;YAAC,CAAC,iCAAiC,CAAC;SAAC;KAAC;IACpD;QACI;QACA;YACI;gBACI,iBAAiB;gBACjB,WAAW;oBACP;oBACA;oBACA;iBACH;YACL;SACH;KACJ;CACJ;AACD,MAAM,aAAa,IAAI,IAAI;IACvB;QAAC;QAAuB;YAAC,CAAC,kBAAkB,CAAC;SAAC;KAAC;IAC/C;QAAC;QAAwB;YAAC,CAAC,kCAAkC,CAAC;SAAC;KAAC;IAChE;QACI;QACA;YACI;gBACI,MAAM,CAAC,YAAY,CAAC;gBACpB,SAAS,CAAC,kCAAkC,CAAC;YACjD;YACA;gBACI,MAAM,CAAC,4BAA4B,CAAC;gBACpC,SAAS,CAAC,oDAAoD,CAAC;YACnE;YACA;gBACI,MAAM,CAAC,iCAAiC,CAAC;gBACzC,SAAS,CAAC,0FAA0F,CAAC;YACzG;SACH;KACJ;IACD;QACI;QACA;YACI,CAAC,kCAAkC,CAAC;YACpC,CAAC,0EAA0E,CAAC;SAC/E;KACJ;IACD;QACI;QACA;YACI,CAAC,8yBAA8yB,CAAC;SACnzB;KACJ;IACD;QACI;QACA;YACI,CAAC,0BAA0B,CAAC;YAC5B,CAAC,8BAA8B,CAAC;YAChC,CAAC,4BAA4B,CAAC;YAC9B,CAAC,cAAc,CAAC;YAChB,CAAC,oBAAoB,CAAC;SACzB;KACJ;IACD;QAAC;QAAa;YAAC,CAAC,yCAAyC,CAAC;YAAE,CAAC,+CAA+C,CAAC;SAAC;KAAC;IAC/G;QACI;QACA;YACI;gBACI,iBAAiB;gBACjB,WAAW;oBAAC;oBAAyB;oBAAgC;iBAAwB;YACjG;SACH;KACJ;CACJ;AACD,MAAM,aAAa,IAAI,IAAI;IACvB;QAAC;QAAuB;YAAC,CAAC,8BAA8B,CAAC;SAAC;KAAC;IAC3D;QAAC;QAAwB;YAAC,CAAC,uCAAuC,CAAC;SAAC;KAAC;IACrE;QACI;QACA;YACI;gBACI,MAAM,CAAC,SAAS,CAAC;gBACjB,SAAS,CAAC,uCAAuC,CAAC;YACtD;SACH;KACJ;IACD;QAAC;QAAe;YAAC,CAAC,uCAAuC,CAAC;SAAC;KAAC;IAC5D;QACI;QACA;YACI,CAAC,wvBAAwvB,CAAC;SAC7vB;KACJ;IACD;QAAC;QAAmB;YAAC,CAAC,mBAAmB,CAAC;YAAE,CAAC,+BAA+B,CAAC;YAAE,CAAC,OAAO,CAAC;SAAC;KAAC;IAC1F;QAAC;QAAa;YAAC,CAAC,kDAAkD,CAAC;SAAC;KAAC;IACrE;QACI;QACA;YACI;gBACI,iBAAiB;gBACjB,WAAW;oBAAC;oBAAyB;oBAAgC;iBAAyB;YAClG;SACH;KACJ;CACJ;AACD,MAAM,aAAa,IAAI,IAAI;IACvB;QAAC;QAAe;YAAC,CAAC,wCAAwC,CAAC;SAAC;KAAC;IAC7D;QAAC;QAAa;YAAC,CAAC,kBAAkB,CAAC;SAAC;KAAC;CACxC;AACD,MAAM,aAAa,IAAI,IAAI;IACvB;QAAC;QAAuB;YAAC,CAAC,gBAAgB,CAAC;SAAC;KAAC;IAC7C;QACI;QACA;YACI,CAAC,mCAAmC,CAAC;YACrC,CAAC,+BAA+B,CAAC;YACjC,CAAC,gDAAgD,CAAC;SACrD;KACJ;IACD;QACI;QACA;YACI;gBACI,MAAM,CAAC,UAAU,CAAC;gBAClB,SAAS,CAAC,mCAAmC,CAAC;YAClD;YACA;gBACI,MAAM,CAAC,UAAU,CAAC;gBAClB,SAAS,CAAC,+BAA+B,CAAC;YAC9C;YACA;gBACI,MAAM,CAAC,gBAAgB,CAAC;gBACxB,SAAS,CAAC,kCAAkC,CAAC;YACjD;SACH;KACJ;IACD;QAAC;QAAe;YAAC,CAAC,mCAAmC,CAAC;YAAE,CAAC,+BAA+B,CAAC;SAAC;KAAC;IAC3F;QACI;QACA;YACI,CAAC,0wBAA0wB,CAAC;SAC/wB;KACJ;IACD;QACI;QACA;YACI,CAAC,2BAA2B,CAAC;YAC7B,CAAC,oCAAoC,CAAC;YACtC,CAAC,yCAAyC,CAAC;YAC3C,CAAC,sBAAsB,CAAC;YACxB,CAAC,eAAe,CAAC;SACpB;KACJ;IACD;QAAC;QAAa;YAAC,CAAC,0BAA0B,CAAC;YAAE,CAAC,6BAA6B,CAAC;SAAC;KAAC;IAC9E;QACI;QACA;YACI;gBACI,iBAAiB;gBACjB,WAAW;oBAAC;oBAA2B;oBAAqC;iBAA8B;YAC9G;SACH;KACJ;CACJ;AACD,MAAM,aAAa,IAAI,IAAI;IACvB;QACI;QACA;YAAC,CAAC,yCAAyC,CAAC;YAAE,CAAC,uBAAuB,CAAC;YAAE,CAAC,iBAAiB,CAAC;YAAE,CAAC,aAAa,CAAC;SAAC;KACjH;IACD;QACI;QACA;YACI,CAAC,4EAA4E,CAAC;YAC9E,CAAC,+DAA+D,CAAC;YACjE,CAAC,4EAA4E,CAAC;SACjF;KACJ;IACD;QACI;QACA;YACI;gBACI,MAAM,CAAC,mBAAmB,CAAC;gBAC3B,SAAS,CAAC,wCAAwC,CAAC;YACvD;YACA;gBACI,MAAM,CAAC,6BAA6B,CAAC;gBACrC,SAAS,CAAC,6CAA6C,CAAC;YAC5D;YACA;gBACI,MAAM,CAAC,YAAY,CAAC;gBACpB,SAAS,CAAC,qCAAqC,CAAC;YACpD;YACA;gBACI,MAAM,CAAC,2CAA2C,CAAC;gBACnD,SAAS;oBACL;oBACA;oBACA;iBACH,CAAC,IAAI,CAAC;YACX;SACH;KACJ;IACD;QACI;QACA;YACI;YACA;SACH;KACJ;IACD;QACI;QACA;YACI;gBACI;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;aACH,CAAC,IAAI,CAAC;SACV;KACJ;IACD;QAAC;QAAmB;YAAC;YAA0B;SAAe;KAAC;IAC/D;QACI;QACA;YACI,CAAC,2DAA2D,CAAC;YAC7D,CAAC,iEAAiE,CAAC;SACtE;KACJ;CACJ;AACD,MAAM,aAAa,IAAI,IAAI;IACvB;QAAC;QAAuB;YAAC,CAAC,WAAW,CAAC;SAAC;KAAC;IACxC;QACI;QACA;YAAC,CAAC,wBAAwB,CAAC;YAAE,CAAC,uBAAuB,CAAC;YAAE,CAAC,mCAAmC,CAAC;SAAC;KACjG;IACD;QACI;QACA;YACI;gBACI,MAAM,CAAC,SAAS,CAAC;gBACjB,SAAS,CAAC,wBAAwB,CAAC;YACvC;YACA;gBACI,MAAM,CAAC,SAAS,CAAC;gBACjB,SAAS,CAAC,uBAAuB,CAAC;YACtC;YACA;gBACI,MAAM,CAAC,QAAQ,CAAC;gBAChB,SAAS,CAAC,wBAAwB,CAAC;YACvC;YACA;gBACI,MAAM,CAAC,+BAA+B,CAAC;gBACvC,SAAS,CAAC,yHAAyH,CAAC;YACxI;SACH;KACJ;IACD;QAAC;QAAe;YAAC,CAAC,wBAAwB,CAAC;YAAE,CAAC,uBAAuB,CAAC;SAAC;KAAC;IACxE;QACI;QACA;YACI,CAAC,kPAAkP,CAAC;SACvP;KACJ;IACD;QACI;QACA;YACI,CAAC,iBAAiB,CAAC;YACnB,CAAC,0DAA0D,CAAC;YAC5D,CAAC,cAAc,CAAC;YAChB,CAAC,wCAAwC,CAAC;YAC1C,CAAC,4BAA4B,CAAC;SACjC;KACJ;IACD;QAAC;QAAa;YAAC,CAAC,mBAAmB,CAAC;YAAE,CAAC,uBAAuB,CAAC;SAAC;KAAC;IACjE;QACI;QACA;YACI;gBACI,iBAAiB;gBACjB,WAAW;oBAAC;oBAAgB;oBAAoB;iBAAoB;YACxE;SACH;KACJ;CACJ;AACD,MAAM,aAAa,IAAI,IAAI;IACvB;QAAC;QAAuB;YAAC,CAAC,+BAA+B,CAAC;SAAC;KAAC;IAC5D;QACI;QACA;YAAC,CAAC,qCAAqC,CAAC;YAAE,CAAC,sBAAsB,CAAC;YAAE,CAAC,0CAA0C,CAAC;SAAC;KACpH;IACD;QAAC;QAAe;YAAC,CAAC,mCAAmC,CAAC;YAAE,CAAC,yBAAyB,CAAC;SAAC;KAAC;IACrF;QACI;QACA;YACI,CAAC,2jCAA2jC,CAAC;SAChkC;KACJ;IACD;QAAC;QAAmB;YAAC,CAAC,eAAe,CAAC;YAAE,CAAC,gBAAgB,CAAC;YAAE,CAAC,gBAAgB,CAAC;SAAC;KAAC;IAChF;QAAC;QAAa;YAAC,CAAC,sBAAsB,CAAC;YAAE,CAAC,yBAAyB,CAAC;SAAC;KAAC;IACtE;QACI;QACA;YACI;gBACI,MAAM,CAAC,wDAAwD,CAAC;gBAChE,SAAS,CAAC,4iBAA4iB,CAAC;YAC3jB;YACA;gBACI,MAAM,CAAC,2DAA2D,CAAC;gBACnE,SAAS,CAAC,+MAA+M,CAAC;YAC9N;YACA;gBACI,MAAM,CAAC,6CAA6C,CAAC;gBACrD,SAAS,CAAC,gfAAgf,CAAC;YAC/f;SACH;KACJ;IACD;QACI;QACA;YACI;gBACI,iBAAiB;gBACjB,WAAW;oBAAC;oBAAmB;oBAAqB;iBAA2B;YACnF;SACH;KACJ;CACJ;AACD,MAAM,aAAa,IAAI,IAAI;IACvB;QAAC;QAAuB;YAAC,CAAC,eAAe,CAAC;SAAC;KAAC;IAC5C;QACI;QACA;YACI,CAAC,6CAA6C,CAAC;YAC/C,CAAC,0CAA0C,CAAC;YAC5C,CAAC,6BAA6B,CAAC;SAClC;KACJ;IACD;QACI;QACA;YACI;gBACI,MAAM,CAAC,sBAAsB,CAAC;gBAC9B,SAAS,CAAC,6CAA6C,CAAC;YAC5D;YACA;gBACI,MAAM,CAAC,mBAAmB,CAAC;gBAC3B,SAAS,CAAC,6CAA6C,CAAC;YAC5D;YACA;gBACI,MAAM,CAAC,0BAA0B,CAAC;gBAClC,SAAS,CAAC,0CAA0C,CAAC;YACzD;SACH;KACJ;IACD;QAAC;QAAe;YAAC,CAAC,6CAA6C,CAAC;YAAE,CAAC,0CAA0C,CAAC;SAAC;KAAC;IAChH;QACI;QACA;YACI,CAAC,siBAAsiB,CAAC;SAC3iB;KACJ;IACD;QACI;QACA;YAAC,CAAC,qBAAqB,CAAC;YAAE,CAAC,uBAAuB,CAAC;YAAE,CAAC,wBAAwB,CAAC;YAAE,CAAC,eAAe,CAAC;SAAC;KACtG;IACD;QAAC;QAAa;YAAC,CAAC,kDAAkD,CAAC;YAAE,CAAC,mCAAmC,CAAC;SAAC;KAAC;IAC5G;QACI;QACA;YACI;gBACI,OAAO,CAAC,0BAA0B,CAAC;gBACnC,KAAK,CAAC,gFAAgF,CAAC;YAC3F;YACA;gBACI,OAAO,CAAC,yBAAyB,CAAC;gBAClC,KAAK,CAAC,+EAA+E,CAAC;YAC1F;SACH;KACJ;IACD;QACI;QACA;YACI,CAAC,sBAAsB,CAAC;YACxB,CAAC,mDAAmD,CAAC;YACrD,CAAC,oCAAoC,CAAC;SACzC;KACJ;IACD;QACI;QACA;YACI;gBACI,iBAAiB;gBACjB,WAAW;oBAAC;oBAAiC;oBAAsC;iBAA4B;YACnH;SACH;KACJ;CACJ;AACD,MAAM,aAAa,IAAI,IAAI;IACvB;QAAC;QAAe;YAAC,CAAC,yBAAyB,CAAC;YAAE,CAAC,oCAAoC,CAAC;SAAC;KAAC;IACtF;QAAC;QAAa;YAAC,CAAC,eAAe,CAAC;YAAE,CAAC,qBAAqB,CAAC;SAAC;KAAC;CAC9D;AACD,MAAM,aAAa,IAAI,IAAI;IACvB;QACI;QACA;YACI;gBACI,MAAM,CAAC,aAAa,CAAC;gBACrB,SAAS,CAAC,6CAA6C,CAAC;YAC5D;YACA;gBACI,MAAM,CAAC,wFAAwF,CAAC;gBAChG,SAAS,CAAC,u6BAAu6B,CAAC;YACt7B;SACH;KACJ;IACD;QACI;QACA;YACI;gBACI,iBAAiB;gBACjB,WAAW;oBACP;oBACA;oBACA;iBACH;YACL;SACH;KACJ;CACJ;AACD,MAAM,aAAa,IAAI,IAAI;IACvB;QAAC;QAAuB;YAAC,CAAC,gCAAgC,CAAC;SAAC;KAAC;IAC7D;QACI;QACA;YACI,CAAC,0DAA0D,CAAC;YAC5D,CAAC,wDAAwD,CAAC;YAC1D,CAAC,gEAAgE,CAAC;SACrE;KACJ;IACD;QACI;QACA;YACI;gBACI,MAAM,CAAC,6BAA6B,CAAC;gBACrC,SAAS,CAAC,0DAA0D,CAAC;YACzE;YACA;gBACI,MAAM,CAAC,6BAA6B,CAAC;gBACrC,SAAS,CAAC,wDAAwD,CAAC;YACvE;YACA;gBACI,MAAM,CAAC,uBAAuB,CAAC;gBAC/B,SAAS,CAAC,wDAAwD,CAAC;YACvE;YACA;gBACI,MAAM,CAAC,kFAAkF,CAAC;gBAC1F,SAAS,CAAC,mwCAAmwC,CAAC;YAClxC;SACH;KACJ;IACD;QACI;QACA;YACI,CAAC,0DAA0D,CAAC;YAC5D,CAAC,wDAAwD,CAAC;SAC7D;KACJ;IACD;QACI;QACA;YACI,CAAC,81BAA81B,CAAC;SACn2B;KACJ;IACD;QACI;QACA;YACI,CAAC,kDAAkD,CAAC;YACpD,CAAC,kDAAkD,CAAC;YACpD,CAAC,wCAAwC,CAAC;YAC1C,CAAC,kBAAkB,CAAC;SACvB;KACJ;IACD;QAAC;QAAa;YAAC,CAAC,gCAAgC,CAAC;YAAE,CAAC,iDAAiD,CAAC;SAAC;KAAC;CAC3G;AACM,MAAM,yBAAyB,IAAI,IAAI;IAC1C;QAAC;QAAM;KAAW;IAClB;QAAC;QAAM;KAAW;IAClB;QAAC;QAAM;KAAW;IAClB;QAAC;QAAM;KAAW;IAClB;QAAC;QAAM;KAAW;IAClB;QAAC;QAAM;KAAW;IAClB;QAAC;QAAM;KAAW;IAClB;QAAC;QAAM;KAAW;IAClB;QAAC;QAAM;KAAW;IAClB;QAAC;QAAM;KAAW;IAClB;QAAC;QAAM;KAAW;IAClB;QAAC;QAAM;KAAW;IAClB;QAAC;QAAM;KAAW;IAClB;QAAC;QAAM;KAAW;CACrB","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 1186, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/pipelines.js"],"sourcesContent":["export const MODALITIES = [\"multimodal\", \"nlp\", \"cv\", \"audio\", \"tabular\", \"rl\", \"other\"];\nexport const MODALITY_LABELS = {\n    multimodal: \"Multimodal\",\n    nlp: \"Natural Language Processing\",\n    audio: \"Audio\",\n    cv: \"Computer Vision\",\n    rl: \"Reinforcement Learning\",\n    tabular: \"Tabular\",\n    other: \"Other\",\n};\n/// Coarse-grained taxonomy of tasks\n///\n/// This type is used in multiple places in the Hugging Face\n/// ecosystem:\n///  - To determine which widget to show.\n///  - To determine which endpoint of Inference Endpoints to use.\n///  - As filters at the left of models and datasets page.\n///\n/// Note that this is sensitive to order.\n/// For each domain, the order should be of decreasing specificity.\n/// This will impact the default pipeline tag of a model when not\n/// specified.\nexport const PIPELINE_DATA = {\n    \"text-classification\": {\n        name: \"Text Classification\",\n        subtasks: [\n            {\n                type: \"acceptability-classification\",\n                name: \"Acceptability Classification\",\n            },\n            {\n                type: \"entity-linking-classification\",\n                name: \"Entity Linking Classification\",\n            },\n            {\n                type: \"fact-checking\",\n                name: \"Fact Checking\",\n            },\n            {\n                type: \"intent-classification\",\n                name: \"Intent Classification\",\n            },\n            {\n                type: \"language-identification\",\n                name: \"Language Identification\",\n            },\n            {\n                type: \"multi-class-classification\",\n                name: \"Multi Class Classification\",\n            },\n            {\n                type: \"multi-label-classification\",\n                name: \"Multi Label Classification\",\n            },\n            {\n                type: \"multi-input-text-classification\",\n                name: \"Multi-input Text Classification\",\n            },\n            {\n                type: \"natural-language-inference\",\n                name: \"Natural Language Inference\",\n            },\n            {\n                type: \"semantic-similarity-classification\",\n                name: \"Semantic Similarity Classification\",\n            },\n            {\n                type: \"sentiment-classification\",\n                name: \"Sentiment Classification\",\n            },\n            {\n                type: \"topic-classification\",\n                name: \"Topic Classification\",\n            },\n            {\n                type: \"semantic-similarity-scoring\",\n                name: \"Semantic Similarity Scoring\",\n            },\n            {\n                type: \"sentiment-scoring\",\n                name: \"Sentiment Scoring\",\n            },\n            {\n                type: \"sentiment-analysis\",\n                name: \"Sentiment Analysis\",\n            },\n            {\n                type: \"hate-speech-detection\",\n                name: \"Hate Speech Detection\",\n            },\n            {\n                type: \"text-scoring\",\n                name: \"Text Scoring\",\n            },\n        ],\n        modality: \"nlp\",\n    },\n    \"token-classification\": {\n        name: \"Token Classification\",\n        subtasks: [\n            {\n                type: \"named-entity-recognition\",\n                name: \"Named Entity Recognition\",\n            },\n            {\n                type: \"part-of-speech\",\n                name: \"Part of Speech\",\n            },\n            {\n                type: \"parsing\",\n                name: \"Parsing\",\n            },\n            {\n                type: \"lemmatization\",\n                name: \"Lemmatization\",\n            },\n            {\n                type: \"word-sense-disambiguation\",\n                name: \"Word Sense Disambiguation\",\n            },\n            {\n                type: \"coreference-resolution\",\n                name: \"Coreference-resolution\",\n            },\n        ],\n        modality: \"nlp\",\n    },\n    \"table-question-answering\": {\n        name: \"Table Question Answering\",\n        modality: \"nlp\",\n    },\n    \"question-answering\": {\n        name: \"Question Answering\",\n        subtasks: [\n            {\n                type: \"extractive-qa\",\n                name: \"Extractive QA\",\n            },\n            {\n                type: \"open-domain-qa\",\n                name: \"Open Domain QA\",\n            },\n            {\n                type: \"closed-domain-qa\",\n                name: \"Closed Domain QA\",\n            },\n        ],\n        modality: \"nlp\",\n    },\n    \"zero-shot-classification\": {\n        name: \"Zero-Shot Classification\",\n        modality: \"nlp\",\n    },\n    translation: {\n        name: \"Translation\",\n        modality: \"nlp\",\n    },\n    summarization: {\n        name: \"Summarization\",\n        subtasks: [\n            {\n                type: \"news-articles-summarization\",\n                name: \"News Articles Summarization\",\n            },\n            {\n                type: \"news-articles-headline-generation\",\n                name: \"News Articles Headline Generation\",\n            },\n        ],\n        modality: \"nlp\",\n    },\n    \"feature-extraction\": {\n        name: \"Feature Extraction\",\n        modality: \"nlp\",\n    },\n    \"text-generation\": {\n        name: \"Text Generation\",\n        subtasks: [\n            {\n                type: \"dialogue-modeling\",\n                name: \"Dialogue Modeling\",\n            },\n            {\n                type: \"dialogue-generation\",\n                name: \"Dialogue Generation\",\n            },\n            {\n                type: \"conversational\",\n                name: \"Conversational\",\n            },\n            {\n                type: \"language-modeling\",\n                name: \"Language Modeling\",\n            },\n            {\n                type: \"text-simplification\",\n                name: \"Text simplification\",\n            },\n            {\n                type: \"explanation-generation\",\n                name: \"Explanation Generation\",\n            },\n            {\n                type: \"abstractive-qa\",\n                name: \"Abstractive QA\",\n            },\n            {\n                type: \"open-domain-abstractive-qa\",\n                name: \"Open Domain Abstractive QA\",\n            },\n            {\n                type: \"closed-domain-qa\",\n                name: \"Closed Domain QA\",\n            },\n            {\n                type: \"open-book-qa\",\n                name: \"Open Book QA\",\n            },\n            {\n                type: \"closed-book-qa\",\n                name: \"Closed Book QA\",\n            },\n            {\n                type: \"text2text-generation\",\n                name: \"Text2Text Generation\",\n            },\n        ],\n        modality: \"nlp\",\n    },\n    \"fill-mask\": {\n        name: \"Fill-Mask\",\n        subtasks: [\n            {\n                type: \"slot-filling\",\n                name: \"Slot Filling\",\n            },\n            {\n                type: \"masked-language-modeling\",\n                name: \"Masked Language Modeling\",\n            },\n        ],\n        modality: \"nlp\",\n    },\n    \"sentence-similarity\": {\n        name: \"Sentence Similarity\",\n        modality: \"nlp\",\n    },\n    \"text-to-speech\": {\n        name: \"Text-to-Speech\",\n        modality: \"audio\",\n    },\n    \"text-to-audio\": {\n        name: \"Text-to-Audio\",\n        modality: \"audio\",\n    },\n    \"automatic-speech-recognition\": {\n        name: \"Automatic Speech Recognition\",\n        modality: \"audio\",\n    },\n    \"audio-to-audio\": {\n        name: \"Audio-to-Audio\",\n        modality: \"audio\",\n    },\n    \"audio-classification\": {\n        name: \"Audio Classification\",\n        subtasks: [\n            {\n                type: \"keyword-spotting\",\n                name: \"Keyword Spotting\",\n            },\n            {\n                type: \"speaker-identification\",\n                name: \"Speaker Identification\",\n            },\n            {\n                type: \"audio-intent-classification\",\n                name: \"Audio Intent Classification\",\n            },\n            {\n                type: \"audio-emotion-recognition\",\n                name: \"Audio Emotion Recognition\",\n            },\n            {\n                type: \"audio-language-identification\",\n                name: \"Audio Language Identification\",\n            },\n        ],\n        modality: \"audio\",\n    },\n    \"audio-text-to-text\": {\n        name: \"Audio-Text-to-Text\",\n        modality: \"multimodal\",\n        hideInDatasets: true,\n    },\n    \"voice-activity-detection\": {\n        name: \"Voice Activity Detection\",\n        modality: \"audio\",\n    },\n    \"depth-estimation\": {\n        name: \"Depth Estimation\",\n        modality: \"cv\",\n    },\n    \"image-classification\": {\n        name: \"Image Classification\",\n        subtasks: [\n            {\n                type: \"multi-label-image-classification\",\n                name: \"Multi Label Image Classification\",\n            },\n            {\n                type: \"multi-class-image-classification\",\n                name: \"Multi Class Image Classification\",\n            },\n        ],\n        modality: \"cv\",\n    },\n    \"object-detection\": {\n        name: \"Object Detection\",\n        subtasks: [\n            {\n                type: \"face-detection\",\n                name: \"Face Detection\",\n            },\n            {\n                type: \"vehicle-detection\",\n                name: \"Vehicle Detection\",\n            },\n        ],\n        modality: \"cv\",\n    },\n    \"image-segmentation\": {\n        name: \"Image Segmentation\",\n        subtasks: [\n            {\n                type: \"instance-segmentation\",\n                name: \"Instance Segmentation\",\n            },\n            {\n                type: \"semantic-segmentation\",\n                name: \"Semantic Segmentation\",\n            },\n            {\n                type: \"panoptic-segmentation\",\n                name: \"Panoptic Segmentation\",\n            },\n        ],\n        modality: \"cv\",\n    },\n    \"text-to-image\": {\n        name: \"Text-to-Image\",\n        modality: \"cv\",\n    },\n    \"image-to-text\": {\n        name: \"Image-to-Text\",\n        subtasks: [\n            {\n                type: \"image-captioning\",\n                name: \"Image Captioning\",\n            },\n        ],\n        modality: \"cv\",\n    },\n    \"image-to-image\": {\n        name: \"Image-to-Image\",\n        subtasks: [\n            {\n                type: \"image-inpainting\",\n                name: \"Image Inpainting\",\n            },\n            {\n                type: \"image-colorization\",\n                name: \"Image Colorization\",\n            },\n            {\n                type: \"super-resolution\",\n                name: \"Super Resolution\",\n            },\n        ],\n        modality: \"cv\",\n    },\n    \"image-to-video\": {\n        name: \"Image-to-Video\",\n        modality: \"cv\",\n    },\n    \"unconditional-image-generation\": {\n        name: \"Unconditional Image Generation\",\n        modality: \"cv\",\n    },\n    \"video-classification\": {\n        name: \"Video Classification\",\n        modality: \"cv\",\n    },\n    \"reinforcement-learning\": {\n        name: \"Reinforcement Learning\",\n        modality: \"rl\",\n    },\n    robotics: {\n        name: \"Robotics\",\n        modality: \"rl\",\n        subtasks: [\n            {\n                type: \"grasping\",\n                name: \"Grasping\",\n            },\n            {\n                type: \"task-planning\",\n                name: \"Task Planning\",\n            },\n        ],\n    },\n    \"tabular-classification\": {\n        name: \"Tabular Classification\",\n        modality: \"tabular\",\n        subtasks: [\n            {\n                type: \"tabular-multi-class-classification\",\n                name: \"Tabular Multi Class Classification\",\n            },\n            {\n                type: \"tabular-multi-label-classification\",\n                name: \"Tabular Multi Label Classification\",\n            },\n        ],\n    },\n    \"tabular-regression\": {\n        name: \"Tabular Regression\",\n        modality: \"tabular\",\n        subtasks: [\n            {\n                type: \"tabular-single-column-regression\",\n                name: \"Tabular Single Column Regression\",\n            },\n        ],\n    },\n    \"tabular-to-text\": {\n        name: \"Tabular to Text\",\n        modality: \"tabular\",\n        subtasks: [\n            {\n                type: \"rdf-to-text\",\n                name: \"RDF to text\",\n            },\n        ],\n        hideInModels: true,\n    },\n    \"table-to-text\": {\n        name: \"Table to Text\",\n        modality: \"nlp\",\n        hideInModels: true,\n    },\n    \"multiple-choice\": {\n        name: \"Multiple Choice\",\n        subtasks: [\n            {\n                type: \"multiple-choice-qa\",\n                name: \"Multiple Choice QA\",\n            },\n            {\n                type: \"multiple-choice-coreference-resolution\",\n                name: \"Multiple Choice Coreference Resolution\",\n            },\n        ],\n        modality: \"nlp\",\n        hideInModels: true,\n    },\n    \"text-ranking\": {\n        name: \"Text Ranking\",\n        modality: \"nlp\",\n    },\n    \"text-retrieval\": {\n        name: \"Text Retrieval\",\n        subtasks: [\n            {\n                type: \"document-retrieval\",\n                name: \"Document Retrieval\",\n            },\n            {\n                type: \"utterance-retrieval\",\n                name: \"Utterance Retrieval\",\n            },\n            {\n                type: \"entity-linking-retrieval\",\n                name: \"Entity Linking Retrieval\",\n            },\n            {\n                type: \"fact-checking-retrieval\",\n                name: \"Fact Checking Retrieval\",\n            },\n        ],\n        modality: \"nlp\",\n        hideInModels: true,\n    },\n    \"time-series-forecasting\": {\n        name: \"Time Series Forecasting\",\n        modality: \"tabular\",\n        subtasks: [\n            {\n                type: \"univariate-time-series-forecasting\",\n                name: \"Univariate Time Series Forecasting\",\n            },\n            {\n                type: \"multivariate-time-series-forecasting\",\n                name: \"Multivariate Time Series Forecasting\",\n            },\n        ],\n    },\n    \"text-to-video\": {\n        name: \"Text-to-Video\",\n        modality: \"cv\",\n    },\n    \"image-text-to-text\": {\n        name: \"Image-Text-to-Text\",\n        modality: \"multimodal\",\n    },\n    \"visual-question-answering\": {\n        name: \"Visual Question Answering\",\n        subtasks: [\n            {\n                type: \"visual-question-answering\",\n                name: \"Visual Question Answering\",\n            },\n        ],\n        modality: \"multimodal\",\n    },\n    \"document-question-answering\": {\n        name: \"Document Question Answering\",\n        subtasks: [\n            {\n                type: \"document-question-answering\",\n                name: \"Document Question Answering\",\n            },\n        ],\n        modality: \"multimodal\",\n        hideInDatasets: true,\n    },\n    \"zero-shot-image-classification\": {\n        name: \"Zero-Shot Image Classification\",\n        modality: \"cv\",\n    },\n    \"graph-ml\": {\n        name: \"Graph Machine Learning\",\n        modality: \"other\",\n    },\n    \"mask-generation\": {\n        name: \"Mask Generation\",\n        modality: \"cv\",\n    },\n    \"zero-shot-object-detection\": {\n        name: \"Zero-Shot Object Detection\",\n        modality: \"cv\",\n    },\n    \"text-to-3d\": {\n        name: \"Text-to-3D\",\n        modality: \"cv\",\n    },\n    \"image-to-3d\": {\n        name: \"Image-to-3D\",\n        modality: \"cv\",\n    },\n    \"image-feature-extraction\": {\n        name: \"Image Feature Extraction\",\n        modality: \"cv\",\n    },\n    \"video-text-to-text\": {\n        name: \"Video-Text-to-Text\",\n        modality: \"multimodal\",\n        hideInDatasets: false,\n    },\n    \"keypoint-detection\": {\n        name: \"Keypoint Detection\",\n        subtasks: [\n            {\n                type: \"pose-estimation\",\n                name: \"Pose Estimation\",\n            },\n        ],\n        modality: \"cv\",\n        hideInDatasets: true,\n    },\n    \"visual-document-retrieval\": {\n        name: \"Visual Document Retrieval\",\n        modality: \"multimodal\",\n    },\n    \"any-to-any\": {\n        name: \"Any-to-Any\",\n        modality: \"multimodal\",\n    },\n    \"video-to-video\": {\n        name: \"Video-to-Video\",\n        modality: \"cv\",\n        hideInDatasets: true,\n    },\n    other: {\n        name: \"Other\",\n        modality: \"other\",\n        hideInModels: true,\n        hideInDatasets: true,\n    },\n};\nexport const PIPELINE_TYPES = Object.keys(PIPELINE_DATA);\nexport const SUBTASK_TYPES = Object.values(PIPELINE_DATA)\n    .flatMap((data) => (\"subtasks\" in data ? data.subtasks : []))\n    .map((s) => s.type);\nexport const PIPELINE_TYPES_SET = new Set(PIPELINE_TYPES);\n"],"names":[],"mappings":";;;;;;;;;;;;;;AAAO,MAAM,aAAa;IAAC;IAAc;IAAO;IAAM;IAAS;IAAW;IAAM;CAAQ;AACjF,MAAM,kBAAkB;IAC3B,YAAY;IACZ,KAAK;IACL,OAAO;IACP,IAAI;IACJ,IAAI;IACJ,SAAS;IACT,OAAO;AACX;AAaO,MAAM,gBAAgB;IACzB,uBAAuB;QACnB,MAAM;QACN,UAAU;YACN;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;SACH;QACD,UAAU;IACd;IACA,wBAAwB;QACpB,MAAM;QACN,UAAU;YACN;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;SACH;QACD,UAAU;IACd;IACA,4BAA4B;QACxB,MAAM;QACN,UAAU;IACd;IACA,sBAAsB;QAClB,MAAM;QACN,UAAU;YACN;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;SACH;QACD,UAAU;IACd;IACA,4BAA4B;QACxB,MAAM;QACN,UAAU;IACd;IACA,aAAa;QACT,MAAM;QACN,UAAU;IACd;IACA,eAAe;QACX,MAAM;QACN,UAAU;YACN;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;SACH;QACD,UAAU;IACd;IACA,sBAAsB;QAClB,MAAM;QACN,UAAU;IACd;IACA,mBAAmB;QACf,MAAM;QACN,UAAU;YACN;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;SACH;QACD,UAAU;IACd;IACA,aAAa;QACT,MAAM;QACN,UAAU;YACN;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;SACH;QACD,UAAU;IACd;IACA,uBAAuB;QACnB,MAAM;QACN,UAAU;IACd;IACA,kBAAkB;QACd,MAAM;QACN,UAAU;IACd;IACA,iBAAiB;QACb,MAAM;QACN,UAAU;IACd;IACA,gCAAgC;QAC5B,MAAM;QACN,UAAU;IACd;IACA,kBAAkB;QACd,MAAM;QACN,UAAU;IACd;IACA,wBAAwB;QACpB,MAAM;QACN,UAAU;YACN;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;SACH;QACD,UAAU;IACd;IACA,sBAAsB;QAClB,MAAM;QACN,UAAU;QACV,gBAAgB;IACpB;IACA,4BAA4B;QACxB,MAAM;QACN,UAAU;IACd;IACA,oBAAoB;QAChB,MAAM;QACN,UAAU;IACd;IACA,wBAAwB;QACpB,MAAM;QACN,UAAU;YACN;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;SACH;QACD,UAAU;IACd;IACA,oBAAoB;QAChB,MAAM;QACN,UAAU;YACN;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;SACH;QACD,UAAU;IACd;IACA,sBAAsB;QAClB,MAAM;QACN,UAAU;YACN;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;SACH;QACD,UAAU;IACd;IACA,iBAAiB;QACb,MAAM;QACN,UAAU;IACd;IACA,iBAAiB;QACb,MAAM;QACN,UAAU;YACN;gBACI,MAAM;gBACN,MAAM;YACV;SACH;QACD,UAAU;IACd;IACA,kBAAkB;QACd,MAAM;QACN,UAAU;YACN;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;SACH;QACD,UAAU;IACd;IACA,kBAAkB;QACd,MAAM;QACN,UAAU;IACd;IACA,kCAAkC;QAC9B,MAAM;QACN,UAAU;IACd;IACA,wBAAwB;QACpB,MAAM;QACN,UAAU;IACd;IACA,0BAA0B;QACtB,MAAM;QACN,UAAU;IACd;IACA,UAAU;QACN,MAAM;QACN,UAAU;QACV,UAAU;YACN;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;SACH;IACL;IACA,0BAA0B;QACtB,MAAM;QACN,UAAU;QACV,UAAU;YACN;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;SACH;IACL;IACA,sBAAsB;QAClB,MAAM;QACN,UAAU;QACV,UAAU;YACN;gBACI,MAAM;gBACN,MAAM;YACV;SACH;IACL;IACA,mBAAmB;QACf,MAAM;QACN,UAAU;QACV,UAAU;YACN;gBACI,MAAM;gBACN,MAAM;YACV;SACH;QACD,cAAc;IAClB;IACA,iBAAiB;QACb,MAAM;QACN,UAAU;QACV,cAAc;IAClB;IACA,mBAAmB;QACf,MAAM;QACN,UAAU;YACN;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;SACH;QACD,UAAU;QACV,cAAc;IAClB;IACA,gBAAgB;QACZ,MAAM;QACN,UAAU;IACd;IACA,kBAAkB;QACd,MAAM;QACN,UAAU;YACN;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;SACH;QACD,UAAU;QACV,cAAc;IAClB;IACA,2BAA2B;QACvB,MAAM;QACN,UAAU;QACV,UAAU;YACN;gBACI,MAAM;gBACN,MAAM;YACV;YACA;gBACI,MAAM;gBACN,MAAM;YACV;SACH;IACL;IACA,iBAAiB;QACb,MAAM;QACN,UAAU;IACd;IACA,sBAAsB;QAClB,MAAM;QACN,UAAU;IACd;IACA,6BAA6B;QACzB,MAAM;QACN,UAAU;YACN;gBACI,MAAM;gBACN,MAAM;YACV;SACH;QACD,UAAU;IACd;IACA,+BAA+B;QAC3B,MAAM;QACN,UAAU;YACN;gBACI,MAAM;gBACN,MAAM;YACV;SACH;QACD,UAAU;QACV,gBAAgB;IACpB;IACA,kCAAkC;QAC9B,MAAM;QACN,UAAU;IACd;IACA,YAAY;QACR,MAAM;QACN,UAAU;IACd;IACA,mBAAmB;QACf,MAAM;QACN,UAAU;IACd;IACA,8BAA8B;QAC1B,MAAM;QACN,UAAU;IACd;IACA,cAAc;QACV,MAAM;QACN,UAAU;IACd;IACA,eAAe;QACX,MAAM;QACN,UAAU;IACd;IACA,4BAA4B;QACxB,MAAM;QACN,UAAU;IACd;IACA,sBAAsB;QAClB,MAAM;QACN,UAAU;QACV,gBAAgB;IACpB;IACA,sBAAsB;QAClB,MAAM;QACN,UAAU;YACN;gBACI,MAAM;gBACN,MAAM;YACV;SACH;QACD,UAAU;QACV,gBAAgB;IACpB;IACA,6BAA6B;QACzB,MAAM;QACN,UAAU;IACd;IACA,cAAc;QACV,MAAM;QACN,UAAU;IACd;IACA,kBAAkB;QACd,MAAM;QACN,UAAU;QACV,gBAAgB;IACpB;IACA,OAAO;QACH,MAAM;QACN,UAAU;QACV,cAAc;QACd,gBAAgB;IACpB;AACJ;AACO,MAAM,iBAAiB,OAAO,IAAI,CAAC;AACnC,MAAM,gBAAgB,OAAO,MAAM,CAAC,eACtC,OAAO,CAAC,CAAC,OAAU,cAAc,OAAO,KAAK,QAAQ,GAAG,EAAE,EAC1D,GAAG,CAAC,CAAC,IAAM,EAAE,IAAI;AACf,MAAM,qBAAqB,IAAI,IAAI","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 1802, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/any-to-any/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"A dataset with multiple modality input and output pairs.\",\n            id: \"PKU-Alignment/align-anything\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                filename: \"any-to-any-input.jpg\",\n                type: \"img\",\n            },\n            {\n                label: \"Text Prompt\",\n                content: \"What is the significance of this place?\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                label: \"Generated Text\",\n                content: \"The place in the picture is Osaka Castle, located in Osaka, Japan. Osaka Castle is a historic castle that was originally built in the 16th century by Toyotomi Hideyoshi, a powerful warlord of the time. It is one of the most famous landmarks in Osaka and is known for its distinctive white walls and black roof tiles. The castle has been rebuilt several times over the centuries and is now a popular tourist attraction, offering visitors a glimpse into Japan's rich history and culture.\",\n                type: \"text\",\n            },\n            {\n                filename: \"any-to-any-output.wav\",\n                type: \"audio\",\n            },\n        ],\n    },\n    metrics: [],\n    models: [\n        {\n            description: \"Strong model that can take in video, audio, image, text and output text and natural speech.\",\n            id: \"Qwen/Qwen2.5-Omni-7B\",\n        },\n        {\n            description: \"Robust model that can take in image and text and generate image and text.\",\n            id: \"OmniGen2/OmniGen2\",\n        },\n        {\n            description: \"Any-to-any model with speech, video, audio, image and text understanding capabilities.\",\n            id: \"openbmb/MiniCPM-o-2_6\",\n        },\n        {\n            description: \"A model that can understand image and text and generate image and text.\",\n            id: \"ByteDance-Seed/BAGEL-7B-MoT\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application to chat with an any-to-any (image & text) model.\",\n            id: \"OmniGen2/OmniGen2\",\n        },\n    ],\n    summary: \"Any-to-any models can understand two or more modalities and output two or more modalities.\",\n    widgetModels: [],\n    youtubeId: \"\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,UAAU;gBACV,MAAM;YACV;YACA;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;YACA;gBACI,UAAU;gBACV,MAAM;YACV;SACH;IACL;IACA,SAAS,EAAE;IACX,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc,EAAE;IAChB,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 1871, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/audio-classification/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"A benchmark of 10 different audio tasks.\",\n            id: \"s3prl/superb\",\n        },\n        {\n            description: \"A dataset of YouTube clips and their sound categories.\",\n            id: \"agkphysics/AudioSet\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                filename: \"audio.wav\",\n                type: \"audio\",\n            },\n        ],\n        outputs: [\n            {\n                data: [\n                    {\n                        label: \"Up\",\n                        score: 0.2,\n                    },\n                    {\n                        label: \"Down\",\n                        score: 0.8,\n                    },\n                ],\n                type: \"chart\",\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"\",\n            id: \"accuracy\",\n        },\n        {\n            description: \"\",\n            id: \"recall\",\n        },\n        {\n            description: \"\",\n            id: \"precision\",\n        },\n        {\n            description: \"\",\n            id: \"f1\",\n        },\n    ],\n    models: [\n        {\n            description: \"An easy-to-use model for command recognition.\",\n            id: \"speechbrain/google_speech_command_xvector\",\n        },\n        {\n            description: \"An emotion recognition model.\",\n            id: \"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\",\n        },\n        {\n            description: \"A language identification model.\",\n            id: \"facebook/mms-lid-126\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application that can classify music into different genre.\",\n            id: \"kurianbenoy/audioclassification\",\n        },\n    ],\n    summary: \"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.\",\n    widgetModels: [\"MIT/ast-finetuned-audioset-10-10-0.4593\"],\n    youtubeId: \"KWwzcmG98Ds\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,UAAU;gBACV,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,MAAM;oBACF;wBACI,OAAO;wBACP,OAAO;oBACX;oBACA;wBACI,OAAO;wBACP,OAAO;oBACX;iBACH;gBACD,MAAM;YACV;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAA0C;IACzD,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 1958, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/audio-text-to-text/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"A dataset containing audio conversations with question–answer pairs.\",\n            id: \"nvidia/AF-Think\",\n        },\n        {\n            description: \"A more advanced and comprehensive dataset that contains characteristics of the audio as well\",\n            id: \"tsinghua-ee/QualiSpeech\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                filename: \"audio.wav\",\n                type: \"audio\",\n            },\n            {\n                label: \"Text Prompt\",\n                content: \"What is the gender of the speaker?\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                label: \"Generated Text\",\n                content: \"The gender of the speaker is female.\",\n                type: \"text\",\n            },\n        ],\n    },\n    metrics: [],\n    models: [\n        {\n            description: \"A lightweight model that has capabilities of taking both audio and text as inputs and generating responses.\",\n            id: \"fixie-ai/ultravox-v0_5-llama-3_2-1b\",\n        },\n        {\n            description: \"A multimodal model that supports voice chat and audio analysis.\",\n            id: \"Qwen/Qwen2-Audio-7B-Instruct\",\n        },\n        {\n            description: \"A model for audio understanding, speech translation, and transcription.\",\n            id: \"mistralai/Voxtral-Small-24B-2507\",\n        },\n        {\n            description: \"A new model capable of audio question answering and reasoning.\",\n            id: \"nvidia/audio-flamingo-3\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"A space that takes input as both audio and text and generates answers.\",\n            id: \"iamomtiwari/ATTT\",\n        },\n        {\n            description: \"A web application that demonstrates chatting with the Qwen2Audio Model.\",\n            id: \"freddyaboulton/talk-to-qwen-webrtc\",\n        },\n    ],\n    summary: \"Audio-text-to-text models take both an audio clip and a text prompt as input, and generate natural language text as output. These models can answer questions about spoken content, summarize meetings, analyze music, or interpret speech beyond simple transcription. They are useful for applications that combine speech understanding with reasoning or conversation.\",\n    widgetModels: [],\n    youtubeId: \"\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,UAAU;gBACV,MAAM;YACV;YACA;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;IACL;IACA,SAAS,EAAE;IACX,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc,EAAE;IAChB,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 2031, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/audio-to-audio/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"512-element X-vector embeddings of speakers from CMU ARCTIC dataset.\",\n            id: \"Matthijs/cmu-arctic-xvectors\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                filename: \"input.wav\",\n                type: \"audio\",\n            },\n        ],\n        outputs: [\n            {\n                filename: \"label-0.wav\",\n                type: \"audio\",\n            },\n            {\n                filename: \"label-1.wav\",\n                type: \"audio\",\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"The Signal-to-Noise ratio is the relationship between the target signal level and the background noise level. It is calculated as the logarithm of the target signal divided by the background noise, in decibels.\",\n            id: \"snri\",\n        },\n        {\n            description: \"The Signal-to-Distortion ratio is the relationship between the target signal and the sum of noise, interference, and artifact errors\",\n            id: \"sdri\",\n        },\n    ],\n    models: [\n        {\n            description: \"A speech enhancement model.\",\n            id: \"ResembleAI/resemble-enhance\",\n        },\n        {\n            description: \"A model that can change the voice in a speech recording.\",\n            id: \"microsoft/speecht5_vc\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application for speech separation.\",\n            id: \"younver/speechbrain-speech-separation\",\n        },\n        {\n            description: \"An application for audio style transfer.\",\n            id: \"nakas/audio-diffusion_style_transfer\",\n        },\n    ],\n    summary: \"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.\",\n    widgetModels: [\"speechbrain/sepformer-wham\"],\n    youtubeId: \"iohj7nCCYoM\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,UAAU;gBACV,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,UAAU;gBACV,MAAM;YACV;YACA;gBACI,UAAU;gBACV,MAAM;YACV;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAA6B;IAC5C,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 2101, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/automatic-speech-recognition/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"31,175 hours of multilingual audio-text dataset in 108 languages.\",\n            id: \"mozilla-foundation/common_voice_17_0\",\n        },\n        {\n            description: \"Multilingual and diverse audio dataset with 101k hours of audio.\",\n            id: \"amphion/Emilia-Dataset\",\n        },\n        {\n            description: \"A dataset with 44.6k hours of English speaker data and 6k hours of other language speakers.\",\n            id: \"parler-tts/mls_eng\",\n        },\n        {\n            description: \"A multilingual audio dataset with 370K hours of audio.\",\n            id: \"espnet/yodas\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                filename: \"input.flac\",\n                type: \"audio\",\n            },\n        ],\n        outputs: [\n            {\n                /// GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES I\n                label: \"Transcript\",\n                content: \"Going along slushy country roads and speaking to damp audiences in...\",\n                type: \"text\",\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"\",\n            id: \"wer\",\n        },\n        {\n            description: \"\",\n            id: \"cer\",\n        },\n    ],\n    models: [\n        {\n            description: \"A powerful ASR model by OpenAI.\",\n            id: \"openai/whisper-large-v3\",\n        },\n        {\n            description: \"A good generic speech model by MetaAI for fine-tuning.\",\n            id: \"facebook/w2v-bert-2.0\",\n        },\n        {\n            description: \"An end-to-end model that performs ASR and Speech Translation by MetaAI.\",\n            id: \"facebook/seamless-m4t-v2-large\",\n        },\n        {\n            description: \"A powerful multilingual ASR and Speech Translation model by Nvidia.\",\n            id: \"nvidia/canary-1b\",\n        },\n        {\n            description: \"Powerful speaker diarization model.\",\n            id: \"pyannote/speaker-diarization-3.1\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"A powerful general-purpose speech recognition application.\",\n            id: \"hf-audio/whisper-large-v3\",\n        },\n        {\n            description: \"Latest ASR model from Useful Sensors.\",\n            id: \"mrfakename/Moonshinex\",\n        },\n        {\n            description: \"A high quality speech and text translation model by Meta.\",\n            id: \"facebook/seamless_m4t\",\n        },\n        {\n            description: \"A powerful multilingual ASR and Speech Translation model by Nvidia\",\n            id: \"nvidia/canary-1b\",\n        },\n    ],\n    summary: \"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.\",\n    widgetModels: [\"openai/whisper-large-v3\"],\n    youtubeId: \"TksaY_FDgnk\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,UAAU;gBACV,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,qEAAqE;gBACrE,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAA0B;IACzC,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 2201, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/document-question-answering/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"Largest document understanding dataset.\",\n            id: \"HuggingFaceM4/Docmatix\",\n        },\n        {\n            description: \"Dataset from the 2020 DocVQA challenge. The documents are taken from the UCSF Industry Documents Library.\",\n            id: \"eliolio/docvqa\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                label: \"Question\",\n                content: \"What is the idea behind the consumer relations efficiency team?\",\n                type: \"text\",\n            },\n            {\n                filename: \"document-question-answering-input.png\",\n                type: \"img\",\n            },\n        ],\n        outputs: [\n            {\n                label: \"Answer\",\n                content: \"Balance cost efficiency with quality customer service\",\n                type: \"text\",\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"The evaluation metric for the DocVQA challenge is the Average Normalized Levenshtein Similarity (ANLS). This metric is flexible to character regognition errors and compares the predicted answer with the ground truth answer.\",\n            id: \"anls\",\n        },\n        {\n            description: \"Exact Match is a metric based on the strict character match of the predicted answer and the right answer. For answers predicted correctly, the Exact Match will be 1. Even if only one character is different, Exact Match will be 0\",\n            id: \"exact-match\",\n        },\n    ],\n    models: [\n        {\n            description: \"A robust document question answering model.\",\n            id: \"impira/layoutlm-document-qa\",\n        },\n        {\n            description: \"A document question answering model specialized in invoices.\",\n            id: \"impira/layoutlm-invoices\",\n        },\n        {\n            description: \"A special model for OCR-free document question answering.\",\n            id: \"microsoft/udop-large\",\n        },\n        {\n            description: \"A powerful model for document question answering.\",\n            id: \"google/pix2struct-docvqa-large\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"A robust document question answering application.\",\n            id: \"impira/docquery\",\n        },\n        {\n            description: \"An application that can answer questions from invoices.\",\n            id: \"impira/invoices\",\n        },\n        {\n            description: \"An application to compare different document question answering models.\",\n            id: \"merve/compare_docvqa_models\",\n        },\n    ],\n    summary: \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\",\n    widgetModels: [\"impira/layoutlm-invoices\"],\n    youtubeId: \"\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;YACA;gBACI,UAAU;gBACV,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAA2B;IAC1C,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 2289, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/feature-extraction/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"Wikipedia dataset containing cleaned articles of all languages. Can be used to train `feature-extraction` models.\",\n            id: \"wikipedia\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                label: \"Input\",\n                content: \"India, officially the Republic of India, is a country in South Asia.\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                table: [\n                    [\"Dimension 1\", \"Dimension 2\", \"Dimension 3\"],\n                    [\"2.583383083343506\", \"2.757075071334839\", \"0.9023529887199402\"],\n                    [\"8.29393482208252\", \"1.1071064472198486\", \"2.03399395942688\"],\n                    [\"-0.7754912972450256\", \"-1.647324562072754\", \"-0.6113331913948059\"],\n                    [\"0.07087723910808563\", \"1.5942802429199219\", \"1.4610432386398315\"],\n                ],\n                type: \"tabular\",\n            },\n        ],\n    },\n    metrics: [],\n    models: [\n        {\n            description: \"A powerful feature extraction model for natural language processing tasks.\",\n            id: \"thenlper/gte-large\",\n        },\n        {\n            description: \"A strong feature extraction model for retrieval.\",\n            id: \"Alibaba-NLP/gte-Qwen1.5-7B-instruct\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"A leaderboard to rank text feature extraction models based on a benchmark.\",\n            id: \"mteb/leaderboard\",\n        },\n        {\n            description: \"A leaderboard to rank best feature extraction models based on human feedback.\",\n            id: \"mteb/arena\",\n        },\n    ],\n    summary: \"Feature extraction is the task of extracting features learnt in a model.\",\n    widgetModels: [\"facebook/bart-base\"],\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,OAAO;oBACH;wBAAC;wBAAe;wBAAe;qBAAc;oBAC7C;wBAAC;wBAAqB;wBAAqB;qBAAqB;oBAChE;wBAAC;wBAAoB;wBAAsB;qBAAmB;oBAC9D;wBAAC;wBAAuB;wBAAsB;qBAAsB;oBACpE;wBAAC;wBAAuB;wBAAsB;qBAAqB;iBACtE;gBACD,MAAM;YACV;SACH;IACL;IACA,SAAS,EAAE;IACX,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAAqB;AACxC;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 2372, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/fill-mask/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"A common dataset that is used to train models for many languages.\",\n            id: \"wikipedia\",\n        },\n        {\n            description: \"A large English dataset with text crawled from the web.\",\n            id: \"c4\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                label: \"Input\",\n                content: \"The <mask> barked at me\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                type: \"chart\",\n                data: [\n                    {\n                        label: \"wolf\",\n                        score: 0.487,\n                    },\n                    {\n                        label: \"dog\",\n                        score: 0.061,\n                    },\n                    {\n                        label: \"cat\",\n                        score: 0.058,\n                    },\n                    {\n                        label: \"fox\",\n                        score: 0.047,\n                    },\n                    {\n                        label: \"squirrel\",\n                        score: 0.025,\n                    },\n                ],\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"Cross Entropy is a metric that calculates the difference between two probability distributions. Each probability distribution is the distribution of predicted words\",\n            id: \"cross_entropy\",\n        },\n        {\n            description: \"Perplexity is the exponential of the cross-entropy loss. It evaluates the probabilities assigned to the next word by the model. Lower perplexity indicates better performance\",\n            id: \"perplexity\",\n        },\n    ],\n    models: [\n        {\n            description: \"State-of-the-art masked language model.\",\n            id: \"answerdotai/ModernBERT-large\",\n        },\n        {\n            description: \"A multilingual model trained on 100 languages.\",\n            id: \"FacebookAI/xlm-roberta-base\",\n        },\n    ],\n    spaces: [],\n    summary: \"Masked language modeling is the task of masking some of the words in a sentence and predicting which words should replace those masks. These models are useful when we want to get a statistical understanding of the language in which the model is trained in.\",\n    widgetModels: [\"distilroberta-base\"],\n    youtubeId: \"mqElG5QJWUg\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,MAAM;gBACN,MAAM;oBACF;wBACI,OAAO;wBACP,OAAO;oBACX;oBACA;wBACI,OAAO;wBACP,OAAO;oBACX;oBACA;wBACI,OAAO;wBACP,OAAO;oBACX;oBACA;wBACI,OAAO;wBACP,OAAO;oBACX;oBACA;wBACI,OAAO;wBACP,OAAO;oBACX;iBACH;YACL;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ,EAAE;IACV,SAAS;IACT,cAAc;QAAC;KAAqB;IACpC,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 2455, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/image-classification/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            // TODO write proper description\n            description: \"Benchmark dataset used for image classification with images that belong to 100 classes.\",\n            id: \"cifar100\",\n        },\n        {\n            // TODO write proper description\n            description: \"Dataset consisting of images of garments.\",\n            id: \"fashion_mnist\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                filename: \"image-classification-input.jpeg\",\n                type: \"img\",\n            },\n        ],\n        outputs: [\n            {\n                type: \"chart\",\n                data: [\n                    {\n                        label: \"Egyptian cat\",\n                        score: 0.514,\n                    },\n                    {\n                        label: \"Tabby cat\",\n                        score: 0.193,\n                    },\n                    {\n                        label: \"Tiger cat\",\n                        score: 0.068,\n                    },\n                ],\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"\",\n            id: \"accuracy\",\n        },\n        {\n            description: \"\",\n            id: \"recall\",\n        },\n        {\n            description: \"\",\n            id: \"precision\",\n        },\n        {\n            description: \"\",\n            id: \"f1\",\n        },\n    ],\n    models: [\n        {\n            description: \"A strong image classification model.\",\n            id: \"google/vit-base-patch16-224\",\n        },\n        {\n            description: \"A robust image classification model.\",\n            id: \"facebook/deit-base-distilled-patch16-224\",\n        },\n        {\n            description: \"A strong image classification model.\",\n            id: \"facebook/convnext-large-224\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"A leaderboard to evaluate different image classification models.\",\n            id: \"timm/leaderboard\",\n        },\n    ],\n    summary: \"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.\",\n    widgetModels: [\"google/vit-base-patch16-224\"],\n    youtubeId: \"tjAIM7BOYhw\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,gCAAgC;YAChC,aAAa;YACb,IAAI;QACR;QACA;YACI,gCAAgC;YAChC,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,UAAU;gBACV,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,MAAM;gBACN,MAAM;oBACF;wBACI,OAAO;wBACP,OAAO;oBACX;oBACA;wBACI,OAAO;wBACP,OAAO;oBACX;oBACA;wBACI,OAAO;wBACP,OAAO;oBACX;iBACH;YACL;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAA8B;IAC7C,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 2548, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/image-feature-extraction/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"ImageNet-1K is a image classification dataset in which images are used to train image-feature-extraction models.\",\n            id: \"imagenet-1k\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                filename: \"mask-generation-input.png\",\n                type: \"img\",\n            },\n        ],\n        outputs: [\n            {\n                table: [\n                    [\"Dimension 1\", \"Dimension 2\", \"Dimension 3\"],\n                    [\"0.21236686408519745\", \"1.0919708013534546\", \"0.8512550592422485\"],\n                    [\"0.809657871723175\", \"-0.18544459342956543\", \"-0.7851548194885254\"],\n                    [\"1.3103108406066895\", \"-0.2479034662246704\", \"-0.9107287526130676\"],\n                    [\"1.8536205291748047\", \"-0.36419737339019775\", \"0.09717650711536407\"],\n                ],\n                type: \"tabular\",\n            },\n        ],\n    },\n    metrics: [],\n    models: [\n        {\n            description: \"A powerful image feature extraction model.\",\n            id: \"timm/vit_large_patch14_dinov2.lvd142m\",\n        },\n        {\n            description: \"A strong image feature extraction model.\",\n            id: \"nvidia/MambaVision-T-1K\",\n        },\n        {\n            description: \"A robust image feature extraction model.\",\n            id: \"facebook/dino-vitb16\",\n        },\n        {\n            description: \"Cutting-edge image feature extraction model.\",\n            id: \"apple/aimv2-large-patch14-336-distilled\",\n        },\n        {\n            description: \"Strong image feature extraction model that can be used on images and documents.\",\n            id: \"OpenGVLab/InternViT-6B-448px-V1-2\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"A leaderboard to evaluate different image-feature-extraction models on classification performances\",\n            id: \"timm/leaderboard\",\n        },\n    ],\n    summary: \"Image feature extraction is the task of extracting features learnt in a computer vision model.\",\n    widgetModels: [],\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,UAAU;gBACV,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,OAAO;oBACH;wBAAC;wBAAe;wBAAe;qBAAc;oBAC7C;wBAAC;wBAAuB;wBAAsB;qBAAqB;oBACnE;wBAAC;wBAAqB;wBAAwB;qBAAsB;oBACpE;wBAAC;wBAAsB;wBAAuB;qBAAsB;oBACpE;wBAAC;wBAAsB;wBAAwB;qBAAsB;iBACxE;gBACD,MAAM;YACV;SACH;IACL;IACA,SAAS,EAAE;IACX,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc,EAAE;AACpB;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 2636, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/image-to-image/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"Synthetic dataset, for image relighting\",\n            id: \"VIDIT\",\n        },\n        {\n            description: \"Multiple images of celebrities, used for facial expression translation\",\n            id: \"huggan/CelebA-faces\",\n        },\n        {\n            description: \"12M image-caption pairs.\",\n            id: \"Spawning/PD12M\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                filename: \"image-to-image-input.jpeg\",\n                type: \"img\",\n            },\n        ],\n        outputs: [\n            {\n                filename: \"image-to-image-output.png\",\n                type: \"img\",\n            },\n        ],\n    },\n    isPlaceholder: false,\n    metrics: [\n        {\n            description: \"Peak Signal to Noise Ratio (PSNR) is an approximation of the human perception, considering the ratio of the absolute intensity with respect to the variations. Measured in dB, a high value indicates a high fidelity.\",\n            id: \"PSNR\",\n        },\n        {\n            description: \"Structural Similarity Index (SSIM) is a perceptual metric which compares the luminance, contrast and structure of two images. The values of SSIM range between -1 and 1, and higher values indicate closer resemblance to the original image.\",\n            id: \"SSIM\",\n        },\n        {\n            description: \"Inception Score (IS) is an analysis of the labels predicted by an image classification model when presented with a sample of the generated images.\",\n            id: \"IS\",\n        },\n    ],\n    models: [\n        {\n            description: \"An image-to-image model to improve image resolution.\",\n            id: \"fal/AuraSR-v2\",\n        },\n        {\n            description: \"Powerful image editing model.\",\n            id: \"black-forest-labs/FLUX.1-Kontext-dev\",\n        },\n        {\n            description: \"Virtual try-on model.\",\n            id: \"yisol/IDM-VTON\",\n        },\n        {\n            description: \"Image re-lighting model.\",\n            id: \"kontext-community/relighting-kontext-dev-lora-v3\",\n        },\n        {\n            description: \"Strong model for inpainting and outpainting.\",\n            id: \"black-forest-labs/FLUX.1-Fill-dev\",\n        },\n        {\n            description: \"Strong model for image editing using depth maps.\",\n            id: \"black-forest-labs/FLUX.1-Depth-dev-lora\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"Image editing application.\",\n            id: \"black-forest-labs/FLUX.1-Kontext-Dev\",\n        },\n        {\n            description: \"Image relighting application.\",\n            id: \"lllyasviel/iclight-v2-vary\",\n        },\n        {\n            description: \"An application for image upscaling.\",\n            id: \"jasperai/Flux.1-dev-Controlnet-Upscaler\",\n        },\n    ],\n    summary: \"Image-to-image is the task of transforming an input image through a variety of possible manipulations and enhancements, such as super-resolution, image inpainting, colorization, and more.\",\n    widgetModels: [\"Qwen/Qwen-Image\"],\n    youtubeId: \"\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,UAAU;gBACV,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,UAAU;gBACV,MAAM;YACV;SACH;IACL;IACA,eAAe;IACf,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAAkB;IACjC,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 2735, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/image-to-text/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            // TODO write proper description\n            description: \"Dataset from 12M image-text of Reddit\",\n            id: \"red_caps\",\n        },\n        {\n            // TODO write proper description\n            description: \"Dataset from 3.3M images of Google\",\n            id: \"datasets/conceptual_captions\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                filename: \"savanna.jpg\",\n                type: \"img\",\n            },\n        ],\n        outputs: [\n            {\n                label: \"Detailed description\",\n                content: \"a herd of giraffes and zebras grazing in a field\",\n                type: \"text\",\n            },\n        ],\n    },\n    metrics: [],\n    models: [\n        {\n            description: \"Strong OCR model.\",\n            id: \"allenai/olmOCR-7B-0725\",\n        },\n        {\n            description: \"Powerful image captioning model.\",\n            id: \"fancyfeast/llama-joycaption-beta-one-hf-llava\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"SVG generator app from images.\",\n            id: \"multimodalart/OmniSVG-3B\",\n        },\n        {\n            description: \"An application that converts documents to markdown.\",\n            id: \"numind/NuMarkdown-8B-Thinking\",\n        },\n        {\n            description: \"An application that can caption images.\",\n            id: \"fancyfeast/joy-caption-beta-one\",\n        },\n    ],\n    summary: \"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.\",\n    widgetModels: [\"Salesforce/blip-image-captioning-large\"],\n    youtubeId: \"\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,gCAAgC;YAChC,aAAa;YACb,IAAI;QACR;QACA;YACI,gCAAgC;YAChC,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,UAAU;gBACV,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;IACL;IACA,SAAS,EAAE;IACX,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAAyC;IACxD,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 2803, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/image-text-to-text/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"Instructions composed of image and text.\",\n            id: \"liuhaotian/LLaVA-Instruct-150K\",\n        },\n        {\n            description: \"Collection of image-text pairs on scientific topics.\",\n            id: \"DAMO-NLP-SG/multimodal_textbook\",\n        },\n        {\n            description: \"A collection of datasets made for model fine-tuning.\",\n            id: \"HuggingFaceM4/the_cauldron\",\n        },\n        {\n            description: \"Screenshots of websites with their HTML/CSS codes.\",\n            id: \"HuggingFaceM4/WebSight\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                filename: \"image-text-to-text-input.png\",\n                type: \"img\",\n            },\n            {\n                label: \"Text Prompt\",\n                content: \"Describe the position of the bee in detail.\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                label: \"Answer\",\n                content: \"The bee is sitting on a pink flower, surrounded by other flowers. The bee is positioned in the center of the flower, with its head and front legs sticking out.\",\n                type: \"text\",\n            },\n        ],\n    },\n    metrics: [],\n    models: [\n        {\n            description: \"Small and efficient yet powerful vision language model.\",\n            id: \"HuggingFaceTB/SmolVLM-Instruct\",\n        },\n        {\n            description: \"Cutting-edge reasoning vision language model.\",\n            id: \"zai-org/GLM-4.5V\",\n        },\n        {\n            description: \"Cutting-edge small vision language model to convert documents to text.\",\n            id: \"rednote-hilab/dots.ocr\",\n        },\n        {\n            description: \"Small yet powerful model.\",\n            id: \"Qwen/Qwen2.5-VL-3B-Instruct\",\n        },\n        {\n            description: \"Image-text-to-text model with agentic capabilities.\",\n            id: \"microsoft/Magma-8B\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"Leaderboard to evaluate vision language models.\",\n            id: \"opencompass/open_vlm_leaderboard\",\n        },\n        {\n            description: \"An application that compares object detection capabilities of different vision language models.\",\n            id: \"sergiopaniego/vlm_object_understanding\",\n        },\n        {\n            description: \"An application to compare different OCR models.\",\n            id: \"prithivMLmods/Multimodal-OCR\",\n        },\n    ],\n    summary: \"Image-text-to-text models take in an image and text prompt and output text. These models are also called vision-language models, or VLMs. The difference from image-to-text models is that these models take an additional text input, not restricting the model to certain use cases like image captioning, and may also be trained to accept a conversation as input.\",\n    widgetModels: [\"zai-org/GLM-4.5V\"],\n    youtubeId: \"IoGaGfU1CIg\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,UAAU;gBACV,MAAM;YACV;YACA;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;IACL;IACA,SAAS,EAAE;IACX,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAAmB;IAClC,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 2894, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/image-segmentation/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"Scene segmentation dataset.\",\n            id: \"scene_parse_150\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                filename: \"image-segmentation-input.jpeg\",\n                type: \"img\",\n            },\n        ],\n        outputs: [\n            {\n                filename: \"image-segmentation-output.png\",\n                type: \"img\",\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"Average Precision (AP) is the Area Under the PR Curve (AUC-PR). It is calculated for each semantic class separately\",\n            id: \"Average Precision\",\n        },\n        {\n            description: \"Mean Average Precision (mAP) is the overall average of the AP values\",\n            id: \"Mean Average Precision\",\n        },\n        {\n            description: \"Intersection over Union (IoU) is the overlap of segmentation masks. Mean IoU is the average of the IoU of all semantic classes\",\n            id: \"Mean Intersection over Union\",\n        },\n        {\n            description: \"APα is the Average Precision at the IoU threshold of a α value, for example, AP50 and AP75\",\n            id: \"APα\",\n        },\n    ],\n    models: [\n        {\n            // TO DO: write description\n            description: \"Solid panoptic segmentation model trained on COCO.\",\n            id: \"tue-mps/coco_panoptic_eomt_large_640\",\n        },\n        {\n            description: \"Background removal model.\",\n            id: \"briaai/RMBG-1.4\",\n        },\n        {\n            description: \"A multipurpose image segmentation model for high resolution images.\",\n            id: \"ZhengPeng7/BiRefNet\",\n        },\n        {\n            description: \"Powerful human-centric image segmentation model.\",\n            id: \"facebook/sapiens-seg-1b\",\n        },\n        {\n            description: \"Panoptic segmentation model trained on the COCO (common objects) dataset.\",\n            id: \"facebook/mask2former-swin-large-coco-panoptic\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"A semantic segmentation application that can predict unseen instances out of the box.\",\n            id: \"facebook/ov-seg\",\n        },\n        {\n            description: \"One of the strongest segmentation applications.\",\n            id: \"jbrinkma/segment-anything\",\n        },\n        {\n            description: \"A human-centric segmentation model.\",\n            id: \"facebook/sapiens-pose\",\n        },\n        {\n            description: \"An instance segmentation application to predict neuronal cell types from microscopy images.\",\n            id: \"rashmi/sartorius-cell-instance-segmentation\",\n        },\n        {\n            description: \"An application that segments videos.\",\n            id: \"ArtGAN/Segment-Anything-Video\",\n        },\n        {\n            description: \"An panoptic segmentation application built for outdoor environments.\",\n            id: \"segments/panoptic-segment-anything\",\n        },\n    ],\n    summary: \"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.\",\n    widgetModels: [\"nvidia/segformer-b0-finetuned-ade-512-512\"],\n    youtubeId: \"dKE8SIt9C-w\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,UAAU;gBACV,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,UAAU;gBACV,MAAM;YACV;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,2BAA2B;YAC3B,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAA4C;IAC3D,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 2997, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/image-to-video/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"A benchmark dataset for reference image controlled video generation.\",\n            id: \"ali-vilab/VACE-Benchmark\",\n        },\n        {\n            description: \"A dataset of video generation style preferences.\",\n            id: \"Rapidata/sora-video-generation-style-likert-scoring\",\n        },\n        {\n            description: \"A dataset with videos and captions throughout the videos.\",\n            id: \"BestWishYsh/ChronoMagic\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                filename: \"image-to-video-input.jpg\",\n                type: \"img\",\n            },\n            {\n                label: \"Optional Text Prompt\",\n                content: \"This penguin is dancing\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                filename: \"image-to-video-output.gif\",\n                type: \"img\",\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"Fréchet Video Distance (FVD) measures the perceptual similarity between the distributions of generated videos and a set of real videos, assessing overall visual quality and temporal coherence of the video generated from an input image.\",\n            id: \"fvd\",\n        },\n        {\n            description: \"CLIP Score measures the semantic similarity between a textual prompt (if provided alongside the input image) and the generated video frames. It evaluates how well the video's generated content and motion align with the textual description, conditioned on the initial image.\",\n            id: \"clip_score\",\n        },\n        {\n            description: \"First Frame Fidelity, often measured using LPIPS (Learned Perceptual Image Patch Similarity), PSNR, or SSIM, quantifies how closely the first frame of the generated video matches the input conditioning image.\",\n            id: \"lpips\",\n        },\n        {\n            description: \"Identity Preservation Score measures the consistency of identity (e.g., a person's face or a specific object's characteristics) between the input image and throughout the generated video frames, often calculated using features from specialized models like face recognition (e.g., ArcFace) or re-identification models.\",\n            id: \"identity_preservation\",\n        },\n        {\n            description: \"Motion Score evaluates the quality, realism, and temporal consistency of motion in the video generated from a static image. This can be based on optical flow analysis (e.g., smoothness, magnitude), consistency of object trajectories, or specific motion plausibility assessments.\",\n            id: \"motion_score\",\n        },\n    ],\n    models: [\n        {\n            description: \"LTX-Video, a 13B parameter model for high quality video generation\",\n            id: \"Lightricks/LTX-Video-0.9.7-dev\",\n        },\n        {\n            description: \"A 14B parameter model for reference image controlled video generation\",\n            id: \"Wan-AI/Wan2.1-VACE-14B\",\n        },\n        {\n            description: \"An image-to-video generation model using FramePack F1 methodology with Hunyuan-DiT architecture\",\n            id: \"lllyasviel/FramePack_F1_I2V_HY_20250503\",\n        },\n        {\n            description: \"A distilled version of the LTX-Video-0.9.7-dev model for faster inference\",\n            id: \"Lightricks/LTX-Video-0.9.7-distilled\",\n        },\n        {\n            description: \"An image-to-video generation model by Skywork AI, 14B parameters, producing 720p videos.\",\n            id: \"Skywork/SkyReels-V2-I2V-14B-720P\",\n        },\n        {\n            description: \"Image-to-video variant of Tencent's HunyuanVideo.\",\n            id: \"tencent/HunyuanVideo-I2V\",\n        },\n        {\n            description: \"A 14B parameter model for 720p image-to-video generation by Wan-AI.\",\n            id: \"Wan-AI/Wan2.1-I2V-14B-720P\",\n        },\n        {\n            description: \"A Diffusers version of the Wan2.1-I2V-14B-720P model for 720p image-to-video generation.\",\n            id: \"Wan-AI/Wan2.1-I2V-14B-720P-Diffusers\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application to generate videos fast.\",\n            id: \"Lightricks/ltx-video-distilled\",\n        },\n        {\n            description: \"Generate videos with the FramePack-F1\",\n            id: \"linoyts/FramePack-F1\",\n        },\n        {\n            description: \"Generate videos with the FramePack\",\n            id: \"lisonallen/framepack-i2v\",\n        },\n        {\n            description: \"Wan2.1 with CausVid LoRA\",\n            id: \"multimodalart/wan2-1-fast\",\n        },\n        {\n            description: \"A demo for Stable Video Diffusion\",\n            id: \"multimodalart/stable-video-diffusion\",\n        },\n    ],\n    summary: \"Image-to-video models take a still image as input and generate a video. These models can be guided by text prompts to influence the content and style of the output video.\",\n    widgetModels: [],\n    youtubeId: undefined,\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,UAAU;gBACV,MAAM;YACV;YACA;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,UAAU;gBACV,MAAM;YACV;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc,EAAE;IAChB,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 3122, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/mask-generation/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"Widely used benchmark dataset for multiple Vision tasks.\",\n            id: \"merve/coco2017\",\n        },\n        {\n            description: \"Medical Imaging dataset of the Human Brain for segmentation and mask generating tasks\",\n            id: \"rocky93/BraTS_segmentation\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                filename: \"mask-generation-input.png\",\n                type: \"img\",\n            },\n        ],\n        outputs: [\n            {\n                filename: \"mask-generation-output.png\",\n                type: \"img\",\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"IoU is used to measure the overlap between predicted mask and the ground truth mask.\",\n            id: \"Intersection over Union (IoU)\",\n        },\n    ],\n    models: [\n        {\n            description: \"Small yet powerful mask generation model.\",\n            id: \"Zigeng/SlimSAM-uniform-50\",\n        },\n        {\n            description: \"Very strong mask generation model.\",\n            id: \"facebook/sam2-hiera-large\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application that combines a mask generation model with a zero-shot object detection model for text-guided image segmentation.\",\n            id: \"merve/OWLSAM2\",\n        },\n        {\n            description: \"An application that compares the performance of a large and a small mask generation model.\",\n            id: \"merve/slimsam\",\n        },\n        {\n            description: \"An application based on an improved mask generation model.\",\n            id: \"SkalskiP/segment-anything-model-2\",\n        },\n        {\n            description: \"An application to remove objects from videos using mask generation models.\",\n            id: \"SkalskiP/SAM_and_ProPainter\",\n        },\n    ],\n    summary: \"Mask generation is the task of generating masks that identify a specific object or region of interest in a given image. Masks are often used in segmentation tasks, where they provide a precise way to isolate the object of interest for further processing or analysis.\",\n    widgetModels: [],\n    youtubeId: \"\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,UAAU;gBACV,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,UAAU;gBACV,MAAM;YACV;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc,EAAE;IAChB,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 3194, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/object-detection/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"Widely used benchmark dataset for multiple vision tasks.\",\n            id: \"merve/coco2017\",\n        },\n        {\n            description: \"Multi-task computer vision benchmark.\",\n            id: \"merve/pascal-voc\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                filename: \"object-detection-input.jpg\",\n                type: \"img\",\n            },\n        ],\n        outputs: [\n            {\n                filename: \"object-detection-output.jpg\",\n                type: \"img\",\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"The Average Precision (AP) metric is the Area Under the PR Curve (AUC-PR). It is calculated for each class separately\",\n            id: \"Average Precision\",\n        },\n        {\n            description: \"The Mean Average Precision (mAP) metric is the overall average of the AP values\",\n            id: \"Mean Average Precision\",\n        },\n        {\n            description: \"The APα metric is the Average Precision at the IoU threshold of a α value, for example, AP50 and AP75\",\n            id: \"APα\",\n        },\n    ],\n    models: [\n        {\n            description: \"Solid object detection model pre-trained on the COCO 2017 dataset.\",\n            id: \"facebook/detr-resnet-50\",\n        },\n        {\n            description: \"Accurate object detection model.\",\n            id: \"IDEA-Research/dab-detr-resnet-50\",\n        },\n        {\n            description: \"Fast and accurate object detection model.\",\n            id: \"PekingU/rtdetr_v2_r50vd\",\n        },\n        {\n            description: \"Object detection model for low-lying objects.\",\n            id: \"StephanST/WALDO30\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"Real-time object detection demo.\",\n            id: \"Roboflow/RF-DETR\",\n        },\n        {\n            description: \"An application that contains various object detection models to try from.\",\n            id: \"Gradio-Blocks/Object-Detection-With-DETR-and-YOLOS\",\n        },\n        {\n            description: \"A cutting-edge object detection application.\",\n            id: \"sunsmarterjieleaf/yolov12\",\n        },\n        {\n            description: \"An object tracking, segmentation and inpainting application.\",\n            id: \"VIPLab/Track-Anything\",\n        },\n        {\n            description: \"Very fast object tracking application based on object detection.\",\n            id: \"merve/RT-DETR-tracking-coco\",\n        },\n    ],\n    summary: \"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.\",\n    widgetModels: [\"facebook/detr-resnet-50\"],\n    youtubeId: \"WdAeKSOpxhw\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,UAAU;gBACV,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,UAAU;gBACV,MAAM;YACV;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAA0B;IACzC,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 3288, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/depth-estimation/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"NYU Depth V2 Dataset: Video dataset containing both RGB and depth sensor data.\",\n            id: \"sayakpaul/nyu_depth_v2\",\n        },\n        {\n            description: \"Monocular depth estimation benchmark based without noise and errors.\",\n            id: \"depth-anything/DA-2K\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                filename: \"depth-estimation-input.jpg\",\n                type: \"img\",\n            },\n        ],\n        outputs: [\n            {\n                filename: \"depth-estimation-output.png\",\n                type: \"img\",\n            },\n        ],\n    },\n    metrics: [],\n    models: [\n        {\n            description: \"Cutting-edge depth estimation model.\",\n            id: \"depth-anything/Depth-Anything-V2-Large\",\n        },\n        {\n            description: \"A strong monocular depth estimation model.\",\n            id: \"jingheya/lotus-depth-g-v1-0\",\n        },\n        {\n            description: \"A depth estimation model that predicts depth in videos.\",\n            id: \"tencent/DepthCrafter\",\n        },\n        {\n            description: \"A robust depth estimation model.\",\n            id: \"apple/DepthPro-hf\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application that predicts the depth of an image and then reconstruct the 3D model as voxels.\",\n            id: \"radames/dpt-depth-estimation-3d-voxels\",\n        },\n        {\n            description: \"An application for bleeding-edge depth estimation.\",\n            id: \"akhaliq/depth-pro\",\n        },\n        {\n            description: \"An application on cutting-edge depth estimation in videos.\",\n            id: \"tencent/DepthCrafter\",\n        },\n        {\n            description: \"A human-centric depth estimation application.\",\n            id: \"facebook/sapiens-depth\",\n        },\n    ],\n    summary: \"Depth estimation is the task of predicting depth of the objects present in an image.\",\n    widgetModels: [\"\"],\n    youtubeId: \"\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,UAAU;gBACV,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,UAAU;gBACV,MAAM;YACV;SACH;IACL;IACA,SAAS,EAAE;IACX,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAAG;IAClB,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 3365, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/placeholder/data.js"],"sourcesContent":["const taskData = {\n    datasets: [],\n    demo: {\n        inputs: [],\n        outputs: [],\n    },\n    isPlaceholder: true,\n    metrics: [],\n    models: [],\n    spaces: [],\n    summary: \"\",\n    widgetModels: [],\n    youtubeId: undefined,\n    /// If this is a subtask, link to the most general task ID\n    /// (eg, text-generation is the canonical ID of text-simplification)\n    canonicalId: undefined,\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU,EAAE;IACZ,MAAM;QACF,QAAQ,EAAE;QACV,SAAS,EAAE;IACf;IACA,eAAe;IACf,SAAS,EAAE;IACX,QAAQ,EAAE;IACV,QAAQ,EAAE;IACV,SAAS;IACT,cAAc,EAAE;IAChB,WAAW;IACX,0DAA0D;IAC1D,oEAAoE;IACpE,aAAa;AACjB;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 3391, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/reinforcement-learning/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"A curation of widely used datasets for Data Driven Deep Reinforcement Learning (D4RL)\",\n            id: \"edbeeching/decision_transformer_gym_replay\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                label: \"State\",\n                content: \"Red traffic light, pedestrians are about to pass.\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                label: \"Action\",\n                content: \"Stop the car.\",\n                type: \"text\",\n            },\n            {\n                label: \"Next State\",\n                content: \"Yellow light, pedestrians have crossed.\",\n                type: \"text\",\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"Accumulated reward across all time steps discounted by a factor that ranges between 0 and 1 and determines how much the agent optimizes for future relative to immediate rewards. Measures how good is the policy ultimately found by a given algorithm considering uncertainty over the future.\",\n            id: \"Discounted Total Reward\",\n        },\n        {\n            description: \"Average return obtained after running the policy for a certain number of evaluation episodes. As opposed to total reward, mean reward considers how much reward a given algorithm receives while learning.\",\n            id: \"Mean Reward\",\n        },\n        {\n            description: \"Measures how good a given algorithm is after a predefined time. Some algorithms may be guaranteed to converge to optimal behavior across many time steps. However, an agent that reaches an acceptable level of optimality after a given time horizon may be preferable to one that ultimately reaches optimality but takes a long time.\",\n            id: \"Level of Performance After Some Time\",\n        },\n    ],\n    models: [\n        {\n            description: \"A Reinforcement Learning model trained on expert data from the Gym Hopper environment\",\n            id: \"edbeeching/decision-transformer-gym-hopper-expert\",\n        },\n        {\n            description: \"A PPO agent playing seals/CartPole-v0 using the stable-baselines3 library and the RL Zoo.\",\n            id: \"HumanCompatibleAI/ppo-seals-CartPole-v0\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application for a cute puppy agent learning to catch a stick.\",\n            id: \"ThomasSimonini/Huggy\",\n        },\n        {\n            description: \"An application to play Snowball Fight with a reinforcement learning agent.\",\n            id: \"ThomasSimonini/SnowballFight\",\n        },\n    ],\n    summary: \"Reinforcement learning is the computational approach of learning from action by interacting with an environment through trial and error and receiving rewards (negative or positive) as feedback\",\n    widgetModels: [],\n    youtubeId: \"q0BiUn5LiBc\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;YACA;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc,EAAE;IAChB,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 3466, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/question-answering/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            // TODO write proper description\n            description: \"A famous question answering dataset based on English articles from Wikipedia.\",\n            id: \"squad_v2\",\n        },\n        {\n            // TODO write proper description\n            description: \"A dataset of aggregated anonymized actual queries issued to the Google search engine.\",\n            id: \"natural_questions\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                label: \"Question\",\n                content: \"Which name is also used to describe the Amazon rainforest in English?\",\n                type: \"text\",\n            },\n            {\n                label: \"Context\",\n                content: \"The Amazon rainforest, also known in English as Amazonia or the Amazon Jungle\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                label: \"Answer\",\n                content: \"Amazonia\",\n                type: \"text\",\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"Exact Match is a metric based on the strict character match of the predicted answer and the right answer. For answers predicted correctly, the Exact Match will be 1. Even if only one character is different, Exact Match will be 0\",\n            id: \"exact-match\",\n        },\n        {\n            description: \" The F1-Score metric is useful if we value both false positives and false negatives equally. The F1-Score is calculated on each word in the predicted sequence against the correct answer\",\n            id: \"f1\",\n        },\n    ],\n    models: [\n        {\n            description: \"A robust baseline model for most question answering domains.\",\n            id: \"deepset/roberta-base-squad2\",\n        },\n        {\n            description: \"Small yet robust model that can answer questions.\",\n            id: \"distilbert/distilbert-base-cased-distilled-squad\",\n        },\n        {\n            description: \"A special model that can answer questions from tables.\",\n            id: \"google/tapas-base-finetuned-wtq\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application that can answer a long question from Wikipedia.\",\n            id: \"deepset/wikipedia-assistant\",\n        },\n    ],\n    summary: \"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document. Some question answering models can generate answers without context!\",\n    widgetModels: [\"deepset/roberta-base-squad2\"],\n    youtubeId: \"ajPx5LwJD-I\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,gCAAgC;YAChC,aAAa;YACb,IAAI;QACR;QACA;YACI,gCAAgC;YAChC,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;YACA;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAA8B;IAC7C,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 3545, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/sentence-similarity/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"Bing queries with relevant passages from various web sources.\",\n            id: \"microsoft/ms_marco\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                label: \"Source sentence\",\n                content: \"Machine learning is so easy.\",\n                type: \"text\",\n            },\n            {\n                label: \"Sentences to compare to\",\n                content: \"Deep learning is so straightforward.\",\n                type: \"text\",\n            },\n            {\n                label: \"\",\n                content: \"This is so difficult, like rocket science.\",\n                type: \"text\",\n            },\n            {\n                label: \"\",\n                content: \"I can't believe how much I struggled with this.\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                type: \"chart\",\n                data: [\n                    {\n                        label: \"Deep learning is so straightforward.\",\n                        score: 0.623,\n                    },\n                    {\n                        label: \"This is so difficult, like rocket science.\",\n                        score: 0.413,\n                    },\n                    {\n                        label: \"I can't believe how much I struggled with this.\",\n                        score: 0.256,\n                    },\n                ],\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"Reciprocal Rank is a measure used to rank the relevancy of documents given a set of documents. Reciprocal Rank is the reciprocal of the rank of the document retrieved, meaning, if the rank is 3, the Reciprocal Rank is 0.33. If the rank is 1, the Reciprocal Rank is 1\",\n            id: \"Mean Reciprocal Rank\",\n        },\n        {\n            description: \"The similarity of the embeddings is evaluated mainly on cosine similarity. It is calculated as the cosine of the angle between two vectors. It is particularly useful when your texts are not the same length\",\n            id: \"Cosine Similarity\",\n        },\n    ],\n    models: [\n        {\n            description: \"This model works well for sentences and paragraphs and can be used for clustering/grouping and semantic searches.\",\n            id: \"sentence-transformers/all-mpnet-base-v2\",\n        },\n        {\n            description: \"A multilingual robust sentence similarity model.\",\n            id: \"BAAI/bge-m3\",\n        },\n        {\n            description: \"A robust sentence similarity model.\",\n            id: \"HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1.5\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application that leverages sentence similarity to answer questions from YouTube videos.\",\n            id: \"Gradio-Blocks/Ask_Questions_To_YouTube_Videos\",\n        },\n        {\n            description: \"An application that retrieves relevant PubMed abstracts for a given online article which can be used as further references.\",\n            id: \"Gradio-Blocks/pubmed-abstract-retriever\",\n        },\n        {\n            description: \"An application that leverages sentence similarity to summarize text.\",\n            id: \"nickmuchi/article-text-summarizer\",\n        },\n        {\n            description: \"A guide that explains how Sentence Transformers can be used for semantic search.\",\n            id: \"sentence-transformers/Sentence_Transformers_for_semantic_search\",\n        },\n    ],\n    summary: \"Sentence Similarity is the task of determining how similar two texts are. Sentence similarity models convert input texts into vectors (embeddings) that capture semantic information and calculate how close (similar) they are between them. This task is particularly useful for information retrieval and clustering/grouping.\",\n    widgetModels: [\"sentence-transformers/all-MiniLM-L6-v2\"],\n    youtubeId: \"VCZq5AkbNEU\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;YACA;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;YACA;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;YACA;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,MAAM;gBACN,MAAM;oBACF;wBACI,OAAO;wBACP,OAAO;oBACX;oBACA;wBACI,OAAO;wBACP,OAAO;oBACX;oBACA;wBACI,OAAO;wBACP,OAAO;oBACX;iBACH;YACL;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAAyC;IACxD,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 3652, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/summarization/data.js"],"sourcesContent":["const taskData = {\n    canonicalId: \"text-generation\",\n    datasets: [\n        {\n            description: \"News articles in five different languages along with their summaries. Widely used for benchmarking multilingual summarization models.\",\n            id: \"mlsum\",\n        },\n        {\n            description: \"English conversations and their summaries. Useful for benchmarking conversational agents.\",\n            id: \"samsum\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                label: \"Input\",\n                content: \"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. It was the first structure to reach a height of 300 metres. Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                label: \"Output\",\n                content: \"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. It was the first structure to reach a height of 300 metres.\",\n                type: \"text\",\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"The generated sequence is compared against its summary, and the overlap of tokens are counted. ROUGE-N refers to overlap of N subsequent tokens, ROUGE-1 refers to overlap of single tokens and ROUGE-2 is the overlap of two subsequent tokens.\",\n            id: \"rouge\",\n        },\n    ],\n    models: [\n        {\n            description: \"A strong summarization model trained on English news articles. Excels at generating factual summaries.\",\n            id: \"facebook/bart-large-cnn\",\n        },\n        {\n            description: \"A summarization model trained on medical articles.\",\n            id: \"Falconsai/medical_summarization\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application that can summarize long paragraphs.\",\n            id: \"pszemraj/summarize-long-text\",\n        },\n        {\n            description: \"A much needed summarization application for terms and conditions.\",\n            id: \"ml6team/distilbart-tos-summarizer-tosdr\",\n        },\n        {\n            description: \"An application that summarizes long documents.\",\n            id: \"pszemraj/document-summarization\",\n        },\n        {\n            description: \"An application that can detect errors in abstractive summarization.\",\n            id: \"ml6team/post-processing-summarization\",\n        },\n    ],\n    summary: \"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.\",\n    widgetModels: [\"facebook/bart-large-cnn\"],\n    youtubeId: \"yHnr5Dk2zCI\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,aAAa;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAA0B;IACzC,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 3729, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/table-question-answering/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"The WikiTableQuestions dataset is a large-scale dataset for the task of question answering on semi-structured tables.\",\n            id: \"wikitablequestions\",\n        },\n        {\n            description: \"WikiSQL is a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia.\",\n            id: \"wikisql\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                table: [\n                    [\"Rank\", \"Name\", \"No.of reigns\", \"Combined days\"],\n                    [\"1\", \"lou Thesz\", \"3\", \"3749\"],\n                    [\"2\", \"Ric Flair\", \"8\", \"3103\"],\n                    [\"3\", \"Harley Race\", \"7\", \"1799\"],\n                ],\n                type: \"tabular\",\n            },\n            { label: \"Question\", content: \"What is the number of reigns for Harley Race?\", type: \"text\" },\n        ],\n        outputs: [{ label: \"Result\", content: \"7\", type: \"text\" }],\n    },\n    metrics: [\n        {\n            description: \"Checks whether the predicted answer(s) is the same as the ground-truth answer(s).\",\n            id: \"Denotation Accuracy\",\n        },\n    ],\n    models: [\n        {\n            description: \"A table question answering model that is capable of neural SQL execution, i.e., employ TAPEX to execute a SQL query on a given table.\",\n            id: \"microsoft/tapex-base\",\n        },\n        {\n            description: \"A robust table question answering model.\",\n            id: \"google/tapas-base-finetuned-wtq\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application that answers questions based on table CSV files.\",\n            id: \"katanaml/table-query\",\n        },\n    ],\n    summary: \"Table Question Answering (Table QA) is the answering a question about an information on a given table.\",\n    widgetModels: [\"google/tapas-base-finetuned-wtq\"],\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,OAAO;oBACH;wBAAC;wBAAQ;wBAAQ;wBAAgB;qBAAgB;oBACjD;wBAAC;wBAAK;wBAAa;wBAAK;qBAAO;oBAC/B;wBAAC;wBAAK;wBAAa;wBAAK;qBAAO;oBAC/B;wBAAC;wBAAK;wBAAe;wBAAK;qBAAO;iBACpC;gBACD,MAAM;YACV;YACA;gBAAE,OAAO;gBAAY,SAAS;gBAAiD,MAAM;YAAO;SAC/F;QACD,SAAS;YAAC;gBAAE,OAAO;gBAAU,SAAS;gBAAK,MAAM;YAAO;SAAE;IAC9D;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAAkC;AACrD;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 3821, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/tabular-classification/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"A comprehensive curation of datasets covering all benchmarks.\",\n            id: \"inria-soda/tabular-benchmark\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                table: [\n                    [\"Glucose\", \"Blood Pressure \", \"Skin Thickness\", \"Insulin\", \"BMI\"],\n                    [\"148\", \"72\", \"35\", \"0\", \"33.6\"],\n                    [\"150\", \"50\", \"30\", \"0\", \"35.1\"],\n                    [\"141\", \"60\", \"29\", \"1\", \"39.2\"],\n                ],\n                type: \"tabular\",\n            },\n        ],\n        outputs: [\n            {\n                table: [[\"Diabetes\"], [\"1\"], [\"1\"], [\"0\"]],\n                type: \"tabular\",\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"\",\n            id: \"accuracy\",\n        },\n        {\n            description: \"\",\n            id: \"recall\",\n        },\n        {\n            description: \"\",\n            id: \"precision\",\n        },\n        {\n            description: \"\",\n            id: \"f1\",\n        },\n    ],\n    models: [\n        {\n            description: \"Breast cancer prediction model based on decision trees.\",\n            id: \"scikit-learn/cancer-prediction-trees\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application that can predict defective products on a production line.\",\n            id: \"scikit-learn/tabular-playground\",\n        },\n        {\n            description: \"An application that compares various tabular classification techniques on different datasets.\",\n            id: \"scikit-learn/classification\",\n        },\n    ],\n    summary: \"Tabular classification is the task of classifying a target category (a group) based on set of attributes.\",\n    widgetModels: [\"scikit-learn/tabular-playground\"],\n    youtubeId: \"\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,OAAO;oBACH;wBAAC;wBAAW;wBAAmB;wBAAkB;wBAAW;qBAAM;oBAClE;wBAAC;wBAAO;wBAAM;wBAAM;wBAAK;qBAAO;oBAChC;wBAAC;wBAAO;wBAAM;wBAAM;wBAAK;qBAAO;oBAChC;wBAAC;wBAAO;wBAAM;wBAAM;wBAAK;qBAAO;iBACnC;gBACD,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,OAAO;oBAAC;wBAAC;qBAAW;oBAAE;wBAAC;qBAAI;oBAAE;wBAAC;qBAAI;oBAAE;wBAAC;qBAAI;iBAAC;gBAC1C,MAAM;YACV;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAAkC;IACjD,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 3933, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/tabular-regression/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"A comprehensive curation of datasets covering all benchmarks.\",\n            id: \"inria-soda/tabular-benchmark\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                table: [\n                    [\"Car Name\", \"Horsepower\", \"Weight\"],\n                    [\"ford torino\", \"140\", \"3,449\"],\n                    [\"amc hornet\", \"97\", \"2,774\"],\n                    [\"toyota corolla\", \"65\", \"1,773\"],\n                ],\n                type: \"tabular\",\n            },\n        ],\n        outputs: [\n            {\n                table: [[\"MPG (miles per gallon)\"], [\"17\"], [\"18\"], [\"31\"]],\n                type: \"tabular\",\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"\",\n            id: \"mse\",\n        },\n        {\n            description: \"Coefficient of determination (or R-squared) is a measure of how well the model fits the data. Higher R-squared is considered a better fit.\",\n            id: \"r-squared\",\n        },\n    ],\n    models: [\n        {\n            description: \"Fish weight prediction based on length measurements and species.\",\n            id: \"scikit-learn/Fish-Weight\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application that can predict weight of a fish based on set of attributes.\",\n            id: \"scikit-learn/fish-weight-prediction\",\n        },\n    ],\n    summary: \"Tabular regression is the task of predicting a numerical value given a set of attributes.\",\n    widgetModels: [\"scikit-learn/Fish-Weight\"],\n    youtubeId: \"\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,OAAO;oBACH;wBAAC;wBAAY;wBAAc;qBAAS;oBACpC;wBAAC;wBAAe;wBAAO;qBAAQ;oBAC/B;wBAAC;wBAAc;wBAAM;qBAAQ;oBAC7B;wBAAC;wBAAkB;wBAAM;qBAAQ;iBACpC;gBACD,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,OAAO;oBAAC;wBAAC;qBAAyB;oBAAE;wBAAC;qBAAK;oBAAE;wBAAC;qBAAK;oBAAE;wBAAC;qBAAK;iBAAC;gBAC3D,MAAM;YACV;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAA2B;IAC1C,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 4025, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/text-to-image/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"RedCaps is a large-scale dataset of 12M image-text pairs collected from Reddit.\",\n            id: \"red_caps\",\n        },\n        {\n            description: \"Conceptual Captions is a dataset consisting of ~3.3M images annotated with captions.\",\n            id: \"conceptual_captions\",\n        },\n        {\n            description: \"12M image-caption pairs.\",\n            id: \"Spawning/PD12M\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                label: \"Input\",\n                content: \"A city above clouds, pastel colors, Victorian style\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                filename: \"image.jpeg\",\n                type: \"img\",\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"The Inception Score (IS) measure assesses diversity and meaningfulness. It uses a generated image sample to predict its label. A higher score signifies more diverse and meaningful images.\",\n            id: \"IS\",\n        },\n        {\n            description: \"The Fréchet Inception Distance (FID) calculates the distance between distributions between synthetic and real samples. A lower FID score indicates better similarity between the distributions of real and generated images.\",\n            id: \"FID\",\n        },\n        {\n            description: \"R-precision assesses how the generated image aligns with the provided text description. It uses the generated images as queries to retrieve relevant text descriptions. The top 'r' relevant descriptions are selected and used to calculate R-precision as r/R, where 'R' is the number of ground truth descriptions associated with the generated images. A higher R-precision value indicates a better model.\",\n            id: \"R-Precision\",\n        },\n    ],\n    models: [\n        {\n            description: \"One of the most powerful image generation models that can generate realistic outputs.\",\n            id: \"black-forest-labs/FLUX.1-Krea-dev\",\n        },\n        {\n            description: \"A powerful image generation model.\",\n            id: \"Qwen/Qwen-Image\",\n        },\n        {\n            description: \"Powerful and fast image generation model.\",\n            id: \"ByteDance/SDXL-Lightning\",\n        },\n        {\n            description: \"A powerful text-to-image model.\",\n            id: \"ByteDance/Hyper-SD\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"A powerful text-to-image application.\",\n            id: \"stabilityai/stable-diffusion-3-medium\",\n        },\n        {\n            description: \"A text-to-image application to generate comics.\",\n            id: \"jbilcke-hf/ai-comic-factory\",\n        },\n        {\n            description: \"An application to match multiple custom image generation models.\",\n            id: \"multimodalart/flux-lora-lab\",\n        },\n        {\n            description: \"A powerful yet very fast image generation application.\",\n            id: \"latent-consistency/lcm-lora-for-sdxl\",\n        },\n        {\n            description: \"A gallery to explore various text-to-image models.\",\n            id: \"multimodalart/LoraTheExplorer\",\n        },\n        {\n            description: \"An application for `text-to-image`, `image-to-image` and image inpainting.\",\n            id: \"ArtGAN/Stable-Diffusion-ControlNet-WebUI\",\n        },\n        {\n            description: \"An application to generate realistic images given photos of a person and a prompt.\",\n            id: \"InstantX/InstantID\",\n        },\n    ],\n    summary: \"Text-to-image is the task of generating images from input text. These pipelines can also be used to modify and edit images based on text prompts.\",\n    widgetModels: [\"black-forest-labs/FLUX.1-dev\"],\n    youtubeId: \"\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,UAAU;gBACV,MAAM;YACV;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAA+B;IAC9C,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 4132, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/text-to-speech/data.js"],"sourcesContent":["const taskData = {\n    canonicalId: \"text-to-audio\",\n    datasets: [\n        {\n            description: \"10K hours of multi-speaker English dataset.\",\n            id: \"parler-tts/mls_eng_10k\",\n        },\n        {\n            description: \"Multi-speaker English dataset.\",\n            id: \"mythicinfinity/libritts_r\",\n        },\n        {\n            description: \"Multi-lingual dataset.\",\n            id: \"facebook/multilingual_librispeech\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                label: \"Input\",\n                content: \"I love audio models on the Hub!\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                filename: \"audio.wav\",\n                type: \"audio\",\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"The Mel Cepstral Distortion (MCD) metric is used to calculate the quality of generated speech.\",\n            id: \"mel cepstral distortion\",\n        },\n    ],\n    models: [\n        {\n            description: \"Small yet powerful TTS model.\",\n            id: \"KittenML/kitten-tts-nano-0.1\",\n        },\n        {\n            description: \"Bleeding edge TTS model.\",\n            id: \"ResembleAI/chatterbox\",\n        },\n        {\n            description: \"A massively multi-lingual TTS model.\",\n            id: \"fishaudio/fish-speech-1.5\",\n        },\n        {\n            description: \"A text-to-dialogue model.\",\n            id: \"nari-labs/Dia-1.6B-0626\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application for generate high quality speech in different languages.\",\n            id: \"hexgrad/Kokoro-TTS\",\n        },\n        {\n            description: \"A multilingual text-to-speech application.\",\n            id: \"fishaudio/fish-speech-1\",\n        },\n        {\n            description: \"Performant TTS application.\",\n            id: \"ResembleAI/Chatterbox\",\n        },\n        {\n            description: \"An application to compare different TTS models.\",\n            id: \"TTS-AGI/TTS-Arena-V2\",\n        },\n        {\n            description: \"An application that generates podcast episodes.\",\n            id: \"ngxson/kokoro-podcast-generator\",\n        },\n    ],\n    summary: \"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.\",\n    widgetModels: [\"suno/bark\"],\n    youtubeId: \"NW62DpzJ274\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,aAAa;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,UAAU;gBACV,MAAM;YACV;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAAY;IAC3B,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 4224, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/token-classification/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"A widely used dataset useful to benchmark named entity recognition models.\",\n            id: \"eriktks/conll2003\",\n        },\n        {\n            description: \"A multilingual dataset of Wikipedia articles annotated for named entity recognition in over 150 different languages.\",\n            id: \"unimelb-nlp/wikiann\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                label: \"Input\",\n                content: \"My name is Omar and I live in Zürich.\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                text: \"My name is Omar and I live in Zürich.\",\n                tokens: [\n                    {\n                        type: \"PERSON\",\n                        start: 11,\n                        end: 15,\n                    },\n                    {\n                        type: \"GPE\",\n                        start: 30,\n                        end: 36,\n                    },\n                ],\n                type: \"text-with-tokens\",\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"\",\n            id: \"accuracy\",\n        },\n        {\n            description: \"\",\n            id: \"recall\",\n        },\n        {\n            description: \"\",\n            id: \"precision\",\n        },\n        {\n            description: \"\",\n            id: \"f1\",\n        },\n    ],\n    models: [\n        {\n            description: \"A robust performance model to identify people, locations, organizations and names of miscellaneous entities.\",\n            id: \"dslim/bert-base-NER\",\n        },\n        {\n            description: \"A strong model to identify people, locations, organizations and names in multiple languages.\",\n            id: \"FacebookAI/xlm-roberta-large-finetuned-conll03-english\",\n        },\n        {\n            description: \"A token classification model specialized on medical entity recognition.\",\n            id: \"blaze999/Medical-NER\",\n        },\n        {\n            description: \"Flair models are typically the state of the art in named entity recognition tasks.\",\n            id: \"flair/ner-english\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application that can recognizes entities, extracts noun chunks and recognizes various linguistic features of each token.\",\n            id: \"spacy/gradio_pipeline_visualizer\",\n        },\n    ],\n    summary: \"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.\",\n    widgetModels: [\"FacebookAI/xlm-roberta-large-finetuned-conll03-english\"],\n    youtubeId: \"wVHdVlPScxA\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,MAAM;gBACN,QAAQ;oBACJ;wBACI,MAAM;wBACN,OAAO;wBACP,KAAK;oBACT;oBACA;wBACI,MAAM;wBACN,OAAO;wBACP,KAAK;oBACT;iBACH;gBACD,MAAM;YACV;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAAyD;IACxE,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 4319, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/translation/data.js"],"sourcesContent":["const taskData = {\n    canonicalId: \"text-generation\",\n    datasets: [\n        {\n            description: \"A dataset of copyright-free books translated into 16 different languages.\",\n            id: \"Helsinki-NLP/opus_books\",\n        },\n        {\n            description: \"An example of translation between programming languages. This dataset consists of functions in Java and C#.\",\n            id: \"google/code_x_glue_cc_code_to_code_trans\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                label: \"Input\",\n                content: \"My name is Omar and I live in Zürich.\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                label: \"Output\",\n                content: \"Mein Name ist Omar und ich wohne in Zürich.\",\n                type: \"text\",\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"BLEU score is calculated by counting the number of shared single or subsequent tokens between the generated sequence and the reference. Subsequent n tokens are called “n-grams”. Unigram refers to a single token while bi-gram refers to token pairs and n-grams refer to n subsequent tokens. The score ranges from 0 to 1, where 1 means the translation perfectly matched and 0 did not match at all\",\n            id: \"bleu\",\n        },\n        {\n            description: \"\",\n            id: \"sacrebleu\",\n        },\n    ],\n    models: [\n        {\n            description: \"Very powerful model that can translate many languages between each other, especially low-resource languages.\",\n            id: \"facebook/nllb-200-1.3B\",\n        },\n        {\n            description: \"A general-purpose Transformer that can be used to translate from English to German, French, or Romanian.\",\n            id: \"google-t5/t5-base\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application that can translate between 100 languages.\",\n            id: \"Iker/Translate-100-languages\",\n        },\n        {\n            description: \"An application that can translate between many languages.\",\n            id: \"Geonmo/nllb-translation-demo\",\n        },\n    ],\n    summary: \"Translation is the task of converting text from one language to another.\",\n    widgetModels: [\"facebook/mbart-large-50-many-to-many-mmt\"],\n    youtubeId: \"1JvfrvZgi6c\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,aAAa;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAA2C;IAC1D,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 4392, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/text-classification/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"A widely used dataset used to benchmark multiple variants of text classification.\",\n            id: \"nyu-mll/glue\",\n        },\n        {\n            description: \"A text classification dataset used to benchmark natural language inference models\",\n            id: \"stanfordnlp/snli\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                label: \"Input\",\n                content: \"I love Hugging Face!\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                type: \"chart\",\n                data: [\n                    {\n                        label: \"POSITIVE\",\n                        score: 0.9,\n                    },\n                    {\n                        label: \"NEUTRAL\",\n                        score: 0.1,\n                    },\n                    {\n                        label: \"NEGATIVE\",\n                        score: 0.0,\n                    },\n                ],\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"\",\n            id: \"accuracy\",\n        },\n        {\n            description: \"\",\n            id: \"recall\",\n        },\n        {\n            description: \"\",\n            id: \"precision\",\n        },\n        {\n            description: \"The F1 metric is the harmonic mean of the precision and recall. It can be calculated as: F1 = 2 * (precision * recall) / (precision + recall)\",\n            id: \"f1\",\n        },\n    ],\n    models: [\n        {\n            description: \"A robust model trained for sentiment analysis.\",\n            id: \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n        },\n        {\n            description: \"A sentiment analysis model specialized in financial sentiment.\",\n            id: \"ProsusAI/finbert\",\n        },\n        {\n            description: \"A sentiment analysis model specialized in analyzing tweets.\",\n            id: \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n        },\n        {\n            description: \"A model that can classify languages.\",\n            id: \"papluca/xlm-roberta-base-language-detection\",\n        },\n        {\n            description: \"A model that can classify text generation attacks.\",\n            id: \"meta-llama/Prompt-Guard-86M\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application that can classify financial sentiment.\",\n            id: \"IoannisTr/Tech_Stocks_Trading_Assistant\",\n        },\n        {\n            description: \"A dashboard that contains various text classification tasks.\",\n            id: \"miesnerjacob/Multi-task-NLP\",\n        },\n        {\n            description: \"An application that analyzes user reviews in healthcare.\",\n            id: \"spacy/healthsea-demo\",\n        },\n    ],\n    summary: \"Text Classification is the task of assigning a label or class to a given text. Some use cases are sentiment analysis, natural language inference, and assessing grammatical correctness.\",\n    widgetModels: [\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"],\n    youtubeId: \"leNG9fN9FQU\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,MAAM;gBACN,MAAM;oBACF;wBACI,OAAO;wBACP,OAAO;oBACX;oBACA;wBACI,OAAO;wBACP,OAAO;oBACX;oBACA;wBACI,OAAO;wBACP,OAAO;oBACX;iBACH;YACL;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAA6D;IAC5E,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 4500, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/text-generation/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"Multilingual dataset used to evaluate text generation models.\",\n            id: \"CohereForAI/Global-MMLU\",\n        },\n        {\n            description: \"High quality multilingual data used to train text-generation models.\",\n            id: \"HuggingFaceFW/fineweb-2\",\n        },\n        {\n            description: \"Truly open-source, curated and cleaned dialogue dataset.\",\n            id: \"HuggingFaceH4/ultrachat_200k\",\n        },\n        {\n            description: \"A reasoning dataset.\",\n            id: \"open-r1/OpenThoughts-114k-math\",\n        },\n        {\n            description: \"A multilingual instruction dataset with preference ratings on responses.\",\n            id: \"allenai/tulu-3-sft-mixture\",\n        },\n        {\n            description: \"A large synthetic dataset for alignment of text generation models.\",\n            id: \"HuggingFaceTB/smoltalk\",\n        },\n        {\n            description: \"A dataset made for training text generation models solving math questions.\",\n            id: \"HuggingFaceTB/finemath\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                label: \"Input\",\n                content: \"Once upon a time,\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                label: \"Output\",\n                content: \"Once upon a time, we knew that our ancestors were on the verge of extinction. The great explorers and poets of the Old World, from Alexander the Great to Chaucer, are dead and gone. A good many of our ancient explorers and poets have\",\n                type: \"text\",\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"Cross Entropy is a metric that calculates the difference between two probability distributions. Each probability distribution is the distribution of predicted words\",\n            id: \"Cross Entropy\",\n        },\n        {\n            description: \"The Perplexity metric is the exponential of the cross-entropy loss. It evaluates the probabilities assigned to the next word by the model. Lower perplexity indicates better performance\",\n            id: \"Perplexity\",\n        },\n    ],\n    models: [\n        { description: \"A text-generation model trained to follow instructions.\", id: \"google/gemma-2-2b-it\" },\n        {\n            description: \"Powerful text generation model for coding.\",\n            id: \"Qwen/Qwen3-Coder-480B-A35B-Instruct\",\n        },\n        {\n            description: \"Great text generation model with top-notch tool calling capabilities.\",\n            id: \"openai/gpt-oss-120b\",\n        },\n        {\n            description: \"Powerful text generation model.\",\n            id: \"zai-org/GLM-4.5\",\n        },\n        {\n            description: \"A powerful small model with reasoning capabilities.\",\n            id: \"Qwen/Qwen3-4B-Thinking-2507\",\n        },\n        {\n            description: \"Strong conversational model that supports very long instructions.\",\n            id: \"Qwen/Qwen2.5-7B-Instruct-1M\",\n        },\n        {\n            description: \"Text generation model used to write code.\",\n            id: \"Qwen/Qwen2.5-Coder-32B-Instruct\",\n        },\n        {\n            description: \"Powerful reasoning based open large language model.\",\n            id: \"deepseek-ai/DeepSeek-R1\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application that writes and executes code from text instructions and supports many models.\",\n            id: \"akhaliq/anycoder\",\n        },\n        {\n            description: \"An application that builds websites from natural language prompts.\",\n            id: \"enzostvs/deepsite\",\n        },\n        {\n            description: \"A leaderboard for comparing chain-of-thought performance of models.\",\n            id: \"logikon/open_cot_leaderboard\",\n        },\n        {\n            description: \"An text generation based application based on a very powerful LLaMA2 model.\",\n            id: \"ysharma/Explore_llamav2_with_TGI\",\n        },\n        {\n            description: \"An text generation based application to converse with Zephyr model.\",\n            id: \"HuggingFaceH4/zephyr-chat\",\n        },\n        {\n            description: \"A leaderboard that ranks text generation models based on blind votes from people.\",\n            id: \"lmsys/chatbot-arena-leaderboard\",\n        },\n        {\n            description: \"An chatbot to converse with a very powerful text generation model.\",\n            id: \"mlabonne/phixtral-chat\",\n        },\n    ],\n    summary: \"Generating text is the task of generating new text given another text. These models can, for example, fill in incomplete text or paraphrase.\",\n    widgetModels: [\"mistralai/Mistral-Nemo-Instruct-2407\"],\n    youtubeId: \"e9gNEAlsOvU\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YAAE,aAAa;YAA2D,IAAI;QAAuB;QACrG;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAAuC;IACtD,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 4636, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/text-ranking/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"Bing queries with relevant passages from various web sources.\",\n            id: \"microsoft/ms_marco\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                label: \"Source sentence\",\n                content: \"Machine learning is so easy.\",\n                type: \"text\",\n            },\n            {\n                label: \"Sentences to compare to\",\n                content: \"Deep learning is so straightforward.\",\n                type: \"text\",\n            },\n            {\n                label: \"\",\n                content: \"This is so difficult, like rocket science.\",\n                type: \"text\",\n            },\n            {\n                label: \"\",\n                content: \"I can't believe how much I struggled with this.\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                type: \"chart\",\n                data: [\n                    {\n                        label: \"Deep learning is so straightforward.\",\n                        score: 2.2006407,\n                    },\n                    {\n                        label: \"This is so difficult, like rocket science.\",\n                        score: -6.2634873,\n                    },\n                    {\n                        label: \"I can't believe how much I struggled with this.\",\n                        score: -10.251488,\n                    },\n                ],\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"Discounted Cumulative Gain (DCG) measures the gain, or usefulness, of search results discounted by their position. The normalization is done by dividing the DCG by the ideal DCG, which is the DCG of the perfect ranking.\",\n            id: \"Normalized Discounted Cumulative Gain\",\n        },\n        {\n            description: \"Reciprocal Rank is a measure used to rank the relevancy of documents given a set of documents. Reciprocal Rank is the reciprocal of the rank of the document retrieved, meaning, if the rank is 3, the Reciprocal Rank is 0.33. If the rank is 1, the Reciprocal Rank is 1\",\n            id: \"Mean Reciprocal Rank\",\n        },\n        {\n            description: \"Mean Average Precision (mAP) is the overall average of the Average Precision (AP) values, where AP is the Area Under the PR Curve (AUC-PR)\",\n            id: \"Mean Average Precision\",\n        },\n    ],\n    models: [\n        {\n            description: \"An extremely efficient text ranking model trained on a web search dataset.\",\n            id: \"cross-encoder/ms-marco-MiniLM-L6-v2\",\n        },\n        {\n            description: \"A strong multilingual text reranker model.\",\n            id: \"Alibaba-NLP/gte-multilingual-reranker-base\",\n        },\n        {\n            description: \"An efficient text ranking model that punches above its weight.\",\n            id: \"Alibaba-NLP/gte-reranker-modernbert-base\",\n        },\n    ],\n    spaces: [],\n    summary: \"Text Ranking is the task of ranking a set of texts based on their relevance to a query. Text ranking models are trained on large datasets of queries and relevant documents to learn how to rank documents based on their relevance to the query. This task is particularly useful for search engines and information retrieval systems.\",\n    widgetModels: [\"cross-encoder/ms-marco-MiniLM-L6-v2\"],\n    youtubeId: \"\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;YACA;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;YACA;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;YACA;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,MAAM;gBACN,MAAM;oBACF;wBACI,OAAO;wBACP,OAAO;oBACX;oBACA;wBACI,OAAO;wBACP,OAAO,CAAC;oBACZ;oBACA;wBACI,OAAO;wBACP,OAAO,CAAC;oBACZ;iBACH;YACL;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ,EAAE;IACV,SAAS;IACT,cAAc;QAAC;KAAsC;IACrD,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 4730, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/text-to-video/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"Microsoft Research Video to Text is a large-scale dataset for open domain video captioning\",\n            id: \"iejMac/CLIP-MSR-VTT\",\n        },\n        {\n            description: \"UCF101 Human Actions dataset consists of 13,320 video clips from YouTube, with 101 classes.\",\n            id: \"quchenyuan/UCF101-ZIP\",\n        },\n        {\n            description: \"A high-quality dataset for human action recognition in YouTube videos.\",\n            id: \"nateraw/kinetics\",\n        },\n        {\n            description: \"A dataset of video clips of humans performing pre-defined basic actions with everyday objects.\",\n            id: \"HuggingFaceM4/something_something_v2\",\n        },\n        {\n            description: \"This dataset consists of text-video pairs and contains noisy samples with irrelevant video descriptions\",\n            id: \"HuggingFaceM4/webvid\",\n        },\n        {\n            description: \"A dataset of short Flickr videos for the temporal localization of events with descriptions.\",\n            id: \"iejMac/CLIP-DiDeMo\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                label: \"Input\",\n                content: \"Darth Vader is surfing on the waves.\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                filename: \"text-to-video-output.gif\",\n                type: \"img\",\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"Inception Score uses an image classification model that predicts class labels and evaluates how distinct and diverse the images are. A higher score indicates better video generation.\",\n            id: \"is\",\n        },\n        {\n            description: \"Frechet Inception Distance uses an image classification model to obtain image embeddings. The metric compares mean and standard deviation of the embeddings of real and generated images. A smaller score indicates better video generation.\",\n            id: \"fid\",\n        },\n        {\n            description: \"Frechet Video Distance uses a model that captures coherence for changes in frames and the quality of each frame. A smaller score indicates better video generation.\",\n            id: \"fvd\",\n        },\n        {\n            description: \"CLIPSIM measures similarity between video frames and text using an image-text similarity model. A higher score indicates better video generation.\",\n            id: \"clipsim\",\n        },\n    ],\n    models: [\n        {\n            description: \"A strong model for consistent video generation.\",\n            id: \"tencent/HunyuanVideo\",\n        },\n        {\n            description: \"A text-to-video model with high fidelity motion and strong prompt adherence.\",\n            id: \"Lightricks/LTX-Video\",\n        },\n        {\n            description: \"A text-to-video model focusing on physics-aware applications like robotics.\",\n            id: \"nvidia/Cosmos-1.0-Diffusion-7B-Text2World\",\n        },\n        {\n            description: \"Very fast model for video generation.\",\n            id: \"Lightricks/LTX-Video-0.9.8-13B-distilled\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application that generates video from text.\",\n            id: \"VideoCrafter/VideoCrafter\",\n        },\n        {\n            description: \"Consistent video generation application.\",\n            id: \"Wan-AI/Wan2.1\",\n        },\n        {\n            description: \"A cutting edge video generation application.\",\n            id: \"Pyramid-Flow/pyramid-flow\",\n        },\n    ],\n    summary: \"Text-to-video models can be used in any application that requires generating consistent sequence of images from text. \",\n    widgetModels: [\"Wan-AI/Wan2.2-TI2V-5B\"],\n    youtubeId: undefined,\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,UAAU;gBACV,MAAM;YACV;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAAwB;IACvC,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 4837, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/unconditional-image-generation/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"The CIFAR-100 dataset consists of 60000 32x32 colour images in 100 classes, with 600 images per class.\",\n            id: \"cifar100\",\n        },\n        {\n            description: \"Multiple images of celebrities, used for facial expression translation.\",\n            id: \"CelebA\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                label: \"Seed\",\n                content: \"42\",\n                type: \"text\",\n            },\n            {\n                label: \"Number of images to generate:\",\n                content: \"4\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                filename: \"unconditional-image-generation-output.jpeg\",\n                type: \"img\",\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"The inception score (IS) evaluates the quality of generated images. It measures the diversity of the generated images (the model predictions are evenly distributed across all possible labels) and their 'distinction' or 'sharpness' (the model confidently predicts a single label for each image).\",\n            id: \"Inception score (IS)\",\n        },\n        {\n            description: \"The Fréchet Inception Distance (FID) evaluates the quality of images created by a generative model by calculating the distance between feature vectors for real and generated images.\",\n            id: \"Frećhet Inception Distance (FID)\",\n        },\n    ],\n    models: [\n        {\n            description: \"High-quality image generation model trained on the CIFAR-10 dataset. It synthesizes images of the ten classes presented in the dataset using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics.\",\n            id: \"google/ddpm-cifar10-32\",\n        },\n        {\n            description: \"High-quality image generation model trained on the 256x256 CelebA-HQ dataset. It synthesizes images of faces using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics.\",\n            id: \"google/ddpm-celebahq-256\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application that can generate realistic faces.\",\n            id: \"CompVis/celeba-latent-diffusion\",\n        },\n    ],\n    summary: \"Unconditional image generation is the task of generating images with no condition in any context (like a prompt text or another image). Once trained, the model will create images that resemble its training data distribution.\",\n    widgetModels: [\"\"],\n    // TODO: Add related video\n    youtubeId: \"\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;YACA;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,UAAU;gBACV,MAAM;YACV;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAAG;IAClB,0BAA0B;IAC1B,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 4910, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/video-classification/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            // TODO write proper description\n            description: \"Benchmark dataset used for video classification with videos that belong to 400 classes.\",\n            id: \"kinetics400\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                filename: \"video-classification-input.gif\",\n                type: \"img\",\n            },\n        ],\n        outputs: [\n            {\n                type: \"chart\",\n                data: [\n                    {\n                        label: \"Playing Guitar\",\n                        score: 0.514,\n                    },\n                    {\n                        label: \"Playing Tennis\",\n                        score: 0.193,\n                    },\n                    {\n                        label: \"Cooking\",\n                        score: 0.068,\n                    },\n                ],\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"\",\n            id: \"accuracy\",\n        },\n        {\n            description: \"\",\n            id: \"recall\",\n        },\n        {\n            description: \"\",\n            id: \"precision\",\n        },\n        {\n            description: \"\",\n            id: \"f1\",\n        },\n    ],\n    models: [\n        {\n            // TO DO: write description\n            description: \"Strong Video Classification model trained on the Kinetics 400 dataset.\",\n            id: \"google/vivit-b-16x2-kinetics400\",\n        },\n        {\n            // TO DO: write description\n            description: \"Strong Video Classification model trained on the Kinetics 400 dataset.\",\n            id: \"microsoft/xclip-base-patch32\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application that classifies video at different timestamps.\",\n            id: \"nateraw/lavila\",\n        },\n        {\n            description: \"An application that classifies video.\",\n            id: \"fcakyon/video-classification\",\n        },\n    ],\n    summary: \"Video classification is the task of assigning a label or class to an entire video. Videos are expected to have only one class for each video. Video classification models take a video as input and return a prediction about which class the video belongs to.\",\n    widgetModels: [],\n    youtubeId: \"\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,gCAAgC;YAChC,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,UAAU;gBACV,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,MAAM;gBACN,MAAM;oBACF;wBACI,OAAO;wBACP,OAAO;oBACX;oBACA;wBACI,OAAO;wBACP,OAAO;oBACX;oBACA;wBACI,OAAO;wBACP,OAAO;oBACX;iBACH;YACL;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,2BAA2B;YAC3B,aAAa;YACb,IAAI;QACR;QACA;YACI,2BAA2B;YAC3B,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc,EAAE;IAChB,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 4998, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/visual-document-retrieval/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"A large dataset used to train visual document retrieval models.\",\n            id: \"vidore/colpali_train_set\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                filename: \"input.png\",\n                type: \"img\",\n            },\n            {\n                label: \"Question\",\n                content: \"Is the model in this paper the fastest for inference?\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                type: \"chart\",\n                data: [\n                    {\n                        label: \"Page 10\",\n                        score: 0.7,\n                    },\n                    {\n                        label: \"Page 11\",\n                        score: 0.06,\n                    },\n                    {\n                        label: \"Page 9\",\n                        score: 0.003,\n                    },\n                ],\n            },\n        ],\n    },\n    isPlaceholder: false,\n    metrics: [\n        {\n            description: \"NDCG@k scores ranked recommendation lists for top-k results. 0 is the worst, 1 is the best.\",\n            id: \"Normalized Discounted Cumulative Gain at K\",\n        },\n    ],\n    models: [\n        {\n            description: \"Very accurate visual document retrieval model for multilingual queries and documents.\",\n            id: \"vidore/colqwen2-v1.0\",\n        },\n        {\n            description: \"Very fast and efficient visual document retrieval model that can also take in other modalities like audio.\",\n            id: \"Tevatron/OmniEmbed-v0.1\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"A leaderboard of visual document retrieval models.\",\n            id: \"vidore/vidore-leaderboard\",\n        },\n        {\n            description: \"Visual retrieval augmented generation demo based on ColQwen2 model.\",\n            id: \"vidore/visual-rag-tool\",\n        },\n    ],\n    summary: \"Visual document retrieval is the task of searching for relevant image-based documents, such as PDFs. These models take a text query and multiple documents as input and return the top-most relevant documents and relevancy scores as output.\",\n    widgetModels: [\"\"],\n    youtubeId: \"\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,UAAU;gBACV,MAAM;YACV;YACA;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,MAAM;gBACN,MAAM;oBACF;wBACI,OAAO;wBACP,OAAO;oBACX;oBACA;wBACI,OAAO;wBACP,OAAO;oBACX;oBACA;wBACI,OAAO;wBACP,OAAO;oBACX;iBACH;YACL;SACH;IACL;IACA,eAAe;IACf,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAAG;IAClB,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 5079, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/visual-question-answering/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"A widely used dataset containing questions (with answers) about images.\",\n            id: \"Graphcore/vqa\",\n        },\n        {\n            description: \"A dataset to benchmark visual reasoning based on text in images.\",\n            id: \"facebook/textvqa\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                filename: \"elephant.jpeg\",\n                type: \"img\",\n            },\n            {\n                label: \"Question\",\n                content: \"What is in this image?\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                type: \"chart\",\n                data: [\n                    {\n                        label: \"elephant\",\n                        score: 0.97,\n                    },\n                    {\n                        label: \"elephants\",\n                        score: 0.06,\n                    },\n                    {\n                        label: \"animal\",\n                        score: 0.003,\n                    },\n                ],\n            },\n        ],\n    },\n    isPlaceholder: false,\n    metrics: [\n        {\n            description: \"\",\n            id: \"accuracy\",\n        },\n        {\n            description: \"Measures how much a predicted answer differs from the ground truth based on the difference in their semantic meaning.\",\n            id: \"wu-palmer similarity\",\n        },\n    ],\n    models: [\n        {\n            description: \"A visual question answering model trained to convert charts and plots to text.\",\n            id: \"google/deplot\",\n        },\n        {\n            description: \"A visual question answering model trained for mathematical reasoning and chart derendering from images.\",\n            id: \"google/matcha-base\",\n        },\n        {\n            description: \"A strong visual question answering that answers questions from book covers.\",\n            id: \"google/pix2struct-ocrvqa-large\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application that compares visual question answering models across different tasks.\",\n            id: \"merve/pix2struct\",\n        },\n        {\n            description: \"An application that can answer questions based on images.\",\n            id: \"nielsr/vilt-vqa\",\n        },\n        {\n            description: \"An application that can caption images and answer questions about a given image. \",\n            id: \"Salesforce/BLIP\",\n        },\n        {\n            description: \"An application that can caption images and answer questions about a given image. \",\n            id: \"vumichien/Img2Prompt\",\n        },\n    ],\n    summary: \"Visual Question Answering is the task of answering open-ended questions based on an image. They output natural language responses to natural language questions.\",\n    widgetModels: [\"dandelin/vilt-b32-finetuned-vqa\"],\n    youtubeId: \"\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,UAAU;gBACV,MAAM;YACV;YACA;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,MAAM;gBACN,MAAM;oBACF;wBACI,OAAO;wBACP,OAAO;oBACX;oBACA;wBACI,OAAO;wBACP,OAAO;oBACX;oBACA;wBACI,OAAO;wBACP,OAAO;oBACX;iBACH;YACL;SACH;IACL;IACA,eAAe;IACf,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAAkC;IACjD,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 5180, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/zero-shot-classification/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"A widely used dataset used to benchmark multiple variants of text classification.\",\n            id: \"nyu-mll/glue\",\n        },\n        {\n            description: \"The Multi-Genre Natural Language Inference (MultiNLI) corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information.\",\n            id: \"nyu-mll/multi_nli\",\n        },\n        {\n            description: \"FEVER is a publicly available dataset for fact extraction and verification against textual sources.\",\n            id: \"fever/fever\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                label: \"Text Input\",\n                content: \"Dune is the best movie ever.\",\n                type: \"text\",\n            },\n            {\n                label: \"Candidate Labels\",\n                content: \"CINEMA, ART, MUSIC\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                type: \"chart\",\n                data: [\n                    {\n                        label: \"CINEMA\",\n                        score: 0.9,\n                    },\n                    {\n                        label: \"ART\",\n                        score: 0.1,\n                    },\n                    {\n                        label: \"MUSIC\",\n                        score: 0.0,\n                    },\n                ],\n            },\n        ],\n    },\n    metrics: [],\n    models: [\n        {\n            description: \"Powerful zero-shot text classification model.\",\n            id: \"facebook/bart-large-mnli\",\n        },\n        {\n            description: \"Cutting-edge zero-shot multilingual text classification model.\",\n            id: \"MoritzLaurer/ModernBERT-large-zeroshot-v2.0\",\n        },\n        {\n            description: \"Zero-shot text classification model that can be used for topic and sentiment classification.\",\n            id: \"knowledgator/gliclass-modern-base-v2.0-init\",\n        },\n    ],\n    spaces: [],\n    summary: \"Zero-shot text classification is a task in natural language processing where a model is trained on a set of labeled examples but is then able to classify new examples from previously unseen classes.\",\n    widgetModels: [\"facebook/bart-large-mnli\"],\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;YACA;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,MAAM;gBACN,MAAM;oBACF;wBACI,OAAO;wBACP,OAAO;oBACX;oBACA;wBACI,OAAO;wBACP,OAAO;oBACX;oBACA;wBACI,OAAO;wBACP,OAAO;oBACX;iBACH;YACL;SACH;IACL;IACA,SAAS,EAAE;IACX,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ,EAAE;IACV,SAAS;IACT,cAAc;QAAC;KAA2B;AAC9C;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 5258, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/zero-shot-image-classification/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            // TODO write proper description\n            description: \"\",\n            id: \"\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                filename: \"image-classification-input.jpeg\",\n                type: \"img\",\n            },\n            {\n                label: \"Classes\",\n                content: \"cat, dog, bird\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                type: \"chart\",\n                data: [\n                    {\n                        label: \"Cat\",\n                        score: 0.664,\n                    },\n                    {\n                        label: \"Dog\",\n                        score: 0.329,\n                    },\n                    {\n                        label: \"Bird\",\n                        score: 0.008,\n                    },\n                ],\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"Computes the number of times the correct label appears in top K labels predicted\",\n            id: \"top-K accuracy\",\n        },\n    ],\n    models: [\n        {\n            description: \"Multilingual image classification model for 80 languages.\",\n            id: \"visheratin/mexma-siglip\",\n        },\n        {\n            description: \"Strong zero-shot image classification model.\",\n            id: \"google/siglip2-base-patch16-224\",\n        },\n        {\n            description: \"Robust zero-shot image classification model.\",\n            id: \"intfloat/mmE5-mllama-11b-instruct\",\n        },\n        {\n            description: \"Powerful zero-shot image classification model supporting 94 languages.\",\n            id: \"jinaai/jina-clip-v2\",\n        },\n        {\n            description: \"Strong image classification model for biomedical domain.\",\n            id: \"microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application that leverages zero-shot image classification to find best captions to generate an image. \",\n            id: \"pharma/CLIP-Interrogator\",\n        },\n        {\n            description: \"An application to compare different zero-shot image classification models. \",\n            id: \"merve/compare_clip_siglip\",\n        },\n    ],\n    summary: \"Zero-shot image classification is the task of classifying previously unseen classes during training of a model.\",\n    widgetModels: [\"google/siglip-so400m-patch14-224\"],\n    youtubeId: \"\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,gCAAgC;YAChC,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,UAAU;gBACV,MAAM;YACV;YACA;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,MAAM;gBACN,MAAM;oBACF;wBACI,OAAO;wBACP,OAAO;oBACX;oBACA;wBACI,OAAO;wBACP,OAAO;oBACX;oBACA;wBACI,OAAO;wBACP,OAAO;oBACX;iBACH;YACL;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAAmC;IAClD,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 5351, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/zero-shot-object-detection/data.js"],"sourcesContent":["const taskData = {\n    datasets: [],\n    demo: {\n        inputs: [\n            {\n                filename: \"zero-shot-object-detection-input.jpg\",\n                type: \"img\",\n            },\n            {\n                label: \"Classes\",\n                content: \"cat, dog, bird\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                filename: \"zero-shot-object-detection-output.jpg\",\n                type: \"img\",\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"The Average Precision (AP) metric is the Area Under the PR Curve (AUC-PR). It is calculated for each class separately\",\n            id: \"Average Precision\",\n        },\n        {\n            description: \"The Mean Average Precision (mAP) metric is the overall average of the AP values\",\n            id: \"Mean Average Precision\",\n        },\n        {\n            description: \"The APα metric is the Average Precision at the IoU threshold of a α value, for example, AP50 and AP75\",\n            id: \"APα\",\n        },\n    ],\n    models: [\n        {\n            description: \"Solid zero-shot object detection model.\",\n            id: \"openmmlab-community/mm_grounding_dino_large_all\",\n        },\n        {\n            description: \"Cutting-edge zero-shot object detection model.\",\n            id: \"fushh7/LLMDet\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"A demo to compare different zero-shot object detection models per output and latency.\",\n            id: \"ariG23498/zero-shot-od\",\n        },\n        {\n            description: \"A demo that combines a zero-shot object detection and mask generation model for zero-shot segmentation.\",\n            id: \"merve/OWLSAM\",\n        },\n    ],\n    summary: \"Zero-shot object detection is a computer vision task to detect objects and their classes in images, without any prior training or knowledge of the classes. Zero-shot object detection models receive an image as input, as well as a list of candidate classes, and output the bounding boxes and labels where the objects have been detected.\",\n    widgetModels: [],\n    youtubeId: \"\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU,EAAE;IACZ,MAAM;QACF,QAAQ;YACJ;gBACI,UAAU;gBACV,MAAM;YACV;YACA;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,UAAU;gBACV,MAAM;YACV;SACH;IACL;IACA,SAAS;QACL;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc,EAAE;IAChB,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 5419, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/image-to-3d/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"A large dataset of over 10 million 3D objects.\",\n            id: \"allenai/objaverse-xl\",\n        },\n        {\n            description: \"A dataset of isolated object images for evaluating image-to-3D models.\",\n            id: \"dylanebert/iso3d\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                filename: \"image-to-3d-image-input.png\",\n                type: \"img\",\n            },\n        ],\n        outputs: [\n            {\n                label: \"Result\",\n                content: \"image-to-3d-3d-output-filename.glb\",\n                type: \"text\",\n            },\n        ],\n    },\n    metrics: [],\n    models: [\n        {\n            description: \"Fast image-to-3D mesh model by Tencent.\",\n            id: \"TencentARC/InstantMesh\",\n        },\n        {\n            description: \"3D world generation model.\",\n            id: \"tencent/HunyuanWorld-1\",\n        },\n        {\n            description: \"A scaled up image-to-3D mesh model derived from TripoSR.\",\n            id: \"hwjiang/Real3D\",\n        },\n        {\n            description: \"Consistent image-to-3d generation model.\",\n            id: \"stabilityai/stable-point-aware-3d\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"Leaderboard to evaluate image-to-3D models.\",\n            id: \"dylanebert/3d-arena\",\n        },\n        {\n            description: \"Image-to-3D demo with mesh outputs.\",\n            id: \"TencentARC/InstantMesh\",\n        },\n        {\n            description: \"Image-to-3D demo.\",\n            id: \"stabilityai/stable-point-aware-3d\",\n        },\n        {\n            description: \"Image-to-3D demo with mesh outputs.\",\n            id: \"hwjiang/Real3D\",\n        },\n        {\n            description: \"Image-to-3D demo with splat outputs.\",\n            id: \"dylanebert/LGM-mini\",\n        },\n    ],\n    summary: \"Image-to-3D models take in image input and produce 3D output.\",\n    widgetModels: [],\n    youtubeId: \"\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,UAAU;gBACV,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;IACL;IACA,SAAS,EAAE;IACX,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc,EAAE;IAChB,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 5499, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/text-to-3d/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"A large dataset of over 10 million 3D objects.\",\n            id: \"allenai/objaverse-xl\",\n        },\n        {\n            description: \"Descriptive captions for 3D objects in Objaverse.\",\n            id: \"tiange/Cap3D\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                label: \"Prompt\",\n                content: \"a cat statue\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                label: \"Result\",\n                content: \"text-to-3d-3d-output-filename.glb\",\n                type: \"text\",\n            },\n        ],\n    },\n    metrics: [],\n    models: [\n        {\n            description: \"Text-to-3D mesh model by OpenAI\",\n            id: \"openai/shap-e\",\n        },\n        {\n            description: \"Generative 3D gaussian splatting model.\",\n            id: \"ashawkey/LGM\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"Text-to-3D demo with mesh outputs.\",\n            id: \"hysts/Shap-E\",\n        },\n        {\n            description: \"Text/image-to-3D demo with splat outputs.\",\n            id: \"ashawkey/LGM\",\n        },\n    ],\n    summary: \"Text-to-3D models take in text input and produce 3D output.\",\n    widgetModels: [],\n    youtubeId: \"\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;IACL;IACA,SAAS,EAAE;IACX,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc,EAAE;IAChB,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 5560, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/keypoint-detection/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"A dataset of hand keypoints of over 500k examples.\",\n            id: \"Vincent-luo/hagrid-mediapipe-hands\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                filename: \"keypoint-detection-input.png\",\n                type: \"img\",\n            },\n        ],\n        outputs: [\n            {\n                filename: \"keypoint-detection-output.png\",\n                type: \"img\",\n            },\n        ],\n    },\n    metrics: [],\n    models: [\n        {\n            description: \"A robust keypoint detection model.\",\n            id: \"magic-leap-community/superpoint\",\n        },\n        {\n            description: \"A robust keypoint matching model.\",\n            id: \"magic-leap-community/superglue_outdoor\",\n        },\n        {\n            description: \"Strong keypoint detection model used to detect human pose.\",\n            id: \"qualcomm/RTMPose-Body2d\",\n        },\n        {\n            description: \"Powerful keypoint matching model.\",\n            id: \"ETH-CVG/lightglue_disk\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application that detects hand keypoints in real-time.\",\n            id: \"datasciencedojo/Hand-Keypoint-Detection-Realtime\",\n        },\n        {\n            description: \"An application for keypoint detection and matching.\",\n            id: \"ETH-CVG/LightGlue\",\n        },\n    ],\n    summary: \"Keypoint detection is the task of identifying meaningful distinctive points or features in an image.\",\n    widgetModels: [],\n    youtubeId: \"\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,UAAU;gBACV,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,UAAU;gBACV,MAAM;YACV;SACH;IACL;IACA,SAAS,EAAE;IACX,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc,EAAE;IAChB,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 5623, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/video-text-to-text/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"Multiple-choice questions and answers about videos.\",\n            id: \"lmms-lab/Video-MME\",\n        },\n        {\n            description: \"A dataset of instructions and question-answer pairs about videos.\",\n            id: \"lmms-lab/VideoChatGPT\",\n        },\n        {\n            description: \"Large video understanding dataset.\",\n            id: \"HuggingFaceFV/finevideo\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                filename: \"video-text-to-text-input.gif\",\n                type: \"img\",\n            },\n            {\n                label: \"Text Prompt\",\n                content: \"What is happening in this video?\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                label: \"Answer\",\n                content: \"The video shows a series of images showing a fountain with water jets and a variety of colorful flowers and butterflies in the background.\",\n                type: \"text\",\n            },\n        ],\n    },\n    metrics: [],\n    models: [\n        {\n            description: \"A robust video-text-to-text model.\",\n            id: \"Vision-CAIR/LongVU_Qwen2_7B\",\n        },\n        {\n            description: \"Strong video-text-to-text model with reasoning capabilities.\",\n            id: \"GoodiesHere/Apollo-LMMs-Apollo-7B-t32\",\n        },\n        {\n            description: \"Strong video-text-to-text model.\",\n            id: \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application to chat with a video-text-to-text model.\",\n            id: \"llava-hf/video-llava\",\n        },\n        {\n            description: \"A leaderboard for various video-text-to-text models.\",\n            id: \"opencompass/openvlm_video_leaderboard\",\n        },\n        {\n            description: \"An application to generate highlights from a video.\",\n            id: \"HuggingFaceTB/SmolVLM2-HighlightGenerator\",\n        },\n    ],\n    summary: \"Video-text-to-text models take in a video and a text prompt and output text. These models are also called video-language models.\",\n    widgetModels: [\"\"],\n    youtubeId: \"\",\n};\nexport default taskData;\n"],"names":[],"mappings":";;;;AAAA,MAAM,WAAW;IACb,UAAU;QACN;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,MAAM;QACF,QAAQ;YACJ;gBACI,UAAU;gBACV,MAAM;YACV;YACA;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;QACD,SAAS;YACL;gBACI,OAAO;gBACP,SAAS;gBACT,MAAM;YACV;SACH;IACL;IACA,SAAS,EAAE;IACX,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,QAAQ;QACJ;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;QACA;YACI,aAAa;YACb,IAAI;QACR;KACH;IACD,SAAS;IACT,cAAc;QAAC;KAAG;IAClB,WAAW;AACf;uCACe","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 5702, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tasks/index.js"],"sourcesContent":["import { PIPELINE_DATA } from \"../pipelines.js\";\nimport anyToAny from \"./any-to-any/data.js\";\nimport audioClassification from \"./audio-classification/data.js\";\nimport audioTextToText from \"./audio-text-to-text/data.js\";\nimport audioToAudio from \"./audio-to-audio/data.js\";\nimport automaticSpeechRecognition from \"./automatic-speech-recognition/data.js\";\nimport documentQuestionAnswering from \"./document-question-answering/data.js\";\nimport featureExtraction from \"./feature-extraction/data.js\";\nimport fillMask from \"./fill-mask/data.js\";\nimport imageClassification from \"./image-classification/data.js\";\nimport imageFeatureExtraction from \"./image-feature-extraction/data.js\";\nimport imageToImage from \"./image-to-image/data.js\";\nimport imageToText from \"./image-to-text/data.js\";\nimport imageTextToText from \"./image-text-to-text/data.js\";\nimport imageSegmentation from \"./image-segmentation/data.js\";\nimport imageToVideo from \"./image-to-video/data.js\";\nimport maskGeneration from \"./mask-generation/data.js\";\nimport objectDetection from \"./object-detection/data.js\";\nimport depthEstimation from \"./depth-estimation/data.js\";\nimport placeholder from \"./placeholder/data.js\";\nimport reinforcementLearning from \"./reinforcement-learning/data.js\";\nimport questionAnswering from \"./question-answering/data.js\";\nimport sentenceSimilarity from \"./sentence-similarity/data.js\";\nimport summarization from \"./summarization/data.js\";\nimport tableQuestionAnswering from \"./table-question-answering/data.js\";\nimport tabularClassification from \"./tabular-classification/data.js\";\nimport tabularRegression from \"./tabular-regression/data.js\";\nimport textToImage from \"./text-to-image/data.js\";\nimport textToSpeech from \"./text-to-speech/data.js\";\nimport tokenClassification from \"./token-classification/data.js\";\nimport translation from \"./translation/data.js\";\nimport textClassification from \"./text-classification/data.js\";\nimport textGeneration from \"./text-generation/data.js\";\nimport textRanking from \"./text-ranking/data.js\";\nimport textToVideo from \"./text-to-video/data.js\";\nimport unconditionalImageGeneration from \"./unconditional-image-generation/data.js\";\nimport videoClassification from \"./video-classification/data.js\";\nimport visualDocumentRetrieval from \"./visual-document-retrieval/data.js\";\nimport visualQuestionAnswering from \"./visual-question-answering/data.js\";\nimport zeroShotClassification from \"./zero-shot-classification/data.js\";\nimport zeroShotImageClassification from \"./zero-shot-image-classification/data.js\";\nimport zeroShotObjectDetection from \"./zero-shot-object-detection/data.js\";\nimport imageTo3D from \"./image-to-3d/data.js\";\nimport textTo3D from \"./text-to-3d/data.js\";\nimport keypointDetection from \"./keypoint-detection/data.js\";\nimport videoTextToText from \"./video-text-to-text/data.js\";\n/**\n * Model libraries compatible with each ML task\n */\nexport const TASKS_MODEL_LIBRARIES = {\n    \"audio-classification\": [\"speechbrain\", \"transformers\", \"transformers.js\"],\n    \"audio-to-audio\": [\"asteroid\", \"fairseq\", \"speechbrain\"],\n    \"automatic-speech-recognition\": [\"espnet\", \"nemo\", \"speechbrain\", \"transformers\", \"transformers.js\"],\n    \"audio-text-to-text\": [\"transformers\"],\n    \"depth-estimation\": [\"transformers\", \"transformers.js\"],\n    \"document-question-answering\": [\"transformers\", \"transformers.js\"],\n    \"feature-extraction\": [\"sentence-transformers\", \"transformers\", \"transformers.js\"],\n    \"fill-mask\": [\"transformers\", \"transformers.js\"],\n    \"graph-ml\": [\"transformers\"],\n    \"image-classification\": [\"keras\", \"timm\", \"transformers\", \"transformers.js\"],\n    \"image-feature-extraction\": [\"timm\", \"transformers\"],\n    \"image-segmentation\": [\"transformers\", \"transformers.js\"],\n    \"image-text-to-text\": [\"transformers\"],\n    \"image-to-image\": [\"diffusers\", \"transformers\", \"transformers.js\"],\n    \"image-to-text\": [\"transformers\", \"transformers.js\"],\n    \"image-to-video\": [\"diffusers\"],\n    \"keypoint-detection\": [\"transformers\"],\n    \"video-classification\": [\"transformers\"],\n    \"mask-generation\": [\"transformers\"],\n    \"multiple-choice\": [\"transformers\"],\n    \"object-detection\": [\"transformers\", \"transformers.js\", \"ultralytics\"],\n    other: [],\n    \"question-answering\": [\"adapter-transformers\", \"allennlp\", \"transformers\", \"transformers.js\"],\n    robotics: [],\n    \"reinforcement-learning\": [\"transformers\", \"stable-baselines3\", \"ml-agents\", \"sample-factory\"],\n    \"sentence-similarity\": [\"sentence-transformers\", \"spacy\", \"transformers.js\"],\n    summarization: [\"transformers\", \"transformers.js\"],\n    \"table-question-answering\": [\"transformers\"],\n    \"table-to-text\": [\"transformers\"],\n    \"tabular-classification\": [\"sklearn\"],\n    \"tabular-regression\": [\"sklearn\"],\n    \"tabular-to-text\": [\"transformers\"],\n    \"text-classification\": [\"adapter-transformers\", \"setfit\", \"spacy\", \"transformers\", \"transformers.js\"],\n    \"text-generation\": [\"transformers\", \"transformers.js\"],\n    \"text-ranking\": [\"sentence-transformers\", \"transformers\"],\n    \"text-retrieval\": [],\n    \"text-to-image\": [\"diffusers\"],\n    \"text-to-speech\": [\"espnet\", \"tensorflowtts\", \"transformers\", \"transformers.js\"],\n    \"text-to-audio\": [\"transformers\", \"transformers.js\"],\n    \"text-to-video\": [\"diffusers\"],\n    \"time-series-forecasting\": [],\n    \"token-classification\": [\n        \"adapter-transformers\",\n        \"flair\",\n        \"spacy\",\n        \"span-marker\",\n        \"stanza\",\n        \"transformers\",\n        \"transformers.js\",\n    ],\n    translation: [\"transformers\", \"transformers.js\"],\n    \"unconditional-image-generation\": [\"diffusers\"],\n    \"video-text-to-text\": [\"transformers\"],\n    \"visual-question-answering\": [\"transformers\", \"transformers.js\"],\n    \"voice-activity-detection\": [],\n    \"zero-shot-classification\": [\"transformers\", \"transformers.js\"],\n    \"zero-shot-image-classification\": [\"transformers\", \"transformers.js\"],\n    \"zero-shot-object-detection\": [\"transformers\", \"transformers.js\"],\n    \"text-to-3d\": [\"diffusers\"],\n    \"image-to-3d\": [\"diffusers\"],\n    \"any-to-any\": [\"transformers\"],\n    \"visual-document-retrieval\": [\"transformers\"],\n    \"video-to-video\": [\"diffusers\"],\n};\n/**\n * Return the whole TaskData object for a certain task.\n * If the partialTaskData argument is left undefined,\n * the default placeholder data will be used.\n */\nfunction getData(type, partialTaskData = placeholder) {\n    return {\n        ...partialTaskData,\n        id: type,\n        label: PIPELINE_DATA[type].name,\n        libraries: TASKS_MODEL_LIBRARIES[type],\n    };\n}\n// To make comparisons easier, task order is the same as in const.ts\n// Tasks set to undefined won't have an associated task page.\n// Tasks that call getData() without the second argument will\n// have a \"placeholder\" page.\nexport const TASKS_DATA = {\n    \"any-to-any\": getData(\"any-to-any\", anyToAny),\n    \"audio-classification\": getData(\"audio-classification\", audioClassification),\n    \"audio-to-audio\": getData(\"audio-to-audio\", audioToAudio),\n    \"audio-text-to-text\": getData(\"audio-text-to-text\", audioTextToText),\n    \"automatic-speech-recognition\": getData(\"automatic-speech-recognition\", automaticSpeechRecognition),\n    \"depth-estimation\": getData(\"depth-estimation\", depthEstimation),\n    \"document-question-answering\": getData(\"document-question-answering\", documentQuestionAnswering),\n    \"visual-document-retrieval\": getData(\"visual-document-retrieval\", visualDocumentRetrieval),\n    \"feature-extraction\": getData(\"feature-extraction\", featureExtraction),\n    \"fill-mask\": getData(\"fill-mask\", fillMask),\n    \"graph-ml\": undefined,\n    \"image-classification\": getData(\"image-classification\", imageClassification),\n    \"image-feature-extraction\": getData(\"image-feature-extraction\", imageFeatureExtraction),\n    \"image-segmentation\": getData(\"image-segmentation\", imageSegmentation),\n    \"image-to-image\": getData(\"image-to-image\", imageToImage),\n    \"image-text-to-text\": getData(\"image-text-to-text\", imageTextToText),\n    \"image-to-text\": getData(\"image-to-text\", imageToText),\n    \"image-to-video\": getData(\"image-to-video\", imageToVideo),\n    \"keypoint-detection\": getData(\"keypoint-detection\", keypointDetection),\n    \"mask-generation\": getData(\"mask-generation\", maskGeneration),\n    \"multiple-choice\": undefined,\n    \"object-detection\": getData(\"object-detection\", objectDetection),\n    \"video-classification\": getData(\"video-classification\", videoClassification),\n    other: undefined,\n    \"question-answering\": getData(\"question-answering\", questionAnswering),\n    \"reinforcement-learning\": getData(\"reinforcement-learning\", reinforcementLearning),\n    robotics: undefined,\n    \"sentence-similarity\": getData(\"sentence-similarity\", sentenceSimilarity),\n    summarization: getData(\"summarization\", summarization),\n    \"table-question-answering\": getData(\"table-question-answering\", tableQuestionAnswering),\n    \"table-to-text\": undefined,\n    \"tabular-classification\": getData(\"tabular-classification\", tabularClassification),\n    \"tabular-regression\": getData(\"tabular-regression\", tabularRegression),\n    \"tabular-to-text\": undefined,\n    \"text-classification\": getData(\"text-classification\", textClassification),\n    \"text-generation\": getData(\"text-generation\", textGeneration),\n    \"text-ranking\": getData(\"text-ranking\", textRanking),\n    \"text-retrieval\": undefined,\n    \"text-to-image\": getData(\"text-to-image\", textToImage),\n    \"text-to-speech\": getData(\"text-to-speech\", textToSpeech),\n    \"text-to-audio\": undefined,\n    \"text-to-video\": getData(\"text-to-video\", textToVideo),\n    \"time-series-forecasting\": undefined,\n    \"token-classification\": getData(\"token-classification\", tokenClassification),\n    translation: getData(\"translation\", translation),\n    \"unconditional-image-generation\": getData(\"unconditional-image-generation\", unconditionalImageGeneration),\n    \"video-text-to-text\": getData(\"video-text-to-text\", videoTextToText),\n    \"video-to-video\": getData(\"video-to-video\", placeholder),\n    \"visual-question-answering\": getData(\"visual-question-answering\", visualQuestionAnswering),\n    \"voice-activity-detection\": undefined,\n    \"zero-shot-classification\": getData(\"zero-shot-classification\", zeroShotClassification),\n    \"zero-shot-image-classification\": getData(\"zero-shot-image-classification\", zeroShotImageClassification),\n    \"zero-shot-object-detection\": getData(\"zero-shot-object-detection\", zeroShotObjectDetection),\n    \"text-to-3d\": getData(\"text-to-3d\", textTo3D),\n    \"image-to-3d\": getData(\"image-to-3d\", imageTo3D),\n};\n"],"names":[],"mappings":";;;;;;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAIO,MAAM,wBAAwB;IACjC,wBAAwB;QAAC;QAAe;QAAgB;KAAkB;IAC1E,kBAAkB;QAAC;QAAY;QAAW;KAAc;IACxD,gCAAgC;QAAC;QAAU;QAAQ;QAAe;QAAgB;KAAkB;IACpG,sBAAsB;QAAC;KAAe;IACtC,oBAAoB;QAAC;QAAgB;KAAkB;IACvD,+BAA+B;QAAC;QAAgB;KAAkB;IAClE,sBAAsB;QAAC;QAAyB;QAAgB;KAAkB;IAClF,aAAa;QAAC;QAAgB;KAAkB;IAChD,YAAY;QAAC;KAAe;IAC5B,wBAAwB;QAAC;QAAS;QAAQ;QAAgB;KAAkB;IAC5E,4BAA4B;QAAC;QAAQ;KAAe;IACpD,sBAAsB;QAAC;QAAgB;KAAkB;IACzD,sBAAsB;QAAC;KAAe;IACtC,kBAAkB;QAAC;QAAa;QAAgB;KAAkB;IAClE,iBAAiB;QAAC;QAAgB;KAAkB;IACpD,kBAAkB;QAAC;KAAY;IAC/B,sBAAsB;QAAC;KAAe;IACtC,wBAAwB;QAAC;KAAe;IACxC,mBAAmB;QAAC;KAAe;IACnC,mBAAmB;QAAC;KAAe;IACnC,oBAAoB;QAAC;QAAgB;QAAmB;KAAc;IACtE,OAAO,EAAE;IACT,sBAAsB;QAAC;QAAwB;QAAY;QAAgB;KAAkB;IAC7F,UAAU,EAAE;IACZ,0BAA0B;QAAC;QAAgB;QAAqB;QAAa;KAAiB;IAC9F,uBAAuB;QAAC;QAAyB;QAAS;KAAkB;IAC5E,eAAe;QAAC;QAAgB;KAAkB;IAClD,4BAA4B;QAAC;KAAe;IAC5C,iBAAiB;QAAC;KAAe;IACjC,0BAA0B;QAAC;KAAU;IACrC,sBAAsB;QAAC;KAAU;IACjC,mBAAmB;QAAC;KAAe;IACnC,uBAAuB;QAAC;QAAwB;QAAU;QAAS;QAAgB;KAAkB;IACrG,mBAAmB;QAAC;QAAgB;KAAkB;IACtD,gBAAgB;QAAC;QAAyB;KAAe;IACzD,kBAAkB,EAAE;IACpB,iBAAiB;QAAC;KAAY;IAC9B,kBAAkB;QAAC;QAAU;QAAiB;QAAgB;KAAkB;IAChF,iBAAiB;QAAC;QAAgB;KAAkB;IACpD,iBAAiB;QAAC;KAAY;IAC9B,2BAA2B,EAAE;IAC7B,wBAAwB;QACpB;QACA;QACA;QACA;QACA;QACA;QACA;KACH;IACD,aAAa;QAAC;QAAgB;KAAkB;IAChD,kCAAkC;QAAC;KAAY;IAC/C,sBAAsB;QAAC;KAAe;IACtC,6BAA6B;QAAC;QAAgB;KAAkB;IAChE,4BAA4B,EAAE;IAC9B,4BAA4B;QAAC;QAAgB;KAAkB;IAC/D,kCAAkC;QAAC;QAAgB;KAAkB;IACrE,8BAA8B;QAAC;QAAgB;KAAkB;IACjE,cAAc;QAAC;KAAY;IAC3B,eAAe;QAAC;KAAY;IAC5B,cAAc;QAAC;KAAe;IAC9B,6BAA6B;QAAC;KAAe;IAC7C,kBAAkB;QAAC;KAAY;AACnC;AACA;;;;CAIC,GACD,SAAS,QAAQ,IAAI,EAAE,kBAAkB,kMAAW;IAChD,OAAO;QACH,GAAG,eAAe;QAClB,IAAI;QACJ,OAAO,qLAAa,CAAC,KAAK,CAAC,IAAI;QAC/B,WAAW,qBAAqB,CAAC,KAAK;IAC1C;AACJ;AAKO,MAAM,aAAa;IACtB,cAAc,QAAQ,cAAc,uMAAQ;IAC5C,wBAAwB,QAAQ,wBAAwB,8MAAmB;IAC3E,kBAAkB,QAAQ,kBAAkB,2MAAY;IACxD,sBAAsB,QAAQ,sBAAsB,kNAAe;IACnE,gCAAgC,QAAQ,gCAAgC,yNAA0B;IAClG,oBAAoB,QAAQ,oBAAoB,0MAAe;IAC/D,+BAA+B,QAAQ,+BAA+B,wNAAyB;IAC/F,6BAA6B,QAAQ,6BAA6B,sNAAuB;IACzF,sBAAsB,QAAQ,sBAAsB,4MAAiB;IACrE,aAAa,QAAQ,aAAa,mMAAQ;IAC1C,YAAY;IACZ,wBAAwB,QAAQ,wBAAwB,8MAAmB;IAC3E,4BAA4B,QAAQ,4BAA4B,qNAAsB;IACtF,sBAAsB,QAAQ,sBAAsB,4MAAiB;IACrE,kBAAkB,QAAQ,kBAAkB,2MAAY;IACxD,sBAAsB,QAAQ,sBAAsB,kNAAe;IACnE,iBAAiB,QAAQ,iBAAiB,0MAAW;IACrD,kBAAkB,QAAQ,kBAAkB,2MAAY;IACxD,sBAAsB,QAAQ,sBAAsB,4MAAiB;IACrE,mBAAmB,QAAQ,mBAAmB,yMAAc;IAC5D,mBAAmB;IACnB,oBAAoB,QAAQ,oBAAoB,0MAAe;IAC/D,wBAAwB,QAAQ,wBAAwB,8MAAmB;IAC3E,OAAO;IACP,sBAAsB,QAAQ,sBAAsB,4MAAiB;IACrE,0BAA0B,QAAQ,0BAA0B,gNAAqB;IACjF,UAAU;IACV,uBAAuB,QAAQ,uBAAuB,6MAAkB;IACxE,eAAe,QAAQ,iBAAiB,oMAAa;IACrD,4BAA4B,QAAQ,4BAA4B,qNAAsB;IACtF,iBAAiB;IACjB,0BAA0B,QAAQ,0BAA0B,gNAAqB;IACjF,sBAAsB,QAAQ,sBAAsB,4MAAiB;IACrE,mBAAmB;IACnB,uBAAuB,QAAQ,uBAAuB,6MAAkB;IACxE,mBAAmB,QAAQ,mBAAmB,yMAAc;IAC5D,gBAAgB,QAAQ,gBAAgB,sMAAW;IACnD,kBAAkB;IAClB,iBAAiB,QAAQ,iBAAiB,0MAAW;IACrD,kBAAkB,QAAQ,kBAAkB,2MAAY;IACxD,iBAAiB;IACjB,iBAAiB,QAAQ,iBAAiB,0MAAW;IACrD,2BAA2B;IAC3B,wBAAwB,QAAQ,wBAAwB,8MAAmB;IAC3E,aAAa,QAAQ,eAAe,kMAAW;IAC/C,kCAAkC,QAAQ,kCAAkC,2NAA4B;IACxG,sBAAsB,QAAQ,sBAAsB,kNAAe;IACnE,kBAAkB,QAAQ,kBAAkB,kMAAW;IACvD,6BAA6B,QAAQ,6BAA6B,sNAAuB;IACzF,4BAA4B;IAC5B,4BAA4B,QAAQ,4BAA4B,qNAAsB;IACtF,kCAAkC,QAAQ,kCAAkC,8NAA2B;IACvG,8BAA8B,QAAQ,8BAA8B,0NAAuB;IAC3F,cAAc,QAAQ,cAAc,uMAAQ;IAC5C,eAAe,QAAQ,eAAe,wMAAS;AACnD","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 6083, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/snippets/inputs.js"],"sourcesContent":["const inputsZeroShotClassification = () => `\"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!\"`;\nconst inputsTranslation = () => `\"Меня зовут Вольфганг и я живу в Берлине\"`;\nconst inputsSummarization = () => `\"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\"`;\nconst inputsTableQuestionAnswering = () => `{\n    \"query\": \"How many stars does the transformers repository have?\",\n    \"table\": {\n        \"Repository\": [\"Transformers\", \"Datasets\", \"Tokenizers\"],\n        \"Stars\": [\"36542\", \"4512\", \"3934\"],\n        \"Contributors\": [\"651\", \"77\", \"34\"],\n        \"Programming language\": [\n            \"Python\",\n            \"Python\",\n            \"Rust, Python and NodeJS\"\n        ]\n    }\n}`;\nconst inputsVisualQuestionAnswering = () => `{\n        \"image\": \"cat.png\",\n        \"question\": \"What is in this image?\"\n    }`;\nconst inputsQuestionAnswering = () => `{\n    \"question\": \"What is my name?\",\n    \"context\": \"My name is Clara and I live in Berkeley.\"\n}`;\nconst inputsTextClassification = () => `\"I like you. I love you\"`;\nconst inputsTokenClassification = () => `\"My name is Sarah Jessica Parker but you can call me Jessica\"`;\nconst inputsTextGeneration = (model) => {\n    if (model.tags.includes(\"conversational\")) {\n        return model.pipeline_tag === \"text-generation\"\n            ? [{ role: \"user\", content: \"What is the capital of France?\" }]\n            : [\n                {\n                    role: \"user\",\n                    content: [\n                        {\n                            type: \"text\",\n                            text: \"Describe this image in one sentence.\",\n                        },\n                        {\n                            type: \"image_url\",\n                            image_url: {\n                                url: \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\",\n                            },\n                        },\n                    ],\n                },\n            ];\n    }\n    return `\"Can you please let us know more details about your \"`;\n};\nconst inputsFillMask = (model) => `\"The answer to the universe is ${model.mask_token}.\"`;\nconst inputsSentenceSimilarity = () => `{\n    \"source_sentence\": \"That is a happy person\",\n    \"sentences\": [\n        \"That is a happy dog\",\n        \"That is a very happy person\",\n        \"Today is a sunny day\"\n    ]\n}`;\nconst inputsFeatureExtraction = () => `\"Today is a sunny day and I will get some ice cream.\"`;\nconst inputsImageClassification = () => `\"cats.jpg\"`;\nconst inputsImageToText = () => `\"cats.jpg\"`;\nconst inputsImageToImage = () => `{\n    \"image\": \"cat.png\",\n    \"prompt\": \"Turn the cat into a tiger.\"\n}`;\nconst inputsImageToVideo = () => `{\n    \"image\": \"cat.png\",\n    \"prompt\": \"The cat starts to dance\"\n}`;\nconst inputsImageSegmentation = () => `\"cats.jpg\"`;\nconst inputsObjectDetection = () => `\"cats.jpg\"`;\nconst inputsAudioToAudio = () => `\"sample1.flac\"`;\nconst inputsAudioClassification = () => `\"sample1.flac\"`;\nconst inputsTextToImage = () => `\"Astronaut riding a horse\"`;\nconst inputsTextToVideo = () => `\"A young man walking on the street\"`;\nconst inputsTextToSpeech = () => `\"The answer to the universe is 42\"`;\nconst inputsTextToAudio = () => `\"liquid drum and bass, atmospheric synths, airy sounds\"`;\nconst inputsAutomaticSpeechRecognition = () => `\"sample1.flac\"`;\nconst inputsTabularPrediction = () => `'{\"Height\":[11.52,12.48],\"Length1\":[23.2,24.0],\"Length2\":[25.4,26.3],\"Species\": [\"Bream\",\"Bream\"]}'`;\nconst inputsZeroShotImageClassification = () => `\"cats.jpg\"`;\nconst modelInputSnippets = {\n    \"audio-to-audio\": inputsAudioToAudio,\n    \"audio-classification\": inputsAudioClassification,\n    \"automatic-speech-recognition\": inputsAutomaticSpeechRecognition,\n    \"document-question-answering\": inputsVisualQuestionAnswering,\n    \"feature-extraction\": inputsFeatureExtraction,\n    \"fill-mask\": inputsFillMask,\n    \"image-classification\": inputsImageClassification,\n    \"image-to-text\": inputsImageToText,\n    \"image-to-image\": inputsImageToImage,\n    \"image-to-video\": inputsImageToVideo,\n    \"image-segmentation\": inputsImageSegmentation,\n    \"object-detection\": inputsObjectDetection,\n    \"question-answering\": inputsQuestionAnswering,\n    \"sentence-similarity\": inputsSentenceSimilarity,\n    summarization: inputsSummarization,\n    \"table-question-answering\": inputsTableQuestionAnswering,\n    \"tabular-regression\": inputsTabularPrediction,\n    \"tabular-classification\": inputsTabularPrediction,\n    \"text-classification\": inputsTextClassification,\n    \"text-generation\": inputsTextGeneration,\n    \"image-text-to-text\": inputsTextGeneration,\n    \"text-to-image\": inputsTextToImage,\n    \"text-to-video\": inputsTextToVideo,\n    \"text-to-speech\": inputsTextToSpeech,\n    \"text-to-audio\": inputsTextToAudio,\n    \"token-classification\": inputsTokenClassification,\n    translation: inputsTranslation,\n    \"zero-shot-classification\": inputsZeroShotClassification,\n    \"zero-shot-image-classification\": inputsZeroShotImageClassification,\n};\n// Use noWrap to put the whole snippet on a single line (removing new lines and tabulations)\n// Use noQuotes to strip quotes from start & end (example: \"abc\" -> abc)\nexport function getModelInputSnippet(model, noWrap = false, noQuotes = false) {\n    if (model.pipeline_tag) {\n        const inputs = modelInputSnippets[model.pipeline_tag];\n        if (inputs) {\n            let result = inputs(model);\n            if (typeof result === \"string\") {\n                if (noWrap) {\n                    result = result.replace(/(?:(?:\\r?\\n|\\r)\\t*)|\\t+/g, \" \");\n                }\n                if (noQuotes) {\n                    const REGEX_QUOTES = /^\"(.+)\"$/s;\n                    const match = result.match(REGEX_QUOTES);\n                    result = match ? match[1] : result;\n                }\n            }\n            return result;\n        }\n    }\n    return \"No input example has been defined for this model task.\";\n}\n"],"names":[],"mappings":";;;;AAAA,MAAM,+BAA+B,IAAM,CAAC,0HAA0H,CAAC;AACvK,MAAM,oBAAoB,IAAM,CAAC,yCAAyC,CAAC;AAC3E,MAAM,sBAAsB,IAAM,CAAC,yuBAAyuB,CAAC;AAC7wB,MAAM,+BAA+B,IAAM,CAAC;;;;;;;;;;;;CAY3C,CAAC;AACF,MAAM,gCAAgC,IAAM,CAAC;;;KAGxC,CAAC;AACN,MAAM,0BAA0B,IAAM,CAAC;;;CAGtC,CAAC;AACF,MAAM,2BAA2B,IAAM,CAAC,wBAAwB,CAAC;AACjE,MAAM,4BAA4B,IAAM,CAAC,6DAA6D,CAAC;AACvG,MAAM,uBAAuB,CAAC;IAC1B,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,mBAAmB;QACvC,OAAO,MAAM,YAAY,KAAK,oBACxB;YAAC;gBAAE,MAAM;gBAAQ,SAAS;YAAiC;SAAE,GAC7D;YACE;gBACI,MAAM;gBACN,SAAS;oBACL;wBACI,MAAM;wBACN,MAAM;oBACV;oBACA;wBACI,MAAM;wBACN,WAAW;4BACP,KAAK;wBACT;oBACJ;iBACH;YACL;SACH;IACT;IACA,OAAO,CAAC,qDAAqD,CAAC;AAClE;AACA,MAAM,iBAAiB,CAAC,QAAU,CAAC,+BAA+B,EAAE,MAAM,UAAU,CAAC,EAAE,CAAC;AACxF,MAAM,2BAA2B,IAAM,CAAC;;;;;;;CAOvC,CAAC;AACF,MAAM,0BAA0B,IAAM,CAAC,qDAAqD,CAAC;AAC7F,MAAM,4BAA4B,IAAM,CAAC,UAAU,CAAC;AACpD,MAAM,oBAAoB,IAAM,CAAC,UAAU,CAAC;AAC5C,MAAM,qBAAqB,IAAM,CAAC;;;CAGjC,CAAC;AACF,MAAM,qBAAqB,IAAM,CAAC;;;CAGjC,CAAC;AACF,MAAM,0BAA0B,IAAM,CAAC,UAAU,CAAC;AAClD,MAAM,wBAAwB,IAAM,CAAC,UAAU,CAAC;AAChD,MAAM,qBAAqB,IAAM,CAAC,cAAc,CAAC;AACjD,MAAM,4BAA4B,IAAM,CAAC,cAAc,CAAC;AACxD,MAAM,oBAAoB,IAAM,CAAC,0BAA0B,CAAC;AAC5D,MAAM,oBAAoB,IAAM,CAAC,mCAAmC,CAAC;AACrE,MAAM,qBAAqB,IAAM,CAAC,kCAAkC,CAAC;AACrE,MAAM,oBAAoB,IAAM,CAAC,uDAAuD,CAAC;AACzF,MAAM,mCAAmC,IAAM,CAAC,cAAc,CAAC;AAC/D,MAAM,0BAA0B,IAAM,CAAC,mGAAmG,CAAC;AAC3I,MAAM,oCAAoC,IAAM,CAAC,UAAU,CAAC;AAC5D,MAAM,qBAAqB;IACvB,kBAAkB;IAClB,wBAAwB;IACxB,gCAAgC;IAChC,+BAA+B;IAC/B,sBAAsB;IACtB,aAAa;IACb,wBAAwB;IACxB,iBAAiB;IACjB,kBAAkB;IAClB,kBAAkB;IAClB,sBAAsB;IACtB,oBAAoB;IACpB,sBAAsB;IACtB,uBAAuB;IACvB,eAAe;IACf,4BAA4B;IAC5B,sBAAsB;IACtB,0BAA0B;IAC1B,uBAAuB;IACvB,mBAAmB;IACnB,sBAAsB;IACtB,iBAAiB;IACjB,iBAAiB;IACjB,kBAAkB;IAClB,iBAAiB;IACjB,wBAAwB;IACxB,aAAa;IACb,4BAA4B;IAC5B,kCAAkC;AACtC;AAGO,SAAS,qBAAqB,KAAK,EAAE,SAAS,KAAK,EAAE,WAAW,KAAK;IACxE,IAAI,MAAM,YAAY,EAAE;QACpB,MAAM,SAAS,kBAAkB,CAAC,MAAM,YAAY,CAAC;QACrD,IAAI,QAAQ;YACR,IAAI,SAAS,OAAO;YACpB,IAAI,OAAO,WAAW,UAAU;gBAC5B,IAAI,QAAQ;oBACR,SAAS,OAAO,OAAO,CAAC,4BAA4B;gBACxD;gBACA,IAAI,UAAU;oBACV,MAAM,eAAe;oBACrB,MAAM,QAAQ,OAAO,KAAK,CAAC;oBAC3B,SAAS,QAAQ,KAAK,CAAC,EAAE,GAAG;gBAChC;YACJ;YACA,OAAO;QACX;IACJ;IACA,OAAO;AACX","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 6226, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/snippets/common.js"],"sourcesContent":["export function stringifyMessages(messages, opts) {\n    let messagesStr = JSON.stringify(messages, null, \"\\t\");\n    if (opts?.indent) {\n        messagesStr = messagesStr.replaceAll(\"\\n\", `\\n${opts.indent}`);\n    }\n    if (!opts?.attributeKeyQuotes) {\n        messagesStr = messagesStr.replace(/\"([^\"]+)\":/g, \"$1:\");\n    }\n    if (opts?.customContentEscaper) {\n        messagesStr = opts.customContentEscaper(messagesStr);\n    }\n    return messagesStr;\n}\nexport function stringifyGenerationConfig(config, opts) {\n    const quote = opts.attributeKeyQuotes ? `\"` : \"\";\n    return Object.entries(config)\n        .map(([key, val]) => `${quote}${key}${quote}${opts.attributeValueConnector}${val},`)\n        .join(`${opts.indent}`);\n}\n"],"names":[],"mappings":";;;;;;AAAO,SAAS,kBAAkB,QAAQ,EAAE,IAAI;IAC5C,IAAI,cAAc,KAAK,SAAS,CAAC,UAAU,MAAM;IACjD,IAAI,MAAM,QAAQ;QACd,cAAc,YAAY,UAAU,CAAC,MAAM,CAAC,EAAE,EAAE,KAAK,MAAM,EAAE;IACjE;IACA,IAAI,CAAC,MAAM,oBAAoB;QAC3B,cAAc,YAAY,OAAO,CAAC,eAAe;IACrD;IACA,IAAI,MAAM,sBAAsB;QAC5B,cAAc,KAAK,oBAAoB,CAAC;IAC5C;IACA,OAAO;AACX;AACO,SAAS,0BAA0B,MAAM,EAAE,IAAI;IAClD,MAAM,QAAQ,KAAK,kBAAkB,GAAG,CAAC,CAAC,CAAC,GAAG;IAC9C,OAAO,OAAO,OAAO,CAAC,QACjB,GAAG,CAAC,CAAC,CAAC,KAAK,IAAI,GAAK,GAAG,QAAQ,MAAM,QAAQ,KAAK,uBAAuB,GAAG,IAAI,CAAC,CAAC,EAClF,IAAI,CAAC,GAAG,KAAK,MAAM,EAAE;AAC9B","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 6253, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/model-libraries-snippets.js"],"sourcesContent":["import { LIBRARY_TASK_MAPPING } from \"./library-to-tasks.js\";\nimport { getModelInputSnippet } from \"./snippets/inputs.js\";\nimport { stringifyMessages } from \"./snippets/common.js\";\nconst TAG_CUSTOM_CODE = \"custom_code\";\nfunction nameWithoutNamespace(modelId) {\n    const splitted = modelId.split(\"/\");\n    return splitted.length === 1 ? splitted[0] : splitted[1];\n}\nconst escapeStringForJson = (str) => JSON.stringify(str).slice(1, -1); // slice is needed to remove surrounding quotes added by JSON.stringify\n//#region snippets\nexport const adapters = (model) => [\n    `from adapters import AutoAdapterModel\n\nmodel = AutoAdapterModel.from_pretrained(\"${model.config?.adapter_transformers?.model_name}\")\nmodel.load_adapter(\"${model.id}\", set_active=True)`,\n];\nconst allennlpUnknown = (model) => [\n    `import allennlp_models\nfrom allennlp.predictors.predictor import Predictor\n\npredictor = Predictor.from_path(\"hf://${model.id}\")`,\n];\nconst allennlpQuestionAnswering = (model) => [\n    `import allennlp_models\nfrom allennlp.predictors.predictor import Predictor\n\npredictor = Predictor.from_path(\"hf://${model.id}\")\npredictor_input = {\"passage\": \"My name is Wolfgang and I live in Berlin\", \"question\": \"Where do I live?\"}\npredictions = predictor.predict_json(predictor_input)`,\n];\nexport const allennlp = (model) => {\n    if (model.tags.includes(\"question-answering\")) {\n        return allennlpQuestionAnswering(model);\n    }\n    return allennlpUnknown(model);\n};\nexport const araclip = (model) => [\n    `from araclip import AraClip\n\nmodel = AraClip.from_pretrained(\"${model.id}\")`,\n];\nexport const asteroid = (model) => [\n    `from asteroid.models import BaseModel\n\nmodel = BaseModel.from_pretrained(\"${model.id}\")`,\n];\nexport const audioseal = (model) => {\n    const watermarkSnippet = `# Watermark Generator\nfrom audioseal import AudioSeal\n\nmodel = AudioSeal.load_generator(\"${model.id}\")\n# pass a tensor (tensor_wav) of shape (batch, channels, samples) and a sample rate\nwav, sr = tensor_wav, 16000\n\t\nwatermark = model.get_watermark(wav, sr)\nwatermarked_audio = wav + watermark`;\n    const detectorSnippet = `# Watermark Detector\nfrom audioseal import AudioSeal\n\ndetector = AudioSeal.load_detector(\"${model.id}\")\n\t\nresult, message = detector.detect_watermark(watermarked_audio, sr)`;\n    return [watermarkSnippet, detectorSnippet];\n};\nfunction get_base_diffusers_model(model) {\n    return model.cardData?.base_model?.toString() ?? \"fill-in-base-model\";\n}\nfunction get_prompt_from_diffusers_model(model) {\n    const prompt = model.widgetData?.[0]?.text ?? model.cardData?.instance_prompt;\n    if (prompt) {\n        return escapeStringForJson(prompt);\n    }\n}\nexport const ben2 = (model) => [\n    `import requests\nfrom PIL import Image\nfrom ben2 import AutoModel\n\nurl = \"https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = AutoModel.from_pretrained(\"${model.id}\")\nmodel.to(\"cuda\").eval()\nforeground = model.inference(image)\n`,\n];\nexport const bertopic = (model) => [\n    `from bertopic import BERTopic\n\nmodel = BERTopic.load(\"${model.id}\")`,\n];\nexport const bm25s = (model) => [\n    `from bm25s.hf import BM25HF\n\nretriever = BM25HF.load_from_hub(\"${model.id}\")`,\n];\nexport const chatterbox = () => [\n    `# pip install chatterbox-tts\nimport torchaudio as ta\nfrom chatterbox.tts import ChatterboxTTS\n\nmodel = ChatterboxTTS.from_pretrained(device=\"cuda\")\n\ntext = \"Ezreal and Jinx teamed up with Ahri, Yasuo, and Teemo to take down the enemy's Nexus in an epic late-game pentakill.\"\nwav = model.generate(text)\nta.save(\"test-1.wav\", wav, model.sr)\n\n# If you want to synthesize with a different voice, specify the audio prompt\nAUDIO_PROMPT_PATH=\"YOUR_FILE.wav\"\nwav = model.generate(text, audio_prompt_path=AUDIO_PROMPT_PATH)\nta.save(\"test-2.wav\", wav, model.sr)`,\n];\nexport const contexttab = () => {\n    const installSnippet = `pip install git+https://github.com/SAP-samples/contexttab`;\n    const classificationSnippet = `# Run a classification task\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom contexttab import ConTextTabClassifier\n\n# Load sample data\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\n# Initialize a classifier\n# You can omit checkpoint and checkpoint_revision to use the default model\nclf = ConTextTabClassifier(checkpoint=\"l2/base.pt\", checkpoint_revision=\"v1.0.0\", bagging=1, max_context_size=2048)\n\nclf.fit(X_train, y_train)\n\n# Predict probabilities\nprediction_probabilities = clf.predict_proba(X_test)\n# Predict labels\npredictions = clf.predict(X_test)\nprint(\"Accuracy\", accuracy_score(y_test, predictions))`;\n    const regressionsSnippet = `# Run a regression task\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\nfrom contexttab import ConTextTabRegressor\n\n\n# Load sample data\ndf = fetch_openml(data_id=531, as_frame=True)\nX = df.data\ny = df.target.astype(float)\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\n# Initialize the regressor\n# You can omit checkpoint and checkpoint_revision to use the default model\nregressor = ConTextTabRegressor(checkpoint=\"l2/base.pt\", checkpoint_revision=\"v1.0.0\", bagging=1, max_context_size=2048)\n\nregressor.fit(X_train, y_train)\n\n# Predict on the test set\npredictions = regressor.predict(X_test)\n\nr2 = r2_score(y_test, predictions)\nprint(\"R² Score:\", r2)`;\n    return [installSnippet, classificationSnippet, regressionsSnippet];\n};\nexport const cxr_foundation = () => [\n    `# pip install git+https://github.com/Google-Health/cxr-foundation.git#subdirectory=python\n\n# Load image as grayscale (Stillwaterising, CC0, via Wikimedia Commons)\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimage_url = \"https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png\"\nimg = Image.open(requests.get(image_url, headers={'User-Agent': 'Demo'}, stream=True).raw).convert('L')\n\n# Run inference\nfrom clientside.clients import make_hugging_face_client\ncxr_client = make_hugging_face_client('cxr_model')\nprint(cxr_client.get_image_embeddings_from_images([img]))`,\n];\nexport const depth_anything_v2 = (model) => {\n    let encoder;\n    let features;\n    let out_channels;\n    encoder = \"<ENCODER>\";\n    features = \"<NUMBER_OF_FEATURES>\";\n    out_channels = \"<OUT_CHANNELS>\";\n    if (model.id === \"depth-anything/Depth-Anything-V2-Small\") {\n        encoder = \"vits\";\n        features = \"64\";\n        out_channels = \"[48, 96, 192, 384]\";\n    }\n    else if (model.id === \"depth-anything/Depth-Anything-V2-Base\") {\n        encoder = \"vitb\";\n        features = \"128\";\n        out_channels = \"[96, 192, 384, 768]\";\n    }\n    else if (model.id === \"depth-anything/Depth-Anything-V2-Large\") {\n        encoder = \"vitl\";\n        features = \"256\";\n        out_channels = \"[256, 512, 1024, 1024\";\n    }\n    return [\n        `\n# Install from https://github.com/DepthAnything/Depth-Anything-V2\n\n# Load the model and infer depth from an image\nimport cv2\nimport torch\n\nfrom depth_anything_v2.dpt import DepthAnythingV2\n\n# instantiate the model\nmodel = DepthAnythingV2(encoder=\"${encoder}\", features=${features}, out_channels=${out_channels})\n\n# load the weights\nfilepath = hf_hub_download(repo_id=\"${model.id}\", filename=\"depth_anything_v2_${encoder}.pth\", repo_type=\"model\")\nstate_dict = torch.load(filepath, map_location=\"cpu\")\nmodel.load_state_dict(state_dict).eval()\n\nraw_img = cv2.imread(\"your/image/path\")\ndepth = model.infer_image(raw_img) # HxW raw depth map in numpy\n    `,\n    ];\n};\nexport const depth_pro = (model) => {\n    const installSnippet = `# Download checkpoint\npip install huggingface-hub\nhuggingface-cli download --local-dir checkpoints ${model.id}`;\n    const inferenceSnippet = `import depth_pro\n\n# Load model and preprocessing transform\nmodel, transform = depth_pro.create_model_and_transforms()\nmodel.eval()\n\n# Load and preprocess an image.\nimage, _, f_px = depth_pro.load_rgb(\"example.png\")\nimage = transform(image)\n\n# Run inference.\nprediction = model.infer(image, f_px=f_px)\n\n# Results: 1. Depth in meters\ndepth = prediction[\"depth\"]\n# Results: 2. Focal length in pixels\nfocallength_px = prediction[\"focallength_px\"]`;\n    return [installSnippet, inferenceSnippet];\n};\nexport const derm_foundation = () => [\n    `from huggingface_hub import from_pretrained_keras\nimport tensorflow as tf, requests\n\n# Load and format input\nIMAGE_URL = \"https://storage.googleapis.com/dx-scin-public-data/dataset/images/3445096909671059178.png\"\ninput_tensor = tf.train.Example(\n    features=tf.train.Features(\n        feature={\n            \"image/encoded\": tf.train.Feature(\n                bytes_list=tf.train.BytesList(value=[requests.get(IMAGE_URL, stream=True).content])\n            )\n        }\n    )\n).SerializeToString()\n\n# Load model and run inference\nloaded_model = from_pretrained_keras(\"google/derm-foundation\")\ninfer = loaded_model.signatures[\"serving_default\"]\nprint(infer(inputs=tf.constant([input_tensor])))`,\n];\nexport const dia = (model) => [\n    `import soundfile as sf\nfrom dia.model import Dia\n\nmodel = Dia.from_pretrained(\"${model.id}\")\ntext = \"[S1] Dia is an open weights text to dialogue model. [S2] You get full control over scripts and voices. [S1] Wow. Amazing. (laughs) [S2] Try it now on Git hub or Hugging Face.\"\noutput = model.generate(text)\n\nsf.write(\"simple.mp3\", output, 44100)`,\n];\nexport const describe_anything = (model) => [\n    `# pip install git+https://github.com/NVlabs/describe-anything\nfrom huggingface_hub import snapshot_download\nfrom dam import DescribeAnythingModel\n\nsnapshot_download(${model.id}, local_dir=\"checkpoints\")\n\ndam = DescribeAnythingModel(\n\tmodel_path=\"checkpoints\",\n\tconv_mode=\"v1\",\n\tprompt_mode=\"focal_prompt\",\n)`,\n];\nconst diffusersDefaultPrompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\";\nconst diffusersImg2ImgDefaultPrompt = \"Turn this cat into a dog\";\nconst diffusersVideoDefaultPrompt = \"A man with short gray hair plays a red electric guitar.\";\nconst diffusers_default = (model) => [\n    `from diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"${model.id}\")\n\nprompt = \"${get_prompt_from_diffusers_model(model) ?? diffusersDefaultPrompt}\"\nimage = pipe(prompt).images[0]`,\n];\nconst diffusers_image_to_image = (model) => [\n    `from diffusers import DiffusionPipeline\nfrom diffusers.utils import load_image\n\npipe = DiffusionPipeline.from_pretrained(\"${model.id}\")\n\nprompt = \"${get_prompt_from_diffusers_model(model) ?? diffusersImg2ImgDefaultPrompt}\"\ninput_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png\")\n\nimage = pipe(image=input_image, prompt=prompt).images[0]`,\n];\nconst diffusers_image_to_video = (model) => [\n    `import torch\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import load_image, export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"${model.id}\", torch_dtype=torch.float16)\npipe.to(\"cuda\")\n\nprompt = \"${get_prompt_from_diffusers_model(model) ?? diffusersVideoDefaultPrompt}\"\nimage = load_image(\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/guitar-man.png\"\n)\n\noutput = pipe(image=image, prompt=prompt).frames[0]\nexport_to_video(output, \"output.mp4\")`,\n];\nconst diffusers_controlnet = (model) => [\n    `from diffusers import ControlNetModel, StableDiffusionControlNetPipeline\n\ncontrolnet = ControlNetModel.from_pretrained(\"${model.id}\")\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n\t\"${get_base_diffusers_model(model)}\", controlnet=controlnet\n)`,\n];\nconst diffusers_lora = (model) => [\n    `from diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"${get_base_diffusers_model(model)}\")\npipe.load_lora_weights(\"${model.id}\")\n\nprompt = \"${get_prompt_from_diffusers_model(model) ?? diffusersDefaultPrompt}\"\nimage = pipe(prompt).images[0]`,\n];\nconst diffusers_lora_image_to_image = (model) => [\n    `from diffusers import DiffusionPipeline\nfrom diffusers.utils import load_image\n\npipe = DiffusionPipeline.from_pretrained(\"${get_base_diffusers_model(model)}\")\npipe.load_lora_weights(\"${model.id}\")\n\nprompt = \"${get_prompt_from_diffusers_model(model) ?? diffusersImg2ImgDefaultPrompt}\"\ninput_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png\")\n\nimage = pipe(image=input_image, prompt=prompt).images[0]`,\n];\nconst diffusers_lora_text_to_video = (model) => [\n    `from diffusers import DiffusionPipeline\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"${get_base_diffusers_model(model)}\")\npipe.load_lora_weights(\"${model.id}\")\n\nprompt = \"${get_prompt_from_diffusers_model(model) ?? diffusersVideoDefaultPrompt}\"\n\noutput = pipe(prompt=prompt).frames[0]\nexport_to_video(output, \"output.mp4\")`,\n];\nconst diffusers_lora_image_to_video = (model) => [\n    `from diffusers import DiffusionPipeline\nfrom diffusers.utils import load_image, export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"${get_base_diffusers_model(model)}\")\npipe.load_lora_weights(\"${model.id}\")\n\nprompt = \"${get_prompt_from_diffusers_model(model) ?? diffusersVideoDefaultPrompt}\"\ninput_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/guitar-man.png\")\n\nimage = pipe(image=input_image, prompt=prompt).frames[0]\nexport_to_video(output, \"output.mp4\")`,\n];\nconst diffusers_textual_inversion = (model) => [\n    `from diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"${get_base_diffusers_model(model)}\")\npipe.load_textual_inversion(\"${model.id}\")`,\n];\nconst diffusers_flux_fill = (model) => [\n    `import torch\nfrom diffusers import FluxFillPipeline\nfrom diffusers.utils import load_image\n\nimage = load_image(\"https://huggingface.co/datasets/diffusers/diffusers-images-docs/resolve/main/cup.png\")\nmask = load_image(\"https://huggingface.co/datasets/diffusers/diffusers-images-docs/resolve/main/cup_mask.png\")\n\npipe = FluxFillPipeline.from_pretrained(\"${model.id}\", torch_dtype=torch.bfloat16).to(\"cuda\")\nimage = pipe(\n    prompt=\"a white paper cup\",\n    image=image,\n    mask_image=mask,\n    height=1632,\n    width=1232,\n    guidance_scale=30,\n    num_inference_steps=50,\n    max_sequence_length=512,\n    generator=torch.Generator(\"cpu\").manual_seed(0)\n).images[0]\nimage.save(f\"flux-fill-dev.png\")`,\n];\nconst diffusers_inpainting = (model) => [\n    `import torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image\n\npipe = AutoPipelineForInpainting.from_pretrained(\"${model.id}\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n\nimg_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\nmask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n\nimage = load_image(img_url).resize((1024, 1024))\nmask_image = load_image(mask_url).resize((1024, 1024))\n\nprompt = \"a tiger sitting on a park bench\"\ngenerator = torch.Generator(device=\"cuda\").manual_seed(0)\n\nimage = pipe(\n  prompt=prompt,\n  image=image,\n  mask_image=mask_image,\n  guidance_scale=8.0,\n  num_inference_steps=20,  # steps between 15 and 30 work well for us\n  strength=0.99,  # make sure to use \\`strength\\` below 1.0\n  generator=generator,\n).images[0]`,\n];\nexport const diffusers = (model) => {\n    if (model.tags.includes(\"StableDiffusionInpaintPipeline\") ||\n        model.tags.includes(\"StableDiffusionXLInpaintPipeline\")) {\n        return diffusers_inpainting(model);\n    }\n    else if (model.tags.includes(\"controlnet\")) {\n        return diffusers_controlnet(model);\n    }\n    else if (model.tags.includes(\"lora\")) {\n        if (model.pipeline_tag === \"image-to-image\") {\n            return diffusers_lora_image_to_image(model);\n        }\n        else if (model.pipeline_tag === \"image-to-video\") {\n            return diffusers_lora_image_to_video(model);\n        }\n        else if (model.pipeline_tag === \"text-to-video\") {\n            return diffusers_lora_text_to_video(model);\n        }\n        else {\n            return diffusers_lora(model);\n        }\n    }\n    else if (model.tags.includes(\"textual_inversion\")) {\n        return diffusers_textual_inversion(model);\n    }\n    else if (model.tags.includes(\"FluxFillPipeline\")) {\n        return diffusers_flux_fill(model);\n    }\n    else if (model.pipeline_tag === \"image-to-video\") {\n        return diffusers_image_to_video(model);\n    }\n    else if (model.pipeline_tag === \"image-to-image\") {\n        return diffusers_image_to_image(model);\n    }\n    else {\n        return diffusers_default(model);\n    }\n};\nexport const diffusionkit = (model) => {\n    const sd3Snippet = `# Pipeline for Stable Diffusion 3\nfrom diffusionkit.mlx import DiffusionPipeline\n\npipeline = DiffusionPipeline(\n\tshift=3.0,\n\tuse_t5=False,\n\tmodel_version=${model.id},\n\tlow_memory_mode=True,\n\ta16=True,\n\tw16=True,\n)`;\n    const fluxSnippet = `# Pipeline for Flux\nfrom diffusionkit.mlx import FluxPipeline\n\npipeline = FluxPipeline(\n  shift=1.0,\n  model_version=${model.id},\n  low_memory_mode=True,\n  a16=True,\n  w16=True,\n)`;\n    const generateSnippet = `# Image Generation\nHEIGHT = 512\nWIDTH = 512\nNUM_STEPS = ${model.tags.includes(\"flux\") ? 4 : 50}\nCFG_WEIGHT = ${model.tags.includes(\"flux\") ? 0 : 5}\n\nimage, _ = pipeline.generate_image(\n  \"a photo of a cat\",\n  cfg_weight=CFG_WEIGHT,\n  num_steps=NUM_STEPS,\n  latent_size=(HEIGHT // 8, WIDTH // 8),\n)`;\n    const pipelineSnippet = model.tags.includes(\"flux\") ? fluxSnippet : sd3Snippet;\n    return [pipelineSnippet, generateSnippet];\n};\nexport const cartesia_pytorch = (model) => [\n    `# pip install --no-binary :all: cartesia-pytorch\nfrom cartesia_pytorch import ReneLMHeadModel\nfrom transformers import AutoTokenizer\n\nmodel = ReneLMHeadModel.from_pretrained(\"${model.id}\")\ntokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-hf\")\n\nin_message = [\"Rene Descartes was\"]\ninputs = tokenizer(in_message, return_tensors=\"pt\")\n\noutputs = model.generate(inputs.input_ids, max_length=50, top_k=100, top_p=0.99)\nout_message = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n\nprint(out_message)\n)`,\n];\nexport const cartesia_mlx = (model) => [\n    `import mlx.core as mx\nimport cartesia_mlx as cmx\n\nmodel = cmx.from_pretrained(\"${model.id}\")\nmodel.set_dtype(mx.float32)   \n\nprompt = \"Rene Descartes was\"\n\nfor text in model.generate(\n    prompt,\n    max_tokens=500,\n    eval_every_n=5,\n    verbose=True,\n    top_p=0.99,\n    temperature=0.85,\n):\n    print(text, end=\"\", flush=True)\n`,\n];\nexport const edsnlp = (model) => {\n    const packageName = nameWithoutNamespace(model.id).replaceAll(\"-\", \"_\");\n    return [\n        `# Load it from the Hub directly\nimport edsnlp\nnlp = edsnlp.load(\"${model.id}\")\n`,\n        `# Or install it as a package\n!pip install git+https://huggingface.co/${model.id}\n\n# and import it as a module\nimport ${packageName}\n\nnlp = ${packageName}.load()  # or edsnlp.load(\"${packageName}\")\n`,\n    ];\n};\nexport const espnetTTS = (model) => [\n    `from espnet2.bin.tts_inference import Text2Speech\n\nmodel = Text2Speech.from_pretrained(\"${model.id}\")\n\nspeech, *_ = model(\"text to generate speech from\")`,\n];\nexport const espnetASR = (model) => [\n    `from espnet2.bin.asr_inference import Speech2Text\n\nmodel = Speech2Text.from_pretrained(\n  \"${model.id}\"\n)\n\nspeech, rate = soundfile.read(\"speech.wav\")\ntext, *_ = model(speech)[0]`,\n];\nconst espnetUnknown = () => [`unknown model type (must be text-to-speech or automatic-speech-recognition)`];\nexport const espnet = (model) => {\n    if (model.tags.includes(\"text-to-speech\")) {\n        return espnetTTS(model);\n    }\n    else if (model.tags.includes(\"automatic-speech-recognition\")) {\n        return espnetASR(model);\n    }\n    return espnetUnknown();\n};\nexport const fairseq = (model) => [\n    `from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\n\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\n    \"${model.id}\"\n)`,\n];\nexport const flair = (model) => [\n    `from flair.models import SequenceTagger\n\ntagger = SequenceTagger.load(\"${model.id}\")`,\n];\nexport const gliner = (model) => [\n    `from gliner import GLiNER\n\nmodel = GLiNER.from_pretrained(\"${model.id}\")`,\n];\nexport const indextts = (model) => [\n    `# Download model\nfrom huggingface_hub import snapshot_download\n\nsnapshot_download(${model.id}, local_dir=\"checkpoints\")\n\nfrom indextts.infer import IndexTTS\n\n# Ensure config.yaml is present in the checkpoints directory\ntts = IndexTTS(model_dir=\"checkpoints\", cfg_path=\"checkpoints/config.yaml\")\n\nvoice = \"path/to/your/reference_voice.wav\"  # Path to the voice reference audio file\ntext = \"Hello, how are you?\"\noutput_path = \"output_index.wav\"\n\ntts.infer(voice, text, output_path)`,\n];\nexport const htrflow = (model) => [\n    `# CLI usage\n# see docs: https://ai-riksarkivet.github.io/htrflow/latest/getting_started/quick_start.html\nhtrflow pipeline <path/to/pipeline.yaml> <path/to/image>`,\n    `# Python usage\nfrom htrflow.pipeline.pipeline import Pipeline\nfrom htrflow.pipeline.steps import Task\nfrom htrflow.models.framework.model import ModelClass\n\npipeline = Pipeline(\n    [\n        Task(\n            ModelClass, {\"model\": \"${model.id}\"}, {}\n        ),\n    ])`,\n];\nexport const keras = (model) => [\n    `# Available backend options are: \"jax\", \"torch\", \"tensorflow\".\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n\t\nimport keras\n\nmodel = keras.saving.load_model(\"hf://${model.id}\")\n`,\n];\nconst _keras_hub_causal_lm = (modelId) => `\nimport keras_hub\n\n# Load CausalLM model (optional: use half precision for inference)\ncausal_lm = keras_hub.models.CausalLM.from_preset(\"hf://${modelId}\", dtype=\"bfloat16\")\ncausal_lm.compile(sampler=\"greedy\")  # (optional) specify a sampler\n\n# Generate text\ncausal_lm.generate(\"Keras: deep learning for\", max_length=64)\n`;\nconst _keras_hub_text_to_image = (modelId) => `\nimport keras_hub\n\n# Load TextToImage model (optional: use half precision for inference)\ntext_to_image = keras_hub.models.TextToImage.from_preset(\"hf://${modelId}\", dtype=\"bfloat16\")\n\n# Generate images with a TextToImage model.\ntext_to_image.generate(\"Astronaut in a jungle\")\n`;\nconst _keras_hub_text_classifier = (modelId) => `\nimport keras_hub\n\n# Load TextClassifier model\ntext_classifier = keras_hub.models.TextClassifier.from_preset(\n    \"hf://${modelId}\",\n    num_classes=2,\n)\n# Fine-tune\ntext_classifier.fit(x=[\"Thilling adventure!\", \"Total snoozefest.\"], y=[1, 0])\n# Classify text\ntext_classifier.predict([\"Not my cup of tea.\"])\n`;\nconst _keras_hub_image_classifier = (modelId) => `\nimport keras_hub\nimport keras\n\n# Load ImageClassifier model\nimage_classifier = keras_hub.models.ImageClassifier.from_preset(\n    \"hf://${modelId}\",\n    num_classes=2,\n)\n# Fine-tune\nimage_classifier.fit(\n    x=keras.random.randint((32, 64, 64, 3), 0, 256),\n    y=keras.random.randint((32, 1), 0, 2),\n)\n# Classify image\nimage_classifier.predict(keras.random.randint((1, 64, 64, 3), 0, 256))\n`;\nconst _keras_hub_tasks_with_example = {\n    CausalLM: _keras_hub_causal_lm,\n    TextToImage: _keras_hub_text_to_image,\n    TextClassifier: _keras_hub_text_classifier,\n    ImageClassifier: _keras_hub_image_classifier,\n};\nconst _keras_hub_task_without_example = (task, modelId) => `\nimport keras_hub\n\n# Create a ${task} model\ntask = keras_hub.models.${task}.from_preset(\"hf://${modelId}\")\n`;\nconst _keras_hub_generic_backbone = (modelId) => `\nimport keras_hub\n\n# Create a Backbone model unspecialized for any task\nbackbone = keras_hub.models.Backbone.from_preset(\"hf://${modelId}\")\n`;\nexport const keras_hub = (model) => {\n    const modelId = model.id;\n    const tasks = model.config?.keras_hub?.tasks ?? [];\n    const snippets = [];\n    // First, generate tasks with examples\n    for (const [task, snippet] of Object.entries(_keras_hub_tasks_with_example)) {\n        if (tasks.includes(task)) {\n            snippets.push(snippet(modelId));\n        }\n    }\n    // Then, add remaining tasks\n    for (const task of tasks) {\n        if (!Object.keys(_keras_hub_tasks_with_example).includes(task)) {\n            snippets.push(_keras_hub_task_without_example(task, modelId));\n        }\n    }\n    // Finally, add generic backbone snippet\n    snippets.push(_keras_hub_generic_backbone(modelId));\n    return snippets;\n};\nexport const kimi_audio = (model) => [\n    `# Example usage for KimiAudio\n# pip install git+https://github.com/MoonshotAI/Kimi-Audio.git\n\nfrom kimia_infer.api.kimia import KimiAudio\n\nmodel = KimiAudio(model_path=\"${model.id}\", load_detokenizer=True)\n\nsampling_params = {\n    \"audio_temperature\": 0.8,\n    \"audio_top_k\": 10,\n    \"text_temperature\": 0.0,\n    \"text_top_k\": 5,\n}\n\n# For ASR\nasr_audio = \"asr_example.wav\"\nmessages_asr = [\n    {\"role\": \"user\", \"message_type\": \"text\", \"content\": \"Please transcribe the following audio:\"},\n    {\"role\": \"user\", \"message_type\": \"audio\", \"content\": asr_audio}\n]\n_, text = model.generate(messages_asr, **sampling_params, output_type=\"text\")\nprint(text)\n\n# For Q&A\nqa_audio = \"qa_example.wav\"\nmessages_conv = [{\"role\": \"user\", \"message_type\": \"audio\", \"content\": qa_audio}]\nwav, text = model.generate(messages_conv, **sampling_params, output_type=\"both\")\nsf.write(\"output_audio.wav\", wav.cpu().view(-1).numpy(), 24000)\nprint(text)\n`,\n];\nexport const kittentts = (model) => [\n    `from kittentts import KittenTTS\nm = KittenTTS(\"${model.id}\")\n\naudio = m.generate(\"This high quality TTS model works without a GPU\")\n\n# Save the audio\nimport soundfile as sf\nsf.write('output.wav', audio, 24000)`,\n];\nexport const lightning_ir = (model) => {\n    if (model.tags.includes(\"bi-encoder\")) {\n        return [\n            `#install from https://github.com/webis-de/lightning-ir\n\nfrom lightning_ir import BiEncoderModule\nmodel = BiEncoderModule(\"${model.id}\")\n\nmodel.score(\"query\", [\"doc1\", \"doc2\", \"doc3\"])`,\n        ];\n    }\n    else if (model.tags.includes(\"cross-encoder\")) {\n        return [\n            `#install from https://github.com/webis-de/lightning-ir\n\nfrom lightning_ir import CrossEncoderModule\nmodel = CrossEncoderModule(\"${model.id}\")\n\nmodel.score(\"query\", [\"doc1\", \"doc2\", \"doc3\"])`,\n        ];\n    }\n    return [\n        `#install from https://github.com/webis-de/lightning-ir\n\nfrom lightning_ir import BiEncoderModule, CrossEncoderModule\n\n# depending on the model type, use either BiEncoderModule or CrossEncoderModule\nmodel = BiEncoderModule(\"${model.id}\") \n# model = CrossEncoderModule(\"${model.id}\")\n\nmodel.score(\"query\", [\"doc1\", \"doc2\", \"doc3\"])`,\n    ];\n};\nexport const llama_cpp_python = (model) => {\n    const snippets = [\n        `# !pip install llama-cpp-python\n\nfrom llama_cpp import Llama\n\nllm = Llama.from_pretrained(\n\trepo_id=\"${model.id}\",\n\tfilename=\"{{GGUF_FILE}}\",\n)\n`,\n    ];\n    if (model.tags.includes(\"conversational\")) {\n        const messages = getModelInputSnippet(model);\n        snippets.push(`llm.create_chat_completion(\n\tmessages = ${stringifyMessages(messages, { attributeKeyQuotes: true, indent: \"\\t\" })}\n)`);\n    }\n    else {\n        snippets.push(`output = llm(\n\t\"Once upon a time,\",\n\tmax_tokens=512,\n\techo=True\n)\nprint(output)`);\n    }\n    return snippets;\n};\nexport const lerobot = (model) => {\n    if (model.tags.includes(\"smolvla\")) {\n        const smolvlaSnippets = [\n            // Installation snippet\n            `# See https://github.com/huggingface/lerobot?tab=readme-ov-file#installation for more details\ngit clone https://github.com/huggingface/lerobot.git\ncd lerobot\npip install -e .[smolvla]`,\n            // Finetune snippet\n            `# Launch finetuning on your dataset\npython lerobot/scripts/train.py \\\\\n--policy.path=${model.id} \\\\\n--dataset.repo_id=lerobot/svla_so101_pickplace \\\\ \n--batch_size=64 \\\\\n--steps=20000 \\\\\n--output_dir=outputs/train/my_smolvla \\\\\n--job_name=my_smolvla_training \\\\\n--policy.device=cuda \\\\\n--wandb.enable=true`,\n        ];\n        if (model.id !== \"lerobot/smolvla_base\") {\n            // Inference snippet (only if not base model)\n            smolvlaSnippets.push(`# Run the policy using the record function\t\npython -m lerobot.record \\\\\n  --robot.type=so101_follower \\\\\n  --robot.port=/dev/ttyACM0 \\\\ # <- Use your port\n  --robot.id=my_blue_follower_arm \\\\ # <- Use your robot id\n  --robot.cameras=\"{ front: {type: opencv, index_or_path: 8, width: 640, height: 480, fps: 30}}\" \\\\ # <- Use your cameras\n  --dataset.single_task=\"Grasp a lego block and put it in the bin.\" \\\\ # <- Use the same task description you used in your dataset recording\n  --dataset.repo_id=HF_USER/dataset_name \\\\  # <- This will be the dataset name on HF Hub\n  --dataset.episode_time_s=50 \\\\\n  --dataset.num_episodes=10 \\\\\n  --policy.path=${model.id}`);\n        }\n        return smolvlaSnippets;\n    }\n    return [];\n};\nexport const tf_keras = (model) => [\n    `# Note: 'keras<3.x' or 'tf_keras' must be installed (legacy)\n# See https://github.com/keras-team/tf-keras for more details.\nfrom huggingface_hub import from_pretrained_keras\n\nmodel = from_pretrained_keras(\"${model.id}\")\n`,\n];\nexport const mamba_ssm = (model) => [\n    `from mamba_ssm import MambaLMHeadModel\n\nmodel = MambaLMHeadModel.from_pretrained(\"${model.id}\")`,\n];\nexport const mars5_tts = (model) => [\n    `# Install from https://github.com/Camb-ai/MARS5-TTS\n\nfrom inference import Mars5TTS\nmars5 = Mars5TTS.from_pretrained(\"${model.id}\")`,\n];\nexport const matanyone = (model) => [\n    `# Install from https://github.com/pq-yang/MatAnyone.git\n\nfrom matanyone.model.matanyone import MatAnyone\nmodel = MatAnyone.from_pretrained(\"${model.id}\")`,\n    `\nfrom matanyone import InferenceCore\nprocessor = InferenceCore(\"${model.id}\")`,\n];\nexport const mesh_anything = () => [\n    `# Install from https://github.com/buaacyw/MeshAnything.git\n\nfrom MeshAnything.models.meshanything import MeshAnything\n\n# refer to https://github.com/buaacyw/MeshAnything/blob/main/main.py#L91 on how to define args\n# and https://github.com/buaacyw/MeshAnything/blob/main/app.py regarding usage\nmodel = MeshAnything(args)`,\n];\nexport const open_clip = (model) => [\n    `import open_clip\n\nmodel, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:${model.id}')\ntokenizer = open_clip.get_tokenizer('hf-hub:${model.id}')`,\n];\nexport const paddlenlp = (model) => {\n    if (model.config?.architectures?.[0]) {\n        const architecture = model.config.architectures[0];\n        return [\n            [\n                `from paddlenlp.transformers import AutoTokenizer, ${architecture}`,\n                \"\",\n                `tokenizer = AutoTokenizer.from_pretrained(\"${model.id}\", from_hf_hub=True)`,\n                `model = ${architecture}.from_pretrained(\"${model.id}\", from_hf_hub=True)`,\n            ].join(\"\\n\"),\n        ];\n    }\n    else {\n        return [\n            [\n                `# ⚠️ Type of model unknown`,\n                `from paddlenlp.transformers import AutoTokenizer, AutoModel`,\n                \"\",\n                `tokenizer = AutoTokenizer.from_pretrained(\"${model.id}\", from_hf_hub=True)`,\n                `model = AutoModel.from_pretrained(\"${model.id}\", from_hf_hub=True)`,\n            ].join(\"\\n\"),\n        ];\n    }\n};\nexport const paddleocr = (model) => {\n    const mapping = {\n        textline_detection: { className: \"TextDetection\" },\n        textline_recognition: { className: \"TextRecognition\" },\n        seal_text_detection: { className: \"SealTextDetection\" },\n        doc_img_unwarping: { className: \"TextImageUnwarping\" },\n        doc_img_orientation_classification: { className: \"DocImgOrientationClassification\" },\n        textline_orientation_classification: { className: \"TextLineOrientationClassification\" },\n        chart_parsing: { className: \"ChartParsing\" },\n        formula_recognition: { className: \"FormulaRecognition\" },\n        layout_detection: { className: \"LayoutDetection\" },\n        table_cells_detection: { className: \"TableCellsDetection\" },\n        wired_table_classification: { className: \"TableClassification\" },\n        table_structure_recognition: { className: \"TableStructureRecognition\" },\n    };\n    if (model.tags.includes(\"doc_vlm\")) {\n        return [\n            `# pip install paddleocr\nfrom paddleocr import DocVLM\nmodel = DocVLM(model_name=\"${nameWithoutNamespace(model.id)}\")\noutput = model.predict(\n    input={\"image\": \"path/to/image.png\", \"query\": \"Parsing this image and output the content in Markdown format.\"},\n    batch_size=1\n)\nfor res in output:\n    res.print()\n    res.save_to_json(save_path=\"./output/res.json\")`,\n        ];\n    }\n    for (const tag of model.tags) {\n        if (tag in mapping) {\n            const { className } = mapping[tag];\n            return [\n                `# pip install paddleocr\nfrom paddleocr import ${className}\nmodel = ${className}(model_name=\"${nameWithoutNamespace(model.id)}\")\noutput = model.predict(input=\"path/to/image.png\", batch_size=1)\nfor res in output:\n    res.print()\n    res.save_to_img(save_path=\"./output/\")\n    res.save_to_json(save_path=\"./output/res.json\")`,\n            ];\n        }\n    }\n    return [\n        `# Please refer to the document for information on how to use the model. \n# https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/module_usage/module_overview.html`,\n    ];\n};\nexport const perception_encoder = (model) => {\n    const clip_model = `# Use PE-Core models as CLIP models\nimport core.vision_encoder.pe as pe\n\nmodel = pe.CLIP.from_config(\"${model.id}\", pretrained=True)`;\n    const vision_encoder = `# Use any PE model as a vision encoder\nimport core.vision_encoder.pe as pe\n\nmodel = pe.VisionTransformer.from_config(\"${model.id}\", pretrained=True)`;\n    if (model.id.includes(\"Core\")) {\n        return [clip_model, vision_encoder];\n    }\n    else {\n        return [vision_encoder];\n    }\n};\nexport const phantom_wan = (model) => [\n    `from huggingface_hub import snapshot_download\nfrom phantom_wan import WANI2V, configs\n\ncheckpoint_dir = snapshot_download(\"${model.id}\")\nwan_i2v = WanI2V(\n            config=configs.WAN_CONFIGS['i2v-14B'],\n            checkpoint_dir=checkpoint_dir,\n        )\n video = wan_i2v.generate(text_prompt, image_prompt)`,\n];\nexport const pyannote_audio_pipeline = (model) => [\n    `from pyannote.audio import Pipeline\n  \npipeline = Pipeline.from_pretrained(\"${model.id}\")\n\n# inference on the whole file\npipeline(\"file.wav\")\n\n# inference on an excerpt\nfrom pyannote.core import Segment\nexcerpt = Segment(start=2.0, end=5.0)\n\nfrom pyannote.audio import Audio\nwaveform, sample_rate = Audio().crop(\"file.wav\", excerpt)\npipeline({\"waveform\": waveform, \"sample_rate\": sample_rate})`,\n];\nconst pyannote_audio_model = (model) => [\n    `from pyannote.audio import Model, Inference\n\nmodel = Model.from_pretrained(\"${model.id}\")\ninference = Inference(model)\n\n# inference on the whole file\ninference(\"file.wav\")\n\n# inference on an excerpt\nfrom pyannote.core import Segment\nexcerpt = Segment(start=2.0, end=5.0)\ninference.crop(\"file.wav\", excerpt)`,\n];\nexport const pyannote_audio = (model) => {\n    if (model.tags.includes(\"pyannote-audio-pipeline\")) {\n        return pyannote_audio_pipeline(model);\n    }\n    return pyannote_audio_model(model);\n};\nexport const relik = (model) => [\n    `from relik import Relik\n \nrelik = Relik.from_pretrained(\"${model.id}\")`,\n];\nexport const renderformer = (model) => [\n    `# Install from https://github.com/microsoft/renderformer\n\nfrom renderformer import RenderFormerRenderingPipeline\npipeline = RenderFormerRenderingPipeline.from_pretrained(\"${model.id}\")`,\n];\nconst tensorflowttsTextToMel = (model) => [\n    `from tensorflow_tts.inference import AutoProcessor, TFAutoModel\n\nprocessor = AutoProcessor.from_pretrained(\"${model.id}\")\nmodel = TFAutoModel.from_pretrained(\"${model.id}\")\n`,\n];\nconst tensorflowttsMelToWav = (model) => [\n    `from tensorflow_tts.inference import TFAutoModel\n\nmodel = TFAutoModel.from_pretrained(\"${model.id}\")\naudios = model.inference(mels)\n`,\n];\nconst tensorflowttsUnknown = (model) => [\n    `from tensorflow_tts.inference import TFAutoModel\n\nmodel = TFAutoModel.from_pretrained(\"${model.id}\")\n`,\n];\nexport const tensorflowtts = (model) => {\n    if (model.tags.includes(\"text-to-mel\")) {\n        return tensorflowttsTextToMel(model);\n    }\n    else if (model.tags.includes(\"mel-to-wav\")) {\n        return tensorflowttsMelToWav(model);\n    }\n    return tensorflowttsUnknown(model);\n};\nexport const timm = (model) => [\n    `import timm\n\nmodel = timm.create_model(\"hf_hub:${model.id}\", pretrained=True)`,\n];\nexport const saelens = ( /* model: ModelData */) => [\n    `# pip install sae-lens\nfrom sae_lens import SAE\n\nsae, cfg_dict, sparsity = SAE.from_pretrained(\n    release = \"RELEASE_ID\", # e.g., \"gpt2-small-res-jb\". See other options in https://github.com/jbloomAus/SAELens/blob/main/sae_lens/pretrained_saes.yaml\n    sae_id = \"SAE_ID\", # e.g., \"blocks.8.hook_resid_pre\". Won't always be a hook point\n)`,\n];\nexport const seed_story = () => [\n    `# seed_story_cfg_path refers to 'https://github.com/TencentARC/SEED-Story/blob/master/configs/clm_models/agent_7b_sft.yaml'\n# llm_cfg_path refers to 'https://github.com/TencentARC/SEED-Story/blob/master/configs/clm_models/llama2chat7b_lora.yaml'\nfrom omegaconf import OmegaConf\nimport hydra\n\n# load Llama2\nllm_cfg = OmegaConf.load(llm_cfg_path)\nllm = hydra.utils.instantiate(llm_cfg, torch_dtype=\"fp16\")\n\n# initialize seed_story\nseed_story_cfg = OmegaConf.load(seed_story_cfg_path)\nseed_story = hydra.utils.instantiate(seed_story_cfg, llm=llm) `,\n];\nconst skopsPickle = (model, modelFile) => {\n    return [\n        `import joblib\nfrom skops.hub_utils import download\ndownload(\"${model.id}\", \"path_to_folder\")\nmodel = joblib.load(\n\t\"${modelFile}\"\n)\n# only load pickle files from sources you trust\n# read more about it here https://skops.readthedocs.io/en/stable/persistence.html`,\n    ];\n};\nconst skopsFormat = (model, modelFile) => {\n    return [\n        `from skops.hub_utils import download\nfrom skops.io import load\ndownload(\"${model.id}\", \"path_to_folder\")\n# make sure model file is in skops format\n# if model is a pickle file, make sure it's from a source you trust\nmodel = load(\"path_to_folder/${modelFile}\")`,\n    ];\n};\nconst skopsJobLib = (model) => {\n    return [\n        `from huggingface_hub import hf_hub_download\nimport joblib\nmodel = joblib.load(\n\thf_hub_download(\"${model.id}\", \"sklearn_model.joblib\")\n)\n# only load pickle files from sources you trust\n# read more about it here https://skops.readthedocs.io/en/stable/persistence.html`,\n    ];\n};\nexport const sklearn = (model) => {\n    if (model.tags.includes(\"skops\")) {\n        const skopsmodelFile = model.config?.sklearn?.model?.file;\n        const skopssaveFormat = model.config?.sklearn?.model_format;\n        if (!skopsmodelFile) {\n            return [`# ⚠️ Model filename not specified in config.json`];\n        }\n        if (skopssaveFormat === \"pickle\") {\n            return skopsPickle(model, skopsmodelFile);\n        }\n        else {\n            return skopsFormat(model, skopsmodelFile);\n        }\n    }\n    else {\n        return skopsJobLib(model);\n    }\n};\nexport const stable_audio_tools = (model) => [\n    `import torch\nimport torchaudio\nfrom einops import rearrange\nfrom stable_audio_tools import get_pretrained_model\nfrom stable_audio_tools.inference.generation import generate_diffusion_cond\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Download model\nmodel, model_config = get_pretrained_model(\"${model.id}\")\nsample_rate = model_config[\"sample_rate\"]\nsample_size = model_config[\"sample_size\"]\n\nmodel = model.to(device)\n\n# Set up text and timing conditioning\nconditioning = [{\n\t\"prompt\": \"128 BPM tech house drum loop\",\n}]\n\n# Generate stereo audio\noutput = generate_diffusion_cond(\n\tmodel,\n\tconditioning=conditioning,\n\tsample_size=sample_size,\n\tdevice=device\n)\n\n# Rearrange audio batch to a single sequence\noutput = rearrange(output, \"b d n -> d (b n)\")\n\n# Peak normalize, clip, convert to int16, and save to file\noutput = output.to(torch.float32).div(torch.max(torch.abs(output))).clamp(-1, 1).mul(32767).to(torch.int16).cpu()\ntorchaudio.save(\"output.wav\", output, sample_rate)`,\n];\nexport const fastai = (model) => [\n    `from huggingface_hub import from_pretrained_fastai\n\nlearn = from_pretrained_fastai(\"${model.id}\")`,\n];\nexport const sam2 = (model) => {\n    const image_predictor = `# Use SAM2 with images\nimport torch\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor\n\npredictor = SAM2ImagePredictor.from_pretrained(${model.id})\n\nwith torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n    predictor.set_image(<your_image>)\n    masks, _, _ = predictor.predict(<input_prompts>)`;\n    const video_predictor = `# Use SAM2 with videos\nimport torch\nfrom sam2.sam2_video_predictor import SAM2VideoPredictor\n\t\npredictor = SAM2VideoPredictor.from_pretrained(${model.id})\n\nwith torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n    state = predictor.init_state(<your_video>)\n\n    # add new prompts and instantly get the output on the same frame\n    frame_idx, object_ids, masks = predictor.add_new_points(state, <your_prompts>):\n\n    # propagate the prompts to get masklets throughout the video\n    for frame_idx, object_ids, masks in predictor.propagate_in_video(state):\n        ...`;\n    return [image_predictor, video_predictor];\n};\nexport const sampleFactory = (model) => [\n    `python -m sample_factory.huggingface.load_from_hub -r ${model.id} -d ./train_dir`,\n];\nfunction get_widget_examples_from_st_model(model) {\n    const widgetExample = model.widgetData?.[0];\n    if (widgetExample?.source_sentence && widgetExample?.sentences?.length) {\n        return [widgetExample.source_sentence, ...widgetExample.sentences];\n    }\n}\nexport const sentenceTransformers = (model) => {\n    const remote_code_snippet = model.tags.includes(TAG_CUSTOM_CODE) ? \", trust_remote_code=True\" : \"\";\n    if (model.tags.includes(\"cross-encoder\") || model.pipeline_tag == \"text-ranking\") {\n        return [\n            `from sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder(\"${model.id}\"${remote_code_snippet})\n\nquery = \"Which planet is known as the Red Planet?\"\npassages = [\n\t\"Venus is often called Earth's twin because of its similar size and proximity.\",\n\t\"Mars, known for its reddish appearance, is often referred to as the Red Planet.\",\n\t\"Jupiter, the largest planet in our solar system, has a prominent red spot.\",\n\t\"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\"\n]\n\nscores = model.predict([(query, passage) for passage in passages])\nprint(scores)`,\n        ];\n    }\n    const exampleSentences = get_widget_examples_from_st_model(model) ?? [\n        \"The weather is lovely today.\",\n        \"It's so sunny outside!\",\n        \"He drove to the stadium.\",\n    ];\n    return [\n        `from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"${model.id}\"${remote_code_snippet})\n\nsentences = ${JSON.stringify(exampleSentences, null, 4)}\nembeddings = model.encode(sentences)\n\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities.shape)\n# [${exampleSentences.length}, ${exampleSentences.length}]`,\n    ];\n};\nexport const setfit = (model) => [\n    `from setfit import SetFitModel\n\nmodel = SetFitModel.from_pretrained(\"${model.id}\")`,\n];\nexport const spacy = (model) => [\n    `!pip install https://huggingface.co/${model.id}/resolve/main/${nameWithoutNamespace(model.id)}-any-py3-none-any.whl\n\n# Using spacy.load().\nimport spacy\nnlp = spacy.load(\"${nameWithoutNamespace(model.id)}\")\n\n# Importing as module.\nimport ${nameWithoutNamespace(model.id)}\nnlp = ${nameWithoutNamespace(model.id)}.load()`,\n];\nexport const span_marker = (model) => [\n    `from span_marker import SpanMarkerModel\n\nmodel = SpanMarkerModel.from_pretrained(\"${model.id}\")`,\n];\nexport const stanza = (model) => [\n    `import stanza\n\nstanza.download(\"${nameWithoutNamespace(model.id).replace(\"stanza-\", \"\")}\")\nnlp = stanza.Pipeline(\"${nameWithoutNamespace(model.id).replace(\"stanza-\", \"\")}\")`,\n];\nconst speechBrainMethod = (speechbrainInterface) => {\n    switch (speechbrainInterface) {\n        case \"EncoderClassifier\":\n            return \"classify_file\";\n        case \"EncoderDecoderASR\":\n        case \"EncoderASR\":\n            return \"transcribe_file\";\n        case \"SpectralMaskEnhancement\":\n            return \"enhance_file\";\n        case \"SepformerSeparation\":\n            return \"separate_file\";\n        default:\n            return undefined;\n    }\n};\nexport const speechbrain = (model) => {\n    const speechbrainInterface = model.config?.speechbrain?.speechbrain_interface;\n    if (speechbrainInterface === undefined) {\n        return [`# interface not specified in config.json`];\n    }\n    const speechbrainMethod = speechBrainMethod(speechbrainInterface);\n    if (speechbrainMethod === undefined) {\n        return [`# interface in config.json invalid`];\n    }\n    return [\n        `from speechbrain.pretrained import ${speechbrainInterface}\nmodel = ${speechbrainInterface}.from_hparams(\n  \"${model.id}\"\n)\nmodel.${speechbrainMethod}(\"file.wav\")`,\n    ];\n};\nexport const terratorch = (model) => [\n    `from terratorch.registry import BACKBONE_REGISTRY\n\nmodel = BACKBONE_REGISTRY.build(\"${model.id}\")`,\n];\nconst hasChatTemplate = (model) => model.config?.tokenizer_config?.chat_template !== undefined ||\n    model.config?.processor_config?.chat_template !== undefined ||\n    model.config?.chat_template_jinja !== undefined;\nexport const transformers = (model) => {\n    const info = model.transformersInfo;\n    if (!info) {\n        return [`# ⚠️ Type of model unknown`];\n    }\n    const remote_code_snippet = model.tags.includes(TAG_CUSTOM_CODE) ? \", trust_remote_code=True\" : \"\";\n    const autoSnippet = [];\n    if (info.processor) {\n        const processorVarName = info.processor === \"AutoTokenizer\"\n            ? \"tokenizer\"\n            : info.processor === \"AutoFeatureExtractor\"\n                ? \"extractor\"\n                : \"processor\";\n        autoSnippet.push(\"# Load model directly\", `from transformers import ${info.processor}, ${info.auto_model}`, \"\", `${processorVarName} = ${info.processor}.from_pretrained(\"${model.id}\"` + remote_code_snippet + \")\", `model = ${info.auto_model}.from_pretrained(\"${model.id}\"` + remote_code_snippet + \")\");\n        if (model.tags.includes(\"conversational\") && hasChatTemplate(model)) {\n            if (model.tags.includes(\"image-text-to-text\")) {\n                autoSnippet.push(\"messages = [\", [\n                    \"    {\",\n                    '        \"role\": \"user\",',\n                    '        \"content\": [',\n                    '            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},',\n                    '            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}',\n                    \"        ]\",\n                    \"    },\",\n                ].join(\"\\n\"), \"]\");\n            }\n            else {\n                autoSnippet.push(\"messages = [\", '    {\"role\": \"user\", \"content\": \"Who are you?\"},', \"]\");\n            }\n            autoSnippet.push(`inputs = ${processorVarName}.apply_chat_template(`, \"\tmessages,\", \"\tadd_generation_prompt=True,\", \"\ttokenize=True,\", \"\treturn_dict=True,\", '\treturn_tensors=\"pt\",', \").to(model.device)\", \"\", \"outputs = model.generate(**inputs, max_new_tokens=40)\", `print(${processorVarName}.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))`);\n        }\n    }\n    else {\n        autoSnippet.push(\"# Load model directly\", `from transformers import ${info.auto_model}`, `model = ${info.auto_model}.from_pretrained(\"${model.id}\"` + remote_code_snippet + ', torch_dtype=\"auto\")');\n    }\n    if (model.pipeline_tag && LIBRARY_TASK_MAPPING.transformers?.includes(model.pipeline_tag)) {\n        const pipelineSnippet = [\n            \"# Use a pipeline as a high-level helper\",\n            \"from transformers import pipeline\",\n            \"\",\n            `pipe = pipeline(\"${model.pipeline_tag}\", model=\"${model.id}\"` + remote_code_snippet + \")\",\n        ];\n        if (model.tags.includes(\"conversational\")) {\n            if (model.tags.includes(\"image-text-to-text\")) {\n                pipelineSnippet.push(\"messages = [\", [\n                    \"    {\",\n                    '        \"role\": \"user\",',\n                    '        \"content\": [',\n                    '            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},',\n                    '            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}',\n                    \"        ]\",\n                    \"    },\",\n                ].join(\"\\n\"), \"]\");\n                pipelineSnippet.push(\"pipe(text=messages)\");\n            }\n            else {\n                pipelineSnippet.push(\"messages = [\", '    {\"role\": \"user\", \"content\": \"Who are you?\"},', \"]\");\n                pipelineSnippet.push(\"pipe(messages)\");\n            }\n        }\n        else if (model.pipeline_tag === \"zero-shot-image-classification\") {\n            pipelineSnippet.push(\"pipe(\", '    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/parrots.png\",', '    candidate_labels=[\"animals\", \"humans\", \"landscape\"],', \")\");\n        }\n        else if (model.pipeline_tag === \"image-classification\") {\n            pipelineSnippet.push('pipe(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/parrots.png\")');\n        }\n        return [pipelineSnippet.join(\"\\n\"), autoSnippet.join(\"\\n\")];\n    }\n    return [autoSnippet.join(\"\\n\")];\n};\nexport const transformersJS = (model) => {\n    if (!model.pipeline_tag) {\n        return [`// ⚠️ Unknown pipeline tag`];\n    }\n    const libName = \"@huggingface/transformers\";\n    return [\n        `// npm i ${libName}\nimport { pipeline } from '${libName}';\n\n// Allocate pipeline\nconst pipe = await pipeline('${model.pipeline_tag}', '${model.id}');`,\n    ];\n};\nconst peftTask = (peftTaskType) => {\n    switch (peftTaskType) {\n        case \"CAUSAL_LM\":\n            return \"CausalLM\";\n        case \"SEQ_2_SEQ_LM\":\n            return \"Seq2SeqLM\";\n        case \"TOKEN_CLS\":\n            return \"TokenClassification\";\n        case \"SEQ_CLS\":\n            return \"SequenceClassification\";\n        default:\n            return undefined;\n    }\n};\nexport const peft = (model) => {\n    const { base_model_name_or_path: peftBaseModel, task_type: peftTaskType } = model.config?.peft ?? {};\n    const pefttask = peftTask(peftTaskType);\n    if (!pefttask) {\n        return [`Task type is invalid.`];\n    }\n    if (!peftBaseModel) {\n        return [`Base model is not found.`];\n    }\n    return [\n        `from peft import PeftModel\nfrom transformers import AutoModelFor${pefttask}\n\nbase_model = AutoModelFor${pefttask}.from_pretrained(\"${peftBaseModel}\")\nmodel = PeftModel.from_pretrained(base_model, \"${model.id}\")`,\n    ];\n};\nexport const fasttext = (model) => [\n    `from huggingface_hub import hf_hub_download\nimport fasttext\n\nmodel = fasttext.load_model(hf_hub_download(\"${model.id}\", \"model.bin\"))`,\n];\nexport const stableBaselines3 = (model) => [\n    `from huggingface_sb3 import load_from_hub\ncheckpoint = load_from_hub(\n\trepo_id=\"${model.id}\",\n\tfilename=\"{MODEL FILENAME}.zip\",\n)`,\n];\nconst nemoDomainResolver = (domain, model) => {\n    switch (domain) {\n        case \"ASR\":\n            return [\n                `import nemo.collections.asr as nemo_asr\nasr_model = nemo_asr.models.ASRModel.from_pretrained(\"${model.id}\")\n\ntranscriptions = asr_model.transcribe([\"file.wav\"])`,\n            ];\n        default:\n            return undefined;\n    }\n};\nexport const mlAgents = (model) => [\n    `mlagents-load-from-hf --repo-id=\"${model.id}\" --local-dir=\"./download: string[]s\"`,\n];\nexport const sentis = ( /* model: ModelData */) => [\n    `string modelName = \"[Your model name here].sentis\";\nModel model = ModelLoader.Load(Application.streamingAssetsPath + \"/\" + modelName);\nIWorker engine = WorkerFactory.CreateWorker(BackendType.GPUCompute, model);\n// Please see provided C# file for more details\n`,\n];\nexport const sana = (model) => [\n    `\n# Load the model and infer image from text\nimport torch\nfrom app.sana_pipeline import SanaPipeline\nfrom torchvision.utils import save_image\n\nsana = SanaPipeline(\"configs/sana_config/1024ms/Sana_1600M_img1024.yaml\")\nsana.from_pretrained(\"hf://${model.id}\")\n\nimage = sana(\n    prompt='a cyberpunk cat with a neon sign that says \"Sana\"',\n    height=1024,\n    width=1024,\n    guidance_scale=5.0,\n    pag_guidance_scale=2.0,\n    num_inference_steps=18,\n) `,\n];\nexport const videoprism = (model) => [\n    `# Install from https://github.com/google-deepmind/videoprism\nimport jax\nfrom videoprism import models as vp\n\nflax_model = vp.get_model(\"${model.id}\")\nloaded_state = vp.load_pretrained_weights(\"${model.id}\")\n\n@jax.jit\ndef forward_fn(inputs, train=False):\n  return flax_model.apply(loaded_state, inputs, train=train)`,\n];\nexport const vfimamba = (model) => [\n    `from Trainer_finetune import Model\n\nmodel = Model.from_pretrained(\"${model.id}\")`,\n];\nexport const lvface = (model) => [\n    `from huggingface_hub import hf_hub_download\n\t from inference_onnx import LVFaceONNXInferencer\n\nmodel_path = hf_hub_download(\"${model.id}\", \"LVFace-L_Glint360K/LVFace-L_Glint360K.onnx\")\ninferencer = LVFaceONNXInferencer(model_path, use_gpu=True, timeout=300)\nimg_path = 'path/to/image1.jpg'\nembedding = inferencer.infer_from_image(img_path)`,\n];\nexport const voicecraft = (model) => [\n    `from voicecraft import VoiceCraft\n\nmodel = VoiceCraft.from_pretrained(\"${model.id}\")`,\n];\nexport const voxcpm = (model) => [\n    `import soundfile as sf\nfrom voxcpm import VoxCPM\n\nmodel = VoxCPM.from_pretrained(\"${model.id}\")\n\nwav = model.generate(\n    text=\"VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.\",\n    prompt_wav_path=None,      # optional: path to a prompt speech for voice cloning\n    prompt_text=None,          # optional: reference text\n    cfg_value=2.0,             # LM guidance on LocDiT, higher for better adherence to the prompt, but maybe worse\n    inference_timesteps=10,   # LocDiT inference timesteps, higher for better result, lower for fast speed\n    normalize=True,           # enable external TN tool\n    denoise=True,             # enable external Denoise tool\n    retry_badcase=True,        # enable retrying mode for some bad cases (unstoppable)\n    retry_badcase_max_times=3,  # maximum retrying times\n    retry_badcase_ratio_threshold=6.0, # maximum length restriction for bad case detection (simple but effective), it could be adjusted for slow pace speech\n)\n\nsf.write(\"output.wav\", wav, 16000)\nprint(\"saved: output.wav\")`,\n];\nexport const vui = () => [\n    `# !pip install git+https://github.com/fluxions-ai/vui\n\nimport torchaudio\n\nfrom vui.inference import render\nfrom vui.model import Vui,\n\nmodel = Vui.from_pretrained().cuda()\nwaveform = render(\n    model,\n    \"Hey, here is some random stuff, usually something quite long as the shorter the text the less likely the model can cope!\",\n)\nprint(waveform.shape)\ntorchaudio.save(\"out.opus\", waveform[0], 22050)\n`,\n];\nexport const chattts = () => [\n    `import ChatTTS\nimport torchaudio\n\nchat = ChatTTS.Chat()\nchat.load_models(compile=False) # Set to True for better performance\n\ntexts = [\"PUT YOUR TEXT HERE\",]\n\nwavs = chat.infer(texts, )\n\ntorchaudio.save(\"output1.wav\", torch.from_numpy(wavs[0]), 24000)`,\n];\nexport const ultralytics = (model) => {\n    // ultralytics models must have a version tag (e.g. `yolov8`)\n    const versionTag = model.tags.find((tag) => tag.match(/^yolov\\d+$/));\n    const className = versionTag ? `YOLOv${versionTag.slice(4)}` : \"YOLOvXX\";\n    const prefix = versionTag\n        ? \"\"\n        : `# Couldn't find a valid YOLO version tag.\\n# Replace XX with the correct version.\\n`;\n    return [\n        prefix +\n            `from ultralytics import ${className}\n\nmodel = ${className}.from_pretrained(\"${model.id}\")\nsource = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nmodel.predict(source=source, save=True)`,\n    ];\n};\nexport const birefnet = (model) => [\n    `# Option 1: use with transformers\n\nfrom transformers import AutoModelForImageSegmentation\nbirefnet = AutoModelForImageSegmentation.from_pretrained(\"${model.id}\", trust_remote_code=True)\n`,\n    `# Option 2: use with BiRefNet\n\n# Install from https://github.com/ZhengPeng7/BiRefNet\n\nfrom models.birefnet import BiRefNet\nmodel = BiRefNet.from_pretrained(\"${model.id}\")`,\n];\nexport const swarmformer = (model) => [\n    `from swarmformer import SwarmFormerModel\n\nmodel = SwarmFormerModel.from_pretrained(\"${model.id}\")\n`,\n];\nexport const univa = (model) => [\n    `# Follow installation instructions at https://github.com/PKU-YuanGroup/UniWorld-V1\n\nfrom univa.models.qwen2p5vl.modeling_univa_qwen2p5vl import UnivaQwen2p5VLForConditionalGeneration\n\tmodel = UnivaQwen2p5VLForConditionalGeneration.from_pretrained(\n        \"${model.id}\",\n        torch_dtype=torch.bfloat16,\n        attn_implementation=\"flash_attention_2\",\n    ).to(\"cuda\")\n\tprocessor = AutoProcessor.from_pretrained(\"${model.id}\")\n`,\n];\nconst mlx_unknown = (model) => [\n    `# Download the model from the Hub\npip install huggingface_hub[hf_xet]\n\nhuggingface-cli download --local-dir ${nameWithoutNamespace(model.id)} ${model.id}`,\n];\nconst mlxlm = (model) => [\n    `# Make sure mlx-lm is installed\n# pip install --upgrade mlx-lm\n# if on a CUDA device, also pip install mlx[cuda]\n\n# Generate text with mlx-lm\nfrom mlx_lm import load, generate\n\nmodel, tokenizer = load(\"${model.id}\")\n\nprompt = \"Once upon a time in\"\ntext = generate(model, tokenizer, prompt=prompt, verbose=True)`,\n];\nconst mlxchat = (model) => [\n    `# Make sure mlx-lm is installed\n# pip install --upgrade mlx-lm\n\n# Generate text with mlx-lm\nfrom mlx_lm import load, generate\n\nmodel, tokenizer = load(\"${model.id}\")\n\nprompt = \"Write a story about Einstein\"\nmessages = [{\"role\": \"user\", \"content\": prompt}]\nprompt = tokenizer.apply_chat_template(\n    messages, add_generation_prompt=True\n)\n\ntext = generate(model, tokenizer, prompt=prompt, verbose=True)`,\n];\nconst mlxvlm = (model) => [\n    `# Make sure mlx-vlm is installed\n# pip install --upgrade mlx-vlm\n\nfrom mlx_vlm import load, generate\nfrom mlx_vlm.prompt_utils import apply_chat_template\nfrom mlx_vlm.utils import load_config\n\n# Load the model\nmodel, processor = load(\"${model.id}\")\nconfig = load_config(\"${model.id}\")\n\n# Prepare input\nimage = [\"http://images.cocodataset.org/val2017/000000039769.jpg\"]\nprompt = \"Describe this image.\"\n\n# Apply chat template\nformatted_prompt = apply_chat_template(\n    processor, config, prompt, num_images=1\n)\n\n# Generate output\noutput = generate(model, processor, formatted_prompt, image)\nprint(output)`,\n];\nexport const mlxim = (model) => [\n    `from mlxim.model import create_model\n\nmodel = create_model(${model.id})`,\n];\nexport const mlx = (model) => {\n    if (model.pipeline_tag === \"image-text-to-text\") {\n        return mlxvlm(model);\n    }\n    if (model.pipeline_tag === \"text-generation\") {\n        if (model.tags.includes(\"conversational\")) {\n            return mlxchat(model);\n        }\n        else {\n            return mlxlm(model);\n        }\n    }\n    return mlx_unknown(model);\n};\nexport const model2vec = (model) => [\n    `from model2vec import StaticModel\n\nmodel = StaticModel.from_pretrained(\"${model.id}\")`,\n];\nexport const pruna = (model) => {\n    let snippets;\n    if (model.tags.includes(\"diffusers\")) {\n        snippets = pruna_diffusers(model);\n    }\n    else if (model.tags.includes(\"transformers\")) {\n        snippets = pruna_transformers(model);\n    }\n    else {\n        snippets = pruna_default(model);\n    }\n    const ensurePrunaModelImport = (snippet) => {\n        if (!/^from pruna import PrunaModel/m.test(snippet)) {\n            return `from pruna import PrunaModel\\n${snippet}`;\n        }\n        return snippet;\n    };\n    snippets = snippets.map(ensurePrunaModelImport);\n    if (model.tags.includes(\"pruna_pro-ai\")) {\n        return snippets.map((snippet) => snippet.replace(/\\bpruna\\b/g, \"pruna_pro\").replace(/\\bPrunaModel\\b/g, \"PrunaProModel\"));\n    }\n    return snippets;\n};\nconst pruna_diffusers = (model) => {\n    const diffusersSnippets = diffusers(model);\n    return diffusersSnippets.map((snippet) => snippet\n        // Replace pipeline classes with PrunaModel\n        .replace(/\\b\\w*Pipeline\\w*\\b/g, \"PrunaModel\")\n        // Clean up diffusers imports containing PrunaModel\n        .replace(/from diffusers import ([^,\\n]*PrunaModel[^,\\n]*)/g, \"\")\n        .replace(/from diffusers import ([^,\\n]+),?\\s*([^,\\n]*PrunaModel[^,\\n]*)/g, \"from diffusers import $1\")\n        .replace(/from diffusers import\\s*(\\n|$)/g, \"\")\n        // Fix PrunaModel imports\n        .replace(/from diffusers import PrunaModel/g, \"from pruna import PrunaModel\")\n        .replace(/from diffusers import ([^,\\n]+), PrunaModel/g, \"from diffusers import $1\")\n        .replace(/from diffusers import PrunaModel, ([^,\\n]+)/g, \"from diffusers import $1\")\n        // Clean up whitespace\n        .replace(/\\n\\n+/g, \"\\n\")\n        .trim());\n};\nconst pruna_transformers = (model) => {\n    const info = model.transformersInfo;\n    const transformersSnippets = transformers(model);\n    // Replace pipeline with PrunaModel\n    let processedSnippets = transformersSnippets.map((snippet) => snippet\n        .replace(/from transformers import pipeline/g, \"from pruna import PrunaModel\")\n        .replace(/pipeline\\([^)]*\\)/g, `PrunaModel.from_pretrained(\"${model.id}\")`));\n    // Additional cleanup if auto_model info is available\n    if (info?.auto_model) {\n        processedSnippets = processedSnippets.map((snippet) => snippet\n            .replace(new RegExp(`from transformers import ${info.auto_model}\\n?`, \"g\"), \"\")\n            .replace(new RegExp(`${info.auto_model}.from_pretrained`, \"g\"), \"PrunaModel.from_pretrained\")\n            .replace(new RegExp(`^.*from.*import.*(, *${info.auto_model})+.*$`, \"gm\"), (line) => line.replace(new RegExp(`, *${info.auto_model}`, \"g\"), \"\")));\n    }\n    return processedSnippets;\n};\nconst pruna_default = (model) => [\n    `from pruna import PrunaModel\nmodel = PrunaModel.from_pretrained(\"${model.id}\")\n`,\n];\nexport const nemo = (model) => {\n    let command = undefined;\n    // Resolve the tag to a nemo domain/sub-domain\n    if (model.tags.includes(\"automatic-speech-recognition\")) {\n        command = nemoDomainResolver(\"ASR\", model);\n    }\n    return command ?? [`# tag did not correspond to a valid NeMo domain.`];\n};\nexport const outetts = (model) => {\n    // Don’t show this block on GGUF / ONNX mirrors\n    const t = model.tags ?? [];\n    if (t.includes(\"gguf\") || t.includes(\"onnx\"))\n        return [];\n    // v1.0 HF → minimal runnable snippet\n    return [\n        `\n  import outetts\n  \n  enum = outetts.Models(\"${model.id}\".split(\"/\", 1)[1])       # VERSION_1_0_SIZE_1B\n  cfg  = outetts.ModelConfig.auto_config(enum, outetts.Backend.HF)\n  tts  = outetts.Interface(cfg)\n  \n  speaker = tts.load_default_speaker(\"EN-FEMALE-1-NEUTRAL\")\n  tts.generate(\n\t  outetts.GenerationConfig(\n\t\t  text=\"Hello there, how are you doing?\",\n\t\t  speaker=speaker,\n\t  )\n  ).save(\"output.wav\")\n  `,\n    ];\n};\nexport const pxia = (model) => [\n    `from pxia import AutoModel\n\nmodel = AutoModel.from_pretrained(\"${model.id}\")`,\n];\nexport const pythae = (model) => [\n    `from pythae.models import AutoModel\n\nmodel = AutoModel.load_from_hf_hub(\"${model.id}\")`,\n];\nconst musicgen = (model) => [\n    `from audiocraft.models import MusicGen\n\nmodel = MusicGen.get_pretrained(\"${model.id}\")\n\ndescriptions = ['happy rock', 'energetic EDM', 'sad jazz']\nwav = model.generate(descriptions)  # generates 3 samples.`,\n];\nconst magnet = (model) => [\n    `from audiocraft.models import MAGNeT\n\t\nmodel = MAGNeT.get_pretrained(\"${model.id}\")\n\ndescriptions = ['disco beat', 'energetic EDM', 'funky groove']\nwav = model.generate(descriptions)  # generates 3 samples.`,\n];\nconst audiogen = (model) => [\n    `from audiocraft.models import AudioGen\n\t\nmodel = AudioGen.get_pretrained(\"${model.id}\")\nmodel.set_generation_params(duration=5)  # generate 5 seconds.\ndescriptions = ['dog barking', 'sirene of an emergency vehicle', 'footsteps in a corridor']\nwav = model.generate(descriptions)  # generates 3 samples.`,\n];\nexport const anemoi = (model) => [\n    `from anemoi.inference.runners.default import DefaultRunner\nfrom anemoi.inference.config.run import RunConfiguration\n# Create Configuration\nconfig = RunConfiguration(checkpoint = {\"huggingface\":\"${model.id}\"})\n# Load Runner\nrunner = DefaultRunner(config)`,\n];\nexport const audiocraft = (model) => {\n    if (model.tags.includes(\"musicgen\")) {\n        return musicgen(model);\n    }\n    else if (model.tags.includes(\"audiogen\")) {\n        return audiogen(model);\n    }\n    else if (model.tags.includes(\"magnet\")) {\n        return magnet(model);\n    }\n    else {\n        return [`# Type of model unknown.`];\n    }\n};\nexport const whisperkit = () => [\n    `# Install CLI with Homebrew on macOS device\nbrew install whisperkit-cli\n\n# View all available inference options\nwhisperkit-cli transcribe --help\n\t\n# Download and run inference using whisper base model\nwhisperkit-cli transcribe --audio-path /path/to/audio.mp3\n\n# Or use your preferred model variant\nwhisperkit-cli transcribe --model \"large-v3\" --model-prefix \"distil\" --audio-path /path/to/audio.mp3 --verbose`,\n];\nexport const threedtopia_xl = (model) => [\n    `from threedtopia_xl.models import threedtopia_xl\n\nmodel = threedtopia_xl.from_pretrained(\"${model.id}\")\nmodel.generate(cond=\"path/to/image.png\")`,\n];\nexport const hezar = (model) => [\n    `from hezar import Model\n\nmodel = Model.load(\"${model.id}\")`,\n];\nexport const zonos = (model) => [\n    `# pip install git+https://github.com/Zyphra/Zonos.git\nimport torchaudio\nfrom zonos.model import Zonos\nfrom zonos.conditioning import make_cond_dict\n\nmodel = Zonos.from_pretrained(\"${model.id}\", device=\"cuda\")\n\nwav, sr = torchaudio.load(\"speaker.wav\")           # 5-10s reference clip\nspeaker = model.make_speaker_embedding(wav, sr)\n\ncond  = make_cond_dict(text=\"Hello, world!\", speaker=speaker, language=\"en-us\")\ncodes = model.generate(model.prepare_conditioning(cond))\n\naudio = model.autoencoder.decode(codes)[0].cpu()\ntorchaudio.save(\"sample.wav\", audio, model.autoencoder.sampling_rate)\n`,\n];\n//#endregion\n"],"names":[],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAAA;AACA;AACA;;;;AACA,MAAM,kBAAkB;AACxB,SAAS,qBAAqB,OAAO;IACjC,MAAM,WAAW,QAAQ,KAAK,CAAC;IAC/B,OAAO,SAAS,MAAM,KAAK,IAAI,QAAQ,CAAC,EAAE,GAAG,QAAQ,CAAC,EAAE;AAC5D;AACA,MAAM,sBAAsB,CAAC,MAAQ,KAAK,SAAS,CAAC,KAAK,KAAK,CAAC,GAAG,CAAC,IAAI,uEAAuE;AAEvI,MAAM,WAAW,CAAC,QAAU;QAC/B,CAAC;;0CAEqC,EAAE,MAAM,MAAM,EAAE,sBAAsB,WAAW;oBACvE,EAAE,MAAM,EAAE,CAAC,mBAAmB,CAAC;KAClD;AACD,MAAM,kBAAkB,CAAC,QAAU;QAC/B,CAAC;;;sCAGiC,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;KACnD;AACD,MAAM,4BAA4B,CAAC,QAAU;QACzC,CAAC;;;sCAGiC,EAAE,MAAM,EAAE,CAAC;;qDAEI,CAAC;KACrD;AACM,MAAM,WAAW,CAAC;IACrB,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,uBAAuB;QAC3C,OAAO,0BAA0B;IACrC;IACA,OAAO,gBAAgB;AAC3B;AACO,MAAM,UAAU,CAAC,QAAU;QAC9B,CAAC;;iCAE4B,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;KAC9C;AACM,MAAM,WAAW,CAAC,QAAU;QAC/B,CAAC;;mCAE8B,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;KAChD;AACM,MAAM,YAAY,CAAC;IACtB,MAAM,mBAAmB,CAAC;;;kCAGI,EAAE,MAAM,EAAE,CAAC;;;;;mCAKV,CAAC;IAChC,MAAM,kBAAkB,CAAC;;;oCAGO,EAAE,MAAM,EAAE,CAAC;;kEAEmB,CAAC;IAC/D,OAAO;QAAC;QAAkB;KAAgB;AAC9C;AACA,SAAS,yBAAyB,KAAK;IACnC,OAAO,MAAM,QAAQ,EAAE,YAAY,cAAc;AACrD;AACA,SAAS,gCAAgC,KAAK;IAC1C,MAAM,SAAS,MAAM,UAAU,EAAE,CAAC,EAAE,EAAE,QAAQ,MAAM,QAAQ,EAAE;IAC9D,IAAI,QAAQ;QACR,OAAO,oBAAoB;IAC/B;AACJ;AACO,MAAM,OAAO,CAAC,QAAU;QAC3B,CAAC;;;;;;;mCAO8B,EAAE,MAAM,EAAE,CAAC;;;AAG9C,CAAC;KACA;AACM,MAAM,WAAW,CAAC,QAAU;QAC/B,CAAC;;uBAEkB,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;KACpC;AACM,MAAM,QAAQ,CAAC,QAAU;QAC5B,CAAC;;kCAE6B,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;KAC/C;AACM,MAAM,aAAa,IAAM;QAC5B,CAAC;;;;;;;;;;;;;oCAa+B,CAAC;KACpC;AACM,MAAM,aAAa;IACtB,MAAM,iBAAiB,CAAC,yDAAyD,CAAC;IAClF,MAAM,wBAAwB,CAAC;;;;;;;;;;;;;;;;;;;;;sDAqBmB,CAAC;IACnD,MAAM,qBAAqB,CAAC;;;;;;;;;;;;;;;;;;;;;;;;;;sBA0BV,CAAC;IACnB,OAAO;QAAC;QAAgB;QAAuB;KAAmB;AACtE;AACO,MAAM,iBAAiB,IAAM;QAChC,CAAC;;;;;;;;;;;;yDAYoD,CAAC;KACzD;AACM,MAAM,oBAAoB,CAAC;IAC9B,IAAI;IACJ,IAAI;IACJ,IAAI;IACJ,UAAU;IACV,WAAW;IACX,eAAe;IACf,IAAI,MAAM,EAAE,KAAK,0CAA0C;QACvD,UAAU;QACV,WAAW;QACX,eAAe;IACnB,OACK,IAAI,MAAM,EAAE,KAAK,yCAAyC;QAC3D,UAAU;QACV,WAAW;QACX,eAAe;IACnB,OACK,IAAI,MAAM,EAAE,KAAK,0CAA0C;QAC5D,UAAU;QACV,WAAW;QACX,eAAe;IACnB;IACA,OAAO;QACH,CAAC;;;;;;;;;;iCAUwB,EAAE,QAAQ,YAAY,EAAE,SAAS,eAAe,EAAE,aAAa;;;oCAG5D,EAAE,MAAM,EAAE,CAAC,+BAA+B,EAAE,QAAQ;;;;;;IAMpF,CAAC;KACA;AACL;AACO,MAAM,YAAY,CAAC;IACtB,MAAM,iBAAiB,CAAC;;iDAEqB,EAAE,MAAM,EAAE,EAAE;IACzD,MAAM,mBAAmB,CAAC;;;;;;;;;;;;;;;;6CAgBe,CAAC;IAC1C,OAAO;QAAC;QAAgB;KAAiB;AAC7C;AACO,MAAM,kBAAkB,IAAM;QACjC,CAAC;;;;;;;;;;;;;;;;;;gDAkB2C,CAAC;KAChD;AACM,MAAM,MAAM,CAAC,QAAU;QAC1B,CAAC;;;6BAGwB,EAAE,MAAM,EAAE,CAAC;;;;qCAIH,CAAC;KACrC;AACM,MAAM,oBAAoB,CAAC,QAAU;QACxC,CAAC;;;;kBAIa,EAAE,MAAM,EAAE,CAAC;;;;;;CAM5B,CAAC;KACD;AACD,MAAM,yBAAyB;AAC/B,MAAM,gCAAgC;AACtC,MAAM,8BAA8B;AACpC,MAAM,oBAAoB,CAAC,QAAU;QACjC,CAAC;;0CAEqC,EAAE,MAAM,EAAE,CAAC;;UAE3C,EAAE,gCAAgC,UAAU,uBAAuB;8BAC/C,CAAC;KAC9B;AACD,MAAM,2BAA2B,CAAC,QAAU;QACxC,CAAC;;;0CAGqC,EAAE,MAAM,EAAE,CAAC;;UAE3C,EAAE,gCAAgC,UAAU,8BAA8B;;;wDAG5B,CAAC;KACxD;AACD,MAAM,2BAA2B,CAAC,QAAU;QACxC,CAAC;;;;0CAIqC,EAAE,MAAM,EAAE,CAAC;;;UAG3C,EAAE,gCAAgC,UAAU,4BAA4B;;;;;;qCAM7C,CAAC;KACrC;AACD,MAAM,uBAAuB,CAAC,QAAU;QACpC,CAAC;;8CAEyC,EAAE,MAAM,EAAE,CAAC;;EAEvD,EAAE,yBAAyB,OAAO;CACnC,CAAC;KACD;AACD,MAAM,iBAAiB,CAAC,QAAU;QAC9B,CAAC;;0CAEqC,EAAE,yBAAyB,OAAO;wBACpD,EAAE,MAAM,EAAE,CAAC;;UAEzB,EAAE,gCAAgC,UAAU,uBAAuB;8BAC/C,CAAC;KAC9B;AACD,MAAM,gCAAgC,CAAC,QAAU;QAC7C,CAAC;;;0CAGqC,EAAE,yBAAyB,OAAO;wBACpD,EAAE,MAAM,EAAE,CAAC;;UAEzB,EAAE,gCAAgC,UAAU,8BAA8B;;;wDAG5B,CAAC;KACxD;AACD,MAAM,+BAA+B,CAAC,QAAU;QAC5C,CAAC;;;0CAGqC,EAAE,yBAAyB,OAAO;wBACpD,EAAE,MAAM,EAAE,CAAC;;UAEzB,EAAE,gCAAgC,UAAU,4BAA4B;;;qCAG7C,CAAC;KACrC;AACD,MAAM,gCAAgC,CAAC,QAAU;QAC7C,CAAC;;;0CAGqC,EAAE,yBAAyB,OAAO;wBACpD,EAAE,MAAM,EAAE,CAAC;;UAEzB,EAAE,gCAAgC,UAAU,4BAA4B;;;;qCAI7C,CAAC;KACrC;AACD,MAAM,8BAA8B,CAAC,QAAU;QAC3C,CAAC;;0CAEqC,EAAE,yBAAyB,OAAO;6BAC/C,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;KAC1C;AACD,MAAM,sBAAsB,CAAC,QAAU;QACnC,CAAC;;;;;;;yCAOoC,EAAE,MAAM,EAAE,CAAC;;;;;;;;;;;;gCAYpB,CAAC;KAChC;AACD,MAAM,uBAAuB,CAAC,QAAU;QACpC,CAAC;;;;kDAI6C,EAAE,MAAM,EAAE,CAAC;;;;;;;;;;;;;;;;;;;WAmBlD,CAAC;KACX;AACM,MAAM,YAAY,CAAC;IACtB,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,qCACpB,MAAM,IAAI,CAAC,QAAQ,CAAC,qCAAqC;QACzD,OAAO,qBAAqB;IAChC,OACK,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,eAAe;QACxC,OAAO,qBAAqB;IAChC,OACK,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,SAAS;QAClC,IAAI,MAAM,YAAY,KAAK,kBAAkB;YACzC,OAAO,8BAA8B;QACzC,OACK,IAAI,MAAM,YAAY,KAAK,kBAAkB;YAC9C,OAAO,8BAA8B;QACzC,OACK,IAAI,MAAM,YAAY,KAAK,iBAAiB;YAC7C,OAAO,6BAA6B;QACxC,OACK;YACD,OAAO,eAAe;QAC1B;IACJ,OACK,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,sBAAsB;QAC/C,OAAO,4BAA4B;IACvC,OACK,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,qBAAqB;QAC9C,OAAO,oBAAoB;IAC/B,OACK,IAAI,MAAM,YAAY,KAAK,kBAAkB;QAC9C,OAAO,yBAAyB;IACpC,OACK,IAAI,MAAM,YAAY,KAAK,kBAAkB;QAC9C,OAAO,yBAAyB;IACpC,OACK;QACD,OAAO,kBAAkB;IAC7B;AACJ;AACO,MAAM,eAAe,CAAC;IACzB,MAAM,aAAa,CAAC;;;;;;eAMT,EAAE,MAAM,EAAE,CAAC;;;;CAIzB,CAAC;IACE,MAAM,cAAc,CAAC;;;;;gBAKT,EAAE,MAAM,EAAE,CAAC;;;;CAI1B,CAAC;IACE,MAAM,kBAAkB,CAAC;;;YAGjB,EAAE,MAAM,IAAI,CAAC,QAAQ,CAAC,UAAU,IAAI,GAAG;aACtC,EAAE,MAAM,IAAI,CAAC,QAAQ,CAAC,UAAU,IAAI,EAAE;;;;;;;CAOlD,CAAC;IACE,MAAM,kBAAkB,MAAM,IAAI,CAAC,QAAQ,CAAC,UAAU,cAAc;IACpE,OAAO;QAAC;QAAiB;KAAgB;AAC7C;AACO,MAAM,mBAAmB,CAAC,QAAU;QACvC,CAAC;;;;yCAIoC,EAAE,MAAM,EAAE,CAAC;;;;;;;;;;CAUnD,CAAC;KACD;AACM,MAAM,eAAe,CAAC,QAAU;QACnC,CAAC;;;6BAGwB,EAAE,MAAM,EAAE,CAAC;;;;;;;;;;;;;;AAcxC,CAAC;KACA;AACM,MAAM,SAAS,CAAC;IACnB,MAAM,cAAc,qBAAqB,MAAM,EAAE,EAAE,UAAU,CAAC,KAAK;IACnE,OAAO;QACH,CAAC;;mBAEU,EAAE,MAAM,EAAE,CAAC;AAC9B,CAAC;QACO,CAAC;wCAC+B,EAAE,MAAM,EAAE,CAAC;;;OAG5C,EAAE,YAAY;;MAEf,EAAE,YAAY,2BAA2B,EAAE,YAAY;AAC7D,CAAC;KACI;AACL;AACO,MAAM,YAAY,CAAC,QAAU;QAChC,CAAC;;qCAEgC,EAAE,MAAM,EAAE,CAAC;;kDAEE,CAAC;KAClD;AACM,MAAM,YAAY,CAAC,QAAU;QAChC,CAAC;;;GAGF,EAAE,MAAM,EAAE,CAAC;;;;2BAIa,CAAC;KAC3B;AACD,MAAM,gBAAgB,IAAM;QAAC,CAAC,2EAA2E,CAAC;KAAC;AACpG,MAAM,SAAS,CAAC;IACnB,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,mBAAmB;QACvC,OAAO,UAAU;IACrB,OACK,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,iCAAiC;QAC1D,OAAO,UAAU;IACrB;IACA,OAAO;AACX;AACO,MAAM,UAAU,CAAC,QAAU;QAC9B,CAAC;;;KAGA,EAAE,MAAM,EAAE,CAAC;CACf,CAAC;KACD;AACM,MAAM,QAAQ,CAAC,QAAU;QAC5B,CAAC;;8BAEyB,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;KAC3C;AACM,MAAM,SAAS,CAAC,QAAU;QAC7B,CAAC;;gCAE2B,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;KAC7C;AACM,MAAM,WAAW,CAAC,QAAU;QAC/B,CAAC;;;kBAGa,EAAE,MAAM,EAAE,CAAC;;;;;;;;;;;mCAWM,CAAC;KACnC;AACM,MAAM,UAAU,CAAC,QAAU;QAC9B,CAAC;;wDAEmD,CAAC;QACrD,CAAC;;;;;;;;mCAQ8B,EAAE,MAAM,EAAE,CAAC;;MAExC,CAAC;KACN;AACM,MAAM,QAAQ,CAAC,QAAU;QAC5B,CAAC;;;;;;sCAMiC,EAAE,MAAM,EAAE,CAAC;AACjD,CAAC;KACA;AACD,MAAM,uBAAuB,CAAC,UAAY,CAAC;;;;wDAIa,EAAE,QAAQ;;;;;AAKlE,CAAC;AACD,MAAM,2BAA2B,CAAC,UAAY,CAAC;;;;+DAIgB,EAAE,QAAQ;;;;AAIzE,CAAC;AACD,MAAM,6BAA6B,CAAC,UAAY,CAAC;;;;;UAKvC,EAAE,QAAQ;;;;;;;AAOpB,CAAC;AACD,MAAM,8BAA8B,CAAC,UAAY,CAAC;;;;;;UAMxC,EAAE,QAAQ;;;;;;;;;;AAUpB,CAAC;AACD,MAAM,gCAAgC;IAClC,UAAU;IACV,aAAa;IACb,gBAAgB;IAChB,iBAAiB;AACrB;AACA,MAAM,kCAAkC,CAAC,MAAM,UAAY,CAAC;;;WAGjD,EAAE,KAAK;wBACM,EAAE,KAAK,mBAAmB,EAAE,QAAQ;AAC5D,CAAC;AACD,MAAM,8BAA8B,CAAC,UAAY,CAAC;;;;uDAIK,EAAE,QAAQ;AACjE,CAAC;AACM,MAAM,YAAY,CAAC;IACtB,MAAM,UAAU,MAAM,EAAE;IACxB,MAAM,QAAQ,MAAM,MAAM,EAAE,WAAW,SAAS,EAAE;IAClD,MAAM,WAAW,EAAE;IACnB,sCAAsC;IACtC,KAAK,MAAM,CAAC,MAAM,QAAQ,IAAI,OAAO,OAAO,CAAC,+BAAgC;QACzE,IAAI,MAAM,QAAQ,CAAC,OAAO;YACtB,SAAS,IAAI,CAAC,QAAQ;QAC1B;IACJ;IACA,4BAA4B;IAC5B,KAAK,MAAM,QAAQ,MAAO;QACtB,IAAI,CAAC,OAAO,IAAI,CAAC,+BAA+B,QAAQ,CAAC,OAAO;YAC5D,SAAS,IAAI,CAAC,gCAAgC,MAAM;QACxD;IACJ;IACA,wCAAwC;IACxC,SAAS,IAAI,CAAC,4BAA4B;IAC1C,OAAO;AACX;AACO,MAAM,aAAa,CAAC,QAAU;QACjC,CAAC;;;;;8BAKyB,EAAE,MAAM,EAAE,CAAC;;;;;;;;;;;;;;;;;;;;;;;;AAwBzC,CAAC;KACA;AACM,MAAM,YAAY,CAAC,QAAU;QAChC,CAAC;eACU,EAAE,MAAM,EAAE,CAAC;;;;;;oCAMU,CAAC;KACpC;AACM,MAAM,eAAe,CAAC;IACzB,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,eAAe;QACnC,OAAO;YACH,CAAC;;;yBAGY,EAAE,MAAM,EAAE,CAAC;;8CAEU,CAAC;SACtC;IACL,OACK,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,kBAAkB;QAC3C,OAAO;YACH,CAAC;;;4BAGe,EAAE,MAAM,EAAE,CAAC;;8CAEO,CAAC;SACtC;IACL;IACA,OAAO;QACH,CAAC;;;;;yBAKgB,EAAE,MAAM,EAAE,CAAC;8BACN,EAAE,MAAM,EAAE,CAAC;;8CAEK,CAAC;KAC1C;AACL;AACO,MAAM,mBAAmB,CAAC;IAC7B,MAAM,WAAW;QACb,CAAC;;;;;UAKC,EAAE,MAAM,EAAE,CAAC;;;AAGrB,CAAC;KACI;IACD,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,mBAAmB;QACvC,MAAM,WAAW,IAAA,qMAAoB,EAAC;QACtC,SAAS,IAAI,CAAC,CAAC;YACX,EAAE,IAAA,kMAAiB,EAAC,UAAU;YAAE,oBAAoB;YAAM,QAAQ;QAAK,GAAG;CACrF,CAAC;IACE,OACK;QACD,SAAS,IAAI,CAAC,CAAC;;;;;aAKV,CAAC;IACV;IACA,OAAO;AACX;AACO,MAAM,UAAU,CAAC;IACpB,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,YAAY;QAChC,MAAM,kBAAkB;YACpB,uBAAuB;YACvB,CAAC;;;yBAGY,CAAC;YACd,mBAAmB;YACnB,CAAC;;cAEC,EAAE,MAAM,EAAE,CAAC;;;;;;;mBAON,CAAC;SACX;QACD,IAAI,MAAM,EAAE,KAAK,wBAAwB;YACrC,6CAA6C;YAC7C,gBAAgB,IAAI,CAAC,CAAC;;;;;;;;;;gBAUlB,EAAE,MAAM,EAAE,EAAE;QACpB;QACA,OAAO;IACX;IACA,OAAO,EAAE;AACb;AACO,MAAM,WAAW,CAAC,QAAU;QAC/B,CAAC;;;;+BAI0B,EAAE,MAAM,EAAE,CAAC;AAC1C,CAAC;KACA;AACM,MAAM,YAAY,CAAC,QAAU;QAChC,CAAC;;0CAEqC,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;KACvD;AACM,MAAM,YAAY,CAAC,QAAU;QAChC,CAAC;;;kCAG6B,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;KAC/C;AACM,MAAM,YAAY,CAAC,QAAU;QAChC,CAAC;;;mCAG8B,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;QAC7C,CAAC;;2BAEsB,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;KACxC;AACM,MAAM,gBAAgB,IAAM;QAC/B,CAAC;;;;;;0BAMqB,CAAC;KAC1B;AACM,MAAM,YAAY,CAAC,QAAU;QAChC,CAAC;;wFAEmF,EAAE,MAAM,EAAE,CAAC;4CACvD,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;KACzD;AACM,MAAM,YAAY,CAAC;IACtB,IAAI,MAAM,MAAM,EAAE,eAAe,CAAC,EAAE,EAAE;QAClC,MAAM,eAAe,MAAM,MAAM,CAAC,aAAa,CAAC,EAAE;QAClD,OAAO;YACH;gBACI,CAAC,kDAAkD,EAAE,cAAc;gBACnE;gBACA,CAAC,2CAA2C,EAAE,MAAM,EAAE,CAAC,oBAAoB,CAAC;gBAC5E,CAAC,QAAQ,EAAE,aAAa,kBAAkB,EAAE,MAAM,EAAE,CAAC,oBAAoB,CAAC;aAC7E,CAAC,IAAI,CAAC;SACV;IACL,OACK;QACD,OAAO;YACH;gBACI,CAAC,0BAA0B,CAAC;gBAC5B,CAAC,2DAA2D,CAAC;gBAC7D;gBACA,CAAC,2CAA2C,EAAE,MAAM,EAAE,CAAC,oBAAoB,CAAC;gBAC5E,CAAC,mCAAmC,EAAE,MAAM,EAAE,CAAC,oBAAoB,CAAC;aACvE,CAAC,IAAI,CAAC;SACV;IACL;AACJ;AACO,MAAM,YAAY,CAAC;IACtB,MAAM,UAAU;QACZ,oBAAoB;YAAE,WAAW;QAAgB;QACjD,sBAAsB;YAAE,WAAW;QAAkB;QACrD,qBAAqB;YAAE,WAAW;QAAoB;QACtD,mBAAmB;YAAE,WAAW;QAAqB;QACrD,oCAAoC;YAAE,WAAW;QAAkC;QACnF,qCAAqC;YAAE,WAAW;QAAoC;QACtF,eAAe;YAAE,WAAW;QAAe;QAC3C,qBAAqB;YAAE,WAAW;QAAqB;QACvD,kBAAkB;YAAE,WAAW;QAAkB;QACjD,uBAAuB;YAAE,WAAW;QAAsB;QAC1D,4BAA4B;YAAE,WAAW;QAAsB;QAC/D,6BAA6B;YAAE,WAAW;QAA4B;IAC1E;IACA,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,YAAY;QAChC,OAAO;YACH,CAAC;;2BAEc,EAAE,qBAAqB,MAAM,EAAE,EAAE;;;;;;;mDAOT,CAAC;SAC3C;IACL;IACA,KAAK,MAAM,OAAO,MAAM,IAAI,CAAE;QAC1B,IAAI,OAAO,SAAS;YAChB,MAAM,EAAE,SAAS,EAAE,GAAG,OAAO,CAAC,IAAI;YAClC,OAAO;gBACH,CAAC;sBACK,EAAE,UAAU;QAC1B,EAAE,UAAU,aAAa,EAAE,qBAAqB,MAAM,EAAE,EAAE;;;;;mDAKf,CAAC;aACvC;QACL;IACJ;IACA,OAAO;QACH,CAAC;iGACwF,CAAC;KAC7F;AACL;AACO,MAAM,qBAAqB,CAAC;IAC/B,MAAM,aAAa,CAAC;;;6BAGK,EAAE,MAAM,EAAE,CAAC,mBAAmB,CAAC;IACxD,MAAM,iBAAiB,CAAC;;;0CAGc,EAAE,MAAM,EAAE,CAAC,mBAAmB,CAAC;IACrE,IAAI,MAAM,EAAE,CAAC,QAAQ,CAAC,SAAS;QAC3B,OAAO;YAAC;YAAY;SAAe;IACvC,OACK;QACD,OAAO;YAAC;SAAe;IAC3B;AACJ;AACO,MAAM,cAAc,CAAC,QAAU;QAClC,CAAC;;;oCAG+B,EAAE,MAAM,EAAE,CAAC;;;;;oDAKK,CAAC;KACpD;AACM,MAAM,0BAA0B,CAAC,QAAU;QAC9C,CAAC;;qCAEgC,EAAE,MAAM,EAAE,CAAC;;;;;;;;;;;4DAWY,CAAC;KAC5D;AACD,MAAM,uBAAuB,CAAC,QAAU;QACpC,CAAC;;+BAE0B,EAAE,MAAM,EAAE,CAAC;;;;;;;;;mCASP,CAAC;KACnC;AACM,MAAM,iBAAiB,CAAC;IAC3B,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,4BAA4B;QAChD,OAAO,wBAAwB;IACnC;IACA,OAAO,qBAAqB;AAChC;AACO,MAAM,QAAQ,CAAC,QAAU;QAC5B,CAAC;;+BAE0B,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;KAC5C;AACM,MAAM,eAAe,CAAC,QAAU;QACnC,CAAC;;;0DAGqD,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;KACvE;AACD,MAAM,yBAAyB,CAAC,QAAU;QACtC,CAAC;;2CAEsC,EAAE,MAAM,EAAE,CAAC;qCACjB,EAAE,MAAM,EAAE,CAAC;AAChD,CAAC;KACA;AACD,MAAM,wBAAwB,CAAC,QAAU;QACrC,CAAC;;qCAEgC,EAAE,MAAM,EAAE,CAAC;;AAEhD,CAAC;KACA;AACD,MAAM,uBAAuB,CAAC,QAAU;QACpC,CAAC;;qCAEgC,EAAE,MAAM,EAAE,CAAC;AAChD,CAAC;KACA;AACM,MAAM,gBAAgB,CAAC;IAC1B,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,gBAAgB;QACpC,OAAO,uBAAuB;IAClC,OACK,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,eAAe;QACxC,OAAO,sBAAsB;IACjC;IACA,OAAO,qBAAqB;AAChC;AACO,MAAM,OAAO,CAAC,QAAU;QAC3B,CAAC;;kCAE6B,EAAE,MAAM,EAAE,CAAC,mBAAmB,CAAC;KAChE;AACM,MAAM,UAAU,IAA6B;QAChD,CAAC;;;;;;CAMJ,CAAC;KACD;AACM,MAAM,aAAa,IAAM;QAC5B,CAAC;;;;;;;;;;;8DAWyD,CAAC;KAC9D;AACD,MAAM,cAAc,CAAC,OAAO;IACxB,OAAO;QACH,CAAC;;UAEC,EAAE,MAAM,EAAE,CAAC;;EAEnB,EAAE,UAAU;;;iFAGmE,CAAC;KAC7E;AACL;AACA,MAAM,cAAc,CAAC,OAAO;IACxB,OAAO;QACH,CAAC;;UAEC,EAAE,MAAM,EAAE,CAAC;;;6BAGQ,EAAE,UAAU,EAAE,CAAC;KACvC;AACL;AACA,MAAM,cAAc,CAAC;IACjB,OAAO;QACH,CAAC;;;kBAGS,EAAE,MAAM,EAAE,CAAC;;;iFAGoD,CAAC;KAC7E;AACL;AACO,MAAM,UAAU,CAAC;IACpB,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,UAAU;QAC9B,MAAM,iBAAiB,MAAM,MAAM,EAAE,SAAS,OAAO;QACrD,MAAM,kBAAkB,MAAM,MAAM,EAAE,SAAS;QAC/C,IAAI,CAAC,gBAAgB;YACjB,OAAO;gBAAC,CAAC,gDAAgD,CAAC;aAAC;QAC/D;QACA,IAAI,oBAAoB,UAAU;YAC9B,OAAO,YAAY,OAAO;QAC9B,OACK;YACD,OAAO,YAAY,OAAO;QAC9B;IACJ,OACK;QACD,OAAO,YAAY;IACvB;AACJ;AACO,MAAM,qBAAqB,CAAC,QAAU;QACzC,CAAC;;;;;;;;;4CASuC,EAAE,MAAM,EAAE,CAAC;;;;;;;;;;;;;;;;;;;;;;;;kDAwBL,CAAC;KAClD;AACM,MAAM,SAAS,CAAC,QAAU;QAC7B,CAAC;;gCAE2B,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;KAC7C;AACM,MAAM,OAAO,CAAC;IACjB,MAAM,kBAAkB,CAAC;;;;+CAIkB,EAAE,MAAM,EAAE,CAAC;;;;oDAIN,CAAC;IACjD,MAAM,kBAAkB,CAAC;;;;+CAIkB,EAAE,MAAM,EAAE,CAAC;;;;;;;;;;WAU/C,CAAC;IACR,OAAO;QAAC;QAAiB;KAAgB;AAC7C;AACO,MAAM,gBAAgB,CAAC,QAAU;QACpC,CAAC,sDAAsD,EAAE,MAAM,EAAE,CAAC,eAAe,CAAC;KACrF;AACD,SAAS,kCAAkC,KAAK;IAC5C,MAAM,gBAAgB,MAAM,UAAU,EAAE,CAAC,EAAE;IAC3C,IAAI,eAAe,mBAAmB,eAAe,WAAW,QAAQ;QACpE,OAAO;YAAC,cAAc,eAAe;eAAK,cAAc,SAAS;SAAC;IACtE;AACJ;AACO,MAAM,uBAAuB,CAAC;IACjC,MAAM,sBAAsB,MAAM,IAAI,CAAC,QAAQ,CAAC,mBAAmB,6BAA6B;IAChG,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,oBAAoB,MAAM,YAAY,IAAI,gBAAgB;QAC9E,OAAO;YACH,CAAC;;sBAES,EAAE,MAAM,EAAE,CAAC,CAAC,EAAE,oBAAoB;;;;;;;;;;;aAW3C,CAAC;SACL;IACL;IACA,MAAM,mBAAmB,kCAAkC,UAAU;QACjE;QACA;QACA;KACH;IACD,OAAO;QACH,CAAC;;6BAEoB,EAAE,MAAM,EAAE,CAAC,CAAC,EAAE,oBAAoB;;YAEnD,EAAE,KAAK,SAAS,CAAC,kBAAkB,MAAM,GAAG;;;;;GAKrD,EAAE,iBAAiB,MAAM,CAAC,EAAE,EAAE,iBAAiB,MAAM,CAAC,CAAC,CAAC;KACtD;AACL;AACO,MAAM,SAAS,CAAC,QAAU;QAC7B,CAAC;;qCAEgC,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;KAClD;AACM,MAAM,QAAQ,CAAC,QAAU;QAC5B,CAAC,oCAAoC,EAAE,MAAM,EAAE,CAAC,cAAc,EAAE,qBAAqB,MAAM,EAAE,EAAE;;;;kBAIjF,EAAE,qBAAqB,MAAM,EAAE,EAAE;;;OAG5C,EAAE,qBAAqB,MAAM,EAAE,EAAE;MAClC,EAAE,qBAAqB,MAAM,EAAE,EAAE,OAAO,CAAC;KAC9C;AACM,MAAM,cAAc,CAAC,QAAU;QAClC,CAAC;;yCAEoC,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;KACtD;AACM,MAAM,SAAS,CAAC,QAAU;QAC7B,CAAC;;iBAEY,EAAE,qBAAqB,MAAM,EAAE,EAAE,OAAO,CAAC,WAAW,IAAI;uBAClD,EAAE,qBAAqB,MAAM,EAAE,EAAE,OAAO,CAAC,WAAW,IAAI,EAAE,CAAC;KACjF;AACD,MAAM,oBAAoB,CAAC;IACvB,OAAQ;QACJ,KAAK;YACD,OAAO;QACX,KAAK;QACL,KAAK;YACD,OAAO;QACX,KAAK;YACD,OAAO;QACX,KAAK;YACD,OAAO;QACX;YACI,OAAO;IACf;AACJ;AACO,MAAM,cAAc,CAAC;IACxB,MAAM,uBAAuB,MAAM,MAAM,EAAE,aAAa;IACxD,IAAI,yBAAyB,WAAW;QACpC,OAAO;YAAC,CAAC,wCAAwC,CAAC;SAAC;IACvD;IACA,MAAM,oBAAoB,kBAAkB;IAC5C,IAAI,sBAAsB,WAAW;QACjC,OAAO;YAAC,CAAC,kCAAkC,CAAC;SAAC;IACjD;IACA,OAAO;QACH,CAAC,mCAAmC,EAAE,qBAAqB;QAC3D,EAAE,qBAAqB;GAC5B,EAAE,MAAM,EAAE,CAAC;;MAER,EAAE,kBAAkB,YAAY,CAAC;KAClC;AACL;AACO,MAAM,aAAa,CAAC,QAAU;QACjC,CAAC;;iCAE4B,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;KAC9C;AACD,MAAM,kBAAkB,CAAC,QAAU,MAAM,MAAM,EAAE,kBAAkB,kBAAkB,aACjF,MAAM,MAAM,EAAE,kBAAkB,kBAAkB,aAClD,MAAM,MAAM,EAAE,wBAAwB;AACnC,MAAM,eAAe,CAAC;IACzB,MAAM,OAAO,MAAM,gBAAgB;IACnC,IAAI,CAAC,MAAM;QACP,OAAO;YAAC,CAAC,0BAA0B,CAAC;SAAC;IACzC;IACA,MAAM,sBAAsB,MAAM,IAAI,CAAC,QAAQ,CAAC,mBAAmB,6BAA6B;IAChG,MAAM,cAAc,EAAE;IACtB,IAAI,KAAK,SAAS,EAAE;QAChB,MAAM,mBAAmB,KAAK,SAAS,KAAK,kBACtC,cACA,KAAK,SAAS,KAAK,yBACf,cACA;QACV,YAAY,IAAI,CAAC,yBAAyB,CAAC,yBAAyB,EAAE,KAAK,SAAS,CAAC,EAAE,EAAE,KAAK,UAAU,EAAE,EAAE,IAAI,GAAG,iBAAiB,GAAG,EAAE,KAAK,SAAS,CAAC,kBAAkB,EAAE,MAAM,EAAE,CAAC,CAAC,CAAC,GAAG,sBAAsB,KAAK,CAAC,QAAQ,EAAE,KAAK,UAAU,CAAC,kBAAkB,EAAE,MAAM,EAAE,CAAC,CAAC,CAAC,GAAG,sBAAsB;QACxS,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,qBAAqB,gBAAgB,QAAQ;YACjE,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,uBAAuB;gBAC3C,YAAY,IAAI,CAAC,gBAAgB;oBAC7B;oBACA;oBACA;oBACA;oBACA;oBACA;oBACA;iBACH,CAAC,IAAI,CAAC,OAAO;YAClB,OACK;gBACD,YAAY,IAAI,CAAC,gBAAgB,oDAAoD;YACzF;YACA,YAAY,IAAI,CAAC,CAAC,SAAS,EAAE,iBAAiB,qBAAqB,CAAC,EAAE,cAAc,gCAAgC,mBAAmB,sBAAsB,yBAAyB,sBAAsB,IAAI,yDAAyD,CAAC,MAAM,EAAE,iBAAiB,oDAAoD,CAAC;QAC5V;IACJ,OACK;QACD,YAAY,IAAI,CAAC,yBAAyB,CAAC,yBAAyB,EAAE,KAAK,UAAU,EAAE,EAAE,CAAC,QAAQ,EAAE,KAAK,UAAU,CAAC,kBAAkB,EAAE,MAAM,EAAE,CAAC,CAAC,CAAC,GAAG,sBAAsB;IAChL;IACA,IAAI,MAAM,YAAY,IAAI,yMAAoB,CAAC,YAAY,EAAE,SAAS,MAAM,YAAY,GAAG;QACvF,MAAM,kBAAkB;YACpB;YACA;YACA;YACA,CAAC,iBAAiB,EAAE,MAAM,YAAY,CAAC,UAAU,EAAE,MAAM,EAAE,CAAC,CAAC,CAAC,GAAG,sBAAsB;SAC1F;QACD,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,mBAAmB;YACvC,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,uBAAuB;gBAC3C,gBAAgB,IAAI,CAAC,gBAAgB;oBACjC;oBACA;oBACA;oBACA;oBACA;oBACA;oBACA;iBACH,CAAC,IAAI,CAAC,OAAO;gBACd,gBAAgB,IAAI,CAAC;YACzB,OACK;gBACD,gBAAgB,IAAI,CAAC,gBAAgB,oDAAoD;gBACzF,gBAAgB,IAAI,CAAC;YACzB;QACJ,OACK,IAAI,MAAM,YAAY,KAAK,kCAAkC;YAC9D,gBAAgB,IAAI,CAAC,SAAS,wGAAwG,4DAA4D;QACtM,OACK,IAAI,MAAM,YAAY,KAAK,wBAAwB;YACpD,gBAAgB,IAAI,CAAC;QACzB;QACA,OAAO;YAAC,gBAAgB,IAAI,CAAC;YAAO,YAAY,IAAI,CAAC;SAAM;IAC/D;IACA,OAAO;QAAC,YAAY,IAAI,CAAC;KAAM;AACnC;AACO,MAAM,iBAAiB,CAAC;IAC3B,IAAI,CAAC,MAAM,YAAY,EAAE;QACrB,OAAO;YAAC,CAAC,0BAA0B,CAAC;SAAC;IACzC;IACA,MAAM,UAAU;IAChB,OAAO;QACH,CAAC,SAAS,EAAE,QAAQ;0BACF,EAAE,QAAQ;;;6BAGP,EAAE,MAAM,YAAY,CAAC,IAAI,EAAE,MAAM,EAAE,CAAC,GAAG,CAAC;KAChE;AACL;AACA,MAAM,WAAW,CAAC;IACd,OAAQ;QACJ,KAAK;YACD,OAAO;QACX,KAAK;YACD,OAAO;QACX,KAAK;YACD,OAAO;QACX,KAAK;YACD,OAAO;QACX;YACI,OAAO;IACf;AACJ;AACO,MAAM,OAAO,CAAC;IACjB,MAAM,EAAE,yBAAyB,aAAa,EAAE,WAAW,YAAY,EAAE,GAAG,MAAM,MAAM,EAAE,QAAQ,CAAC;IACnG,MAAM,WAAW,SAAS;IAC1B,IAAI,CAAC,UAAU;QACX,OAAO;YAAC,CAAC,qBAAqB,CAAC;SAAC;IACpC;IACA,IAAI,CAAC,eAAe;QAChB,OAAO;YAAC,CAAC,wBAAwB,CAAC;SAAC;IACvC;IACA,OAAO;QACH,CAAC;qCAC4B,EAAE,SAAS;;yBAEvB,EAAE,SAAS,kBAAkB,EAAE,cAAc;+CACvB,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;KACxD;AACL;AACO,MAAM,WAAW,CAAC,QAAU;QAC/B,CAAC;;;6CAGwC,EAAE,MAAM,EAAE,CAAC,gBAAgB,CAAC;KACxE;AACM,MAAM,mBAAmB,CAAC,QAAU;QACvC,CAAC;;UAEK,EAAE,MAAM,EAAE,CAAC;;CAEpB,CAAC;KACD;AACD,MAAM,qBAAqB,CAAC,QAAQ;IAChC,OAAQ;QACJ,KAAK;YACD,OAAO;gBACH,CAAC;sDACqC,EAAE,MAAM,EAAE,CAAC;;mDAEd,CAAC;aACvC;QACL;YACI,OAAO;IACf;AACJ;AACO,MAAM,WAAW,CAAC,QAAU;QAC/B,CAAC,iCAAiC,EAAE,MAAM,EAAE,CAAC,qCAAqC,CAAC;KACtF;AACM,MAAM,SAAS,IAA6B;QAC/C,CAAC;;;;AAIL,CAAC;KACA;AACM,MAAM,OAAO,CAAC,QAAU;QAC3B,CAAC;;;;;;;2BAOsB,EAAE,MAAM,EAAE,CAAC;;;;;;;;;EASpC,CAAC;KACF;AACM,MAAM,aAAa,CAAC,QAAU;QACjC,CAAC;;;;2BAIsB,EAAE,MAAM,EAAE,CAAC;2CACK,EAAE,MAAM,EAAE,CAAC;;;;4DAIM,CAAC;KAC5D;AACM,MAAM,WAAW,CAAC,QAAU;QAC/B,CAAC;;+BAE0B,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;KAC5C;AACM,MAAM,SAAS,CAAC,QAAU;QAC7B,CAAC;;;8BAGyB,EAAE,MAAM,EAAE,CAAC;;;iDAGQ,CAAC;KACjD;AACM,MAAM,aAAa,CAAC,QAAU;QACjC,CAAC;;oCAE+B,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;KACjD;AACM,MAAM,SAAS,CAAC,QAAU;QAC7B,CAAC;;;gCAG2B,EAAE,MAAM,EAAE,CAAC;;;;;;;;;;;;;;;;0BAgBjB,CAAC;KAC1B;AACM,MAAM,MAAM,IAAM;QACrB,CAAC;;;;;;;;;;;;;;AAcL,CAAC;KACA;AACM,MAAM,UAAU,IAAM;QACzB,CAAC;;;;;;;;;;gEAU2D,CAAC;KAChE;AACM,MAAM,cAAc,CAAC;IACxB,6DAA6D;IAC7D,MAAM,aAAa,MAAM,IAAI,CAAC,IAAI,CAAC,CAAC,MAAQ,IAAI,KAAK,CAAC;IACtD,MAAM,YAAY,aAAa,CAAC,KAAK,EAAE,WAAW,KAAK,CAAC,IAAI,GAAG;IAC/D,MAAM,SAAS,aACT,KACA,CAAC,mFAAmF,CAAC;IAC3F,OAAO;QACH,SACI,CAAC,wBAAwB,EAAE,UAAU;;QAEzC,EAAE,UAAU,kBAAkB,EAAE,MAAM,EAAE,CAAC;;uCAEV,CAAC;KACnC;AACL;AACO,MAAM,WAAW,CAAC,QAAU;QAC/B,CAAC;;;0DAGqD,EAAE,MAAM,EAAE,CAAC;AACrE,CAAC;QACG,CAAC;;;;;kCAK6B,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;KAC/C;AACM,MAAM,cAAc,CAAC,QAAU;QAClC,CAAC;;0CAEqC,EAAE,MAAM,EAAE,CAAC;AACrD,CAAC;KACA;AACM,MAAM,QAAQ,CAAC,QAAU;QAC5B,CAAC;;;;SAII,EAAE,MAAM,EAAE,CAAC;;;;4CAIwB,EAAE,MAAM,EAAE,CAAC;AACvD,CAAC;KACA;AACD,MAAM,cAAc,CAAC,QAAU;QAC3B,CAAC;;;qCAGgC,EAAE,qBAAqB,MAAM,EAAE,EAAE,CAAC,EAAE,MAAM,EAAE,EAAE;KAClF;AACD,MAAM,QAAQ,CAAC,QAAU;QACrB,CAAC;;;;;;;yBAOoB,EAAE,MAAM,EAAE,CAAC;;;8DAG0B,CAAC;KAC9D;AACD,MAAM,UAAU,CAAC,QAAU;QACvB,CAAC;;;;;;yBAMoB,EAAE,MAAM,EAAE,CAAC;;;;;;;;8DAQ0B,CAAC;KAC9D;AACD,MAAM,SAAS,CAAC,QAAU;QACtB,CAAC;;;;;;;;yBAQoB,EAAE,MAAM,EAAE,CAAC;sBACd,EAAE,MAAM,EAAE,CAAC;;;;;;;;;;;;;aAapB,CAAC;KACb;AACM,MAAM,QAAQ,CAAC,QAAU;QAC5B,CAAC;;qBAEgB,EAAE,MAAM,EAAE,CAAC,CAAC,CAAC;KACjC;AACM,MAAM,MAAM,CAAC;IAChB,IAAI,MAAM,YAAY,KAAK,sBAAsB;QAC7C,OAAO,OAAO;IAClB;IACA,IAAI,MAAM,YAAY,KAAK,mBAAmB;QAC1C,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,mBAAmB;YACvC,OAAO,QAAQ;QACnB,OACK;YACD,OAAO,MAAM;QACjB;IACJ;IACA,OAAO,YAAY;AACvB;AACO,MAAM,YAAY,CAAC,QAAU;QAChC,CAAC;;qCAEgC,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;KAClD;AACM,MAAM,QAAQ,CAAC;IAClB,IAAI;IACJ,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,cAAc;QAClC,WAAW,gBAAgB;IAC/B,OACK,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,iBAAiB;QAC1C,WAAW,mBAAmB;IAClC,OACK;QACD,WAAW,cAAc;IAC7B;IACA,MAAM,yBAAyB,CAAC;QAC5B,IAAI,CAAC,iCAAiC,IAAI,CAAC,UAAU;YACjD,OAAO,CAAC,8BAA8B,EAAE,SAAS;QACrD;QACA,OAAO;IACX;IACA,WAAW,SAAS,GAAG,CAAC;IACxB,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,iBAAiB;QACrC,OAAO,SAAS,GAAG,CAAC,CAAC,UAAY,QAAQ,OAAO,CAAC,cAAc,aAAa,OAAO,CAAC,mBAAmB;IAC3G;IACA,OAAO;AACX;AACA,MAAM,kBAAkB,CAAC;IACrB,MAAM,oBAAoB,UAAU;IACpC,OAAO,kBAAkB,GAAG,CAAC,CAAC,UAAY,OACtC,2CAA2C;SAC1C,OAAO,CAAC,uBAAuB,aAChC,mDAAmD;SAClD,OAAO,CAAC,qDAAqD,IAC7D,OAAO,CAAC,mEAAmE,4BAC3E,OAAO,CAAC,mCAAmC,GAC5C,yBAAyB;SACxB,OAAO,CAAC,qCAAqC,gCAC7C,OAAO,CAAC,gDAAgD,4BACxD,OAAO,CAAC,gDAAgD,2BACzD,sBAAsB;SACrB,OAAO,CAAC,UAAU,MAClB,IAAI;AACb;AACA,MAAM,qBAAqB,CAAC;IACxB,MAAM,OAAO,MAAM,gBAAgB;IACnC,MAAM,uBAAuB,aAAa;IAC1C,mCAAmC;IACnC,IAAI,oBAAoB,qBAAqB,GAAG,CAAC,CAAC,UAAY,QACzD,OAAO,CAAC,sCAAsC,gCAC9C,OAAO,CAAC,sBAAsB,CAAC,4BAA4B,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;IAC9E,qDAAqD;IACrD,IAAI,MAAM,YAAY;QAClB,oBAAoB,kBAAkB,GAAG,CAAC,CAAC,UAAY,QAClD,OAAO,CAAC,IAAI,OAAO,CAAC,yBAAyB,EAAE,KAAK,UAAU,CAAC,GAAG,CAAC,EAAE,MAAM,IAC3E,OAAO,CAAC,IAAI,OAAO,GAAG,KAAK,UAAU,CAAC,gBAAgB,CAAC,EAAE,MAAM,8BAC/D,OAAO,CAAC,IAAI,OAAO,CAAC,qBAAqB,EAAE,KAAK,UAAU,CAAC,KAAK,CAAC,EAAE,OAAO,CAAC,OAAS,KAAK,OAAO,CAAC,IAAI,OAAO,CAAC,GAAG,EAAE,KAAK,UAAU,EAAE,EAAE,MAAM;IACpJ;IACA,OAAO;AACX;AACA,MAAM,gBAAgB,CAAC,QAAU;QAC7B,CAAC;oCAC+B,EAAE,MAAM,EAAE,CAAC;AAC/C,CAAC;KACA;AACM,MAAM,OAAO,CAAC;IACjB,IAAI,UAAU;IACd,8CAA8C;IAC9C,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,iCAAiC;QACrD,UAAU,mBAAmB,OAAO;IACxC;IACA,OAAO,WAAW;QAAC,CAAC,gDAAgD,CAAC;KAAC;AAC1E;AACO,MAAM,UAAU,CAAC;IACpB,+CAA+C;IAC/C,MAAM,IAAI,MAAM,IAAI,IAAI,EAAE;IAC1B,IAAI,EAAE,QAAQ,CAAC,WAAW,EAAE,QAAQ,CAAC,SACjC,OAAO,EAAE;IACb,qCAAqC;IACrC,OAAO;QACH,CAAC;;;yBAGgB,EAAE,MAAM,EAAE,CAAC;;;;;;;;;;;EAWlC,CAAC;KACE;AACL;AACO,MAAM,OAAO,CAAC,QAAU;QAC3B,CAAC;;mCAE8B,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;KAChD;AACM,MAAM,SAAS,CAAC,QAAU;QAC7B,CAAC;;oCAE+B,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;KACjD;AACD,MAAM,WAAW,CAAC,QAAU;QACxB,CAAC;;iCAE4B,EAAE,MAAM,EAAE,CAAC;;;0DAGc,CAAC;KAC1D;AACD,MAAM,SAAS,CAAC,QAAU;QACtB,CAAC;;+BAE0B,EAAE,MAAM,EAAE,CAAC;;;0DAGgB,CAAC;KAC1D;AACD,MAAM,WAAW,CAAC,QAAU;QACxB,CAAC;;iCAE4B,EAAE,MAAM,EAAE,CAAC;;;0DAGc,CAAC;KAC1D;AACM,MAAM,SAAS,CAAC,QAAU;QAC7B,CAAC;;;uDAGkD,EAAE,MAAM,EAAE,CAAC;;8BAEpC,CAAC;KAC9B;AACM,MAAM,aAAa,CAAC;IACvB,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,aAAa;QACjC,OAAO,SAAS;IACpB,OACK,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,aAAa;QACtC,OAAO,SAAS;IACpB,OACK,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,WAAW;QACpC,OAAO,OAAO;IAClB,OACK;QACD,OAAO;YAAC,CAAC,wBAAwB,CAAC;SAAC;IACvC;AACJ;AACO,MAAM,aAAa,IAAM;QAC5B,CAAC;;;;;;;;;;8GAUyG,CAAC;KAC9G;AACM,MAAM,iBAAiB,CAAC,QAAU;QACrC,CAAC;;wCAEmC,EAAE,MAAM,EAAE,CAAC;wCACX,CAAC;KACxC;AACM,MAAM,QAAQ,CAAC,QAAU;QAC5B,CAAC;;oBAEe,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;KACjC;AACM,MAAM,QAAQ,CAAC,QAAU;QAC5B,CAAC;;;;;+BAK0B,EAAE,MAAM,EAAE,CAAC;;;;;;;;;;AAU1C,CAAC;KACA,EACD,YAAY","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 8416, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/model-libraries.js"],"sourcesContent":["import * as snippets from \"./model-libraries-snippets.js\";\n/**\n * Add your new library here.\n *\n * This is for modeling (= architectures) libraries, not for file formats (like ONNX, etc).\n * (unlike libraries, file formats live in an enum inside the internal codebase.)\n *\n * Doc on how to add a library to the Hub:\n *\n * https://huggingface.co/docs/hub/models-adding-libraries\n *\n * /!\\ IMPORTANT\n *\n * The key you choose is the tag your models have in their library_name on the Hub.\n */\nexport const MODEL_LIBRARIES_UI_ELEMENTS = {\n    acestep: {\n        prettyLabel: \"ACE-Step\",\n        repoName: \"ACE-Step\",\n        repoUrl: \"https://github.com/ace-step/ACE-Step\",\n        filter: false,\n        countDownloads: `path:\"ace_step_transformer/config.json\"`,\n    },\n    \"adapter-transformers\": {\n        prettyLabel: \"Adapters\",\n        repoName: \"adapters\",\n        repoUrl: \"https://github.com/Adapter-Hub/adapters\",\n        docsUrl: \"https://huggingface.co/docs/hub/adapters\",\n        snippets: snippets.adapters,\n        filter: true,\n        countDownloads: `path:\"adapter_config.json\"`,\n    },\n    allennlp: {\n        prettyLabel: \"AllenNLP\",\n        repoName: \"AllenNLP\",\n        repoUrl: \"https://github.com/allenai/allennlp\",\n        docsUrl: \"https://huggingface.co/docs/hub/allennlp\",\n        snippets: snippets.allennlp,\n        filter: true,\n    },\n    anemoi: {\n        prettyLabel: \"AnemoI\",\n        repoName: \"AnemoI\",\n        repoUrl: \"https://github.com/ecmwf/anemoi-inference\",\n        docsUrl: \"https://anemoi.readthedocs.io/en/latest/\",\n        filter: false,\n        countDownloads: `path_extension:\"ckpt\"`,\n        snippets: snippets.anemoi,\n    },\n    araclip: {\n        prettyLabel: \"AraClip\",\n        repoName: \"AraClip\",\n        repoUrl: \"https://huggingface.co/Arabic-Clip/araclip\",\n        filter: false,\n        snippets: snippets.araclip,\n    },\n    asteroid: {\n        prettyLabel: \"Asteroid\",\n        repoName: \"Asteroid\",\n        repoUrl: \"https://github.com/asteroid-team/asteroid\",\n        docsUrl: \"https://huggingface.co/docs/hub/asteroid\",\n        snippets: snippets.asteroid,\n        filter: true,\n        countDownloads: `path:\"pytorch_model.bin\"`,\n    },\n    audiocraft: {\n        prettyLabel: \"Audiocraft\",\n        repoName: \"audiocraft\",\n        repoUrl: \"https://github.com/facebookresearch/audiocraft\",\n        snippets: snippets.audiocraft,\n        filter: false,\n        countDownloads: `path:\"state_dict.bin\"`,\n    },\n    audioseal: {\n        prettyLabel: \"AudioSeal\",\n        repoName: \"audioseal\",\n        repoUrl: \"https://github.com/facebookresearch/audioseal\",\n        filter: false,\n        countDownloads: `path_extension:\"pth\"`,\n        snippets: snippets.audioseal,\n    },\n    \"bagel-mot\": {\n        prettyLabel: \"Bagel\",\n        repoName: \"Bagel\",\n        repoUrl: \"https://github.com/ByteDance-Seed/Bagel/\",\n        filter: false,\n        countDownloads: `path:\"llm_config.json\"`,\n    },\n    bboxmaskpose: {\n        prettyLabel: \"BBoxMaskPose\",\n        repoName: \"BBoxMaskPose\",\n        repoUrl: \"https://github.com/MiraPurkrabek/BBoxMaskPose\",\n        filter: false,\n        countDownloads: `path_extension:\"pth\"`,\n    },\n    ben2: {\n        prettyLabel: \"BEN2\",\n        repoName: \"BEN2\",\n        repoUrl: \"https://github.com/PramaLLC/BEN2\",\n        snippets: snippets.ben2,\n        filter: false,\n    },\n    bertopic: {\n        prettyLabel: \"BERTopic\",\n        repoName: \"BERTopic\",\n        repoUrl: \"https://github.com/MaartenGr/BERTopic\",\n        snippets: snippets.bertopic,\n        filter: true,\n    },\n    big_vision: {\n        prettyLabel: \"Big Vision\",\n        repoName: \"big_vision\",\n        repoUrl: \"https://github.com/google-research/big_vision\",\n        filter: false,\n        countDownloads: `path_extension:\"npz\"`,\n    },\n    birder: {\n        prettyLabel: \"Birder\",\n        repoName: \"Birder\",\n        repoUrl: \"https://gitlab.com/birder/birder\",\n        filter: false,\n        countDownloads: `path_extension:\"pt\"`,\n    },\n    birefnet: {\n        prettyLabel: \"BiRefNet\",\n        repoName: \"BiRefNet\",\n        repoUrl: \"https://github.com/ZhengPeng7/BiRefNet\",\n        snippets: snippets.birefnet,\n        filter: false,\n    },\n    bm25s: {\n        prettyLabel: \"BM25S\",\n        repoName: \"bm25s\",\n        repoUrl: \"https://github.com/xhluca/bm25s\",\n        snippets: snippets.bm25s,\n        filter: false,\n        countDownloads: `path:\"params.index.json\"`,\n    },\n    champ: {\n        prettyLabel: \"Champ\",\n        repoName: \"Champ\",\n        repoUrl: \"https://github.com/fudan-generative-vision/champ\",\n        countDownloads: `path:\"champ/motion_module.pth\"`,\n    },\n    chatterbox: {\n        prettyLabel: \"Chatterbox\",\n        repoName: \"Chatterbox\",\n        repoUrl: \"https://github.com/resemble-ai/chatterbox\",\n        snippets: snippets.chatterbox,\n        countDownloads: `path:\"tokenizer.json\"`,\n        filter: false,\n    },\n    chat_tts: {\n        prettyLabel: \"ChatTTS\",\n        repoName: \"ChatTTS\",\n        repoUrl: \"https://github.com/2noise/ChatTTS.git\",\n        snippets: snippets.chattts,\n        filter: false,\n        countDownloads: `path:\"asset/GPT.pt\"`,\n    },\n    \"cloud-agents\": {\n        prettyLabel: \"Cloud Agents\",\n        repoName: \"Cloud Agents\",\n        repoUrl: \"https://huggingface.co/OpenPeerAI/Cloud-Agents\",\n        filter: false,\n        countDownloads: `path:\"setup.py\"`,\n    },\n    colpali: {\n        prettyLabel: \"ColPali\",\n        repoName: \"ColPali\",\n        repoUrl: \"https://github.com/ManuelFay/colpali\",\n        filter: false,\n        countDownloads: `path:\"adapter_config.json\"`,\n    },\n    comet: {\n        prettyLabel: \"COMET\",\n        repoName: \"COMET\",\n        repoUrl: \"https://github.com/Unbabel/COMET/\",\n        countDownloads: `path:\"hparams.yaml\"`,\n    },\n    contexttab: {\n        prettyLabel: \"ConTextTab\",\n        repoName: \"ConTextTab\",\n        repoUrl: \"https://github.com/SAP-samples/contexttab\",\n        countDownloads: `path_extension:\"pt\"`,\n        snippets: snippets.contexttab,\n    },\n    cosmos: {\n        prettyLabel: \"Cosmos\",\n        repoName: \"Cosmos\",\n        repoUrl: \"https://github.com/NVIDIA/Cosmos\",\n        countDownloads: `path:\"config.json\" OR path_extension:\"pt\"`,\n    },\n    \"cxr-foundation\": {\n        prettyLabel: \"CXR Foundation\",\n        repoName: \"cxr-foundation\",\n        repoUrl: \"https://github.com/google-health/cxr-foundation\",\n        snippets: snippets.cxr_foundation,\n        filter: false,\n        countDownloads: `path:\"precomputed_embeddings/embeddings.npz\" OR path:\"pax-elixr-b-text/saved_model.pb\"`,\n    },\n    deepforest: {\n        prettyLabel: \"DeepForest\",\n        repoName: \"deepforest\",\n        docsUrl: \"https://deepforest.readthedocs.io/en/latest/\",\n        repoUrl: \"https://github.com/weecology/DeepForest\",\n    },\n    \"depth-anything-v2\": {\n        prettyLabel: \"DepthAnythingV2\",\n        repoName: \"Depth Anything V2\",\n        repoUrl: \"https://github.com/DepthAnything/Depth-Anything-V2\",\n        snippets: snippets.depth_anything_v2,\n        filter: false,\n        countDownloads: `path_extension:\"pth\"`,\n    },\n    \"depth-pro\": {\n        prettyLabel: \"Depth Pro\",\n        repoName: \"Depth Pro\",\n        repoUrl: \"https://github.com/apple/ml-depth-pro\",\n        countDownloads: `path_extension:\"pt\"`,\n        snippets: snippets.depth_pro,\n        filter: false,\n    },\n    \"derm-foundation\": {\n        prettyLabel: \"Derm Foundation\",\n        repoName: \"derm-foundation\",\n        repoUrl: \"https://github.com/google-health/derm-foundation\",\n        snippets: snippets.derm_foundation,\n        filter: false,\n        countDownloads: `path:\"scin_dataset_precomputed_embeddings.npz\" OR path:\"saved_model.pb\"`,\n    },\n    \"describe-anything\": {\n        prettyLabel: \"Describe Anything\",\n        repoName: \"Describe Anything\",\n        repoUrl: \"https://github.com/NVlabs/describe-anything\",\n        snippets: snippets.describe_anything,\n        filter: false,\n    },\n    \"dia-tts\": {\n        prettyLabel: \"Dia\",\n        repoName: \"Dia\",\n        repoUrl: \"https://github.com/nari-labs/dia\",\n        snippets: snippets.dia,\n        filter: false,\n    },\n    diffree: {\n        prettyLabel: \"Diffree\",\n        repoName: \"Diffree\",\n        repoUrl: \"https://github.com/OpenGVLab/Diffree\",\n        filter: false,\n        countDownloads: `path:\"diffree-step=000010999.ckpt\"`,\n    },\n    diffusers: {\n        prettyLabel: \"Diffusers\",\n        repoName: \"🤗/diffusers\",\n        repoUrl: \"https://github.com/huggingface/diffusers\",\n        docsUrl: \"https://huggingface.co/docs/hub/diffusers\",\n        snippets: snippets.diffusers,\n        filter: true,\n        /// diffusers has its own more complex \"countDownloads\" query\n    },\n    diffusionkit: {\n        prettyLabel: \"DiffusionKit\",\n        repoName: \"DiffusionKit\",\n        repoUrl: \"https://github.com/argmaxinc/DiffusionKit\",\n        snippets: snippets.diffusionkit,\n    },\n    doctr: {\n        prettyLabel: \"docTR\",\n        repoName: \"doctr\",\n        repoUrl: \"https://github.com/mindee/doctr\",\n    },\n    cartesia_pytorch: {\n        prettyLabel: \"Cartesia Pytorch\",\n        repoName: \"Cartesia Pytorch\",\n        repoUrl: \"https://github.com/cartesia-ai/cartesia_pytorch\",\n        snippets: snippets.cartesia_pytorch,\n    },\n    cartesia_mlx: {\n        prettyLabel: \"Cartesia MLX\",\n        repoName: \"Cartesia MLX\",\n        repoUrl: \"https://github.com/cartesia-ai/cartesia_mlx\",\n        snippets: snippets.cartesia_mlx,\n    },\n    clipscope: {\n        prettyLabel: \"clipscope\",\n        repoName: \"clipscope\",\n        repoUrl: \"https://github.com/Lewington-pitsos/clipscope\",\n        filter: false,\n        countDownloads: `path_extension:\"pt\"`,\n    },\n    cosyvoice: {\n        prettyLabel: \"CosyVoice\",\n        repoName: \"CosyVoice\",\n        repoUrl: \"https://github.com/FunAudioLLM/CosyVoice\",\n        filter: false,\n        countDownloads: `path_extension:\"onnx\" OR path_extension:\"pt\"`,\n    },\n    cotracker: {\n        prettyLabel: \"CoTracker\",\n        repoName: \"CoTracker\",\n        repoUrl: \"https://github.com/facebookresearch/co-tracker\",\n        filter: false,\n        countDownloads: `path_extension:\"pth\"`,\n    },\n    edsnlp: {\n        prettyLabel: \"EDS-NLP\",\n        repoName: \"edsnlp\",\n        repoUrl: \"https://github.com/aphp/edsnlp\",\n        docsUrl: \"https://aphp.github.io/edsnlp/latest/\",\n        filter: false,\n        snippets: snippets.edsnlp,\n        countDownloads: `path_filename:\"config\" AND path_extension:\"cfg\"`,\n    },\n    elm: {\n        prettyLabel: \"ELM\",\n        repoName: \"elm\",\n        repoUrl: \"https://github.com/slicex-ai/elm\",\n        filter: false,\n        countDownloads: `path_filename:\"slicex_elm_config\" AND path_extension:\"json\"`,\n    },\n    espnet: {\n        prettyLabel: \"ESPnet\",\n        repoName: \"ESPnet\",\n        repoUrl: \"https://github.com/espnet/espnet\",\n        docsUrl: \"https://huggingface.co/docs/hub/espnet\",\n        snippets: snippets.espnet,\n        filter: true,\n    },\n    fairseq: {\n        prettyLabel: \"Fairseq\",\n        repoName: \"fairseq\",\n        repoUrl: \"https://github.com/pytorch/fairseq\",\n        snippets: snippets.fairseq,\n        filter: true,\n    },\n    fastai: {\n        prettyLabel: \"fastai\",\n        repoName: \"fastai\",\n        repoUrl: \"https://github.com/fastai/fastai\",\n        docsUrl: \"https://huggingface.co/docs/hub/fastai\",\n        snippets: snippets.fastai,\n        filter: true,\n    },\n    fastprint: {\n        prettyLabel: \"Fast Print\",\n        repoName: \"Fast Print\",\n        repoUrl: \"https://huggingface.co/OpenPeerAI/FastPrint\",\n        countDownloads: `path_extension:\"cs\"`,\n    },\n    fasttext: {\n        prettyLabel: \"fastText\",\n        repoName: \"fastText\",\n        repoUrl: \"https://fasttext.cc/\",\n        snippets: snippets.fasttext,\n        filter: true,\n        countDownloads: `path_extension:\"bin\"`,\n    },\n    flair: {\n        prettyLabel: \"Flair\",\n        repoName: \"Flair\",\n        repoUrl: \"https://github.com/flairNLP/flair\",\n        docsUrl: \"https://huggingface.co/docs/hub/flair\",\n        snippets: snippets.flair,\n        filter: true,\n        countDownloads: `path:\"pytorch_model.bin\"`,\n    },\n    fme: {\n        prettyLabel: \"Full Model Emulation\",\n        repoName: \"Full Model Emulation\",\n        repoUrl: \"https://github.com/ai2cm/ace\",\n        docsUrl: \"https://ai2-climate-emulator.readthedocs.io/en/latest/\",\n        filter: false,\n        countDownloads: `path_extension:\"tar\"`,\n    },\n    \"gemma.cpp\": {\n        prettyLabel: \"gemma.cpp\",\n        repoName: \"gemma.cpp\",\n        repoUrl: \"https://github.com/google/gemma.cpp\",\n        filter: false,\n        countDownloads: `path_extension:\"sbs\"`,\n    },\n    \"geometry-crafter\": {\n        prettyLabel: \"GeometryCrafter\",\n        repoName: \"GeometryCrafter\",\n        repoUrl: \"https://github.com/TencentARC/GeometryCrafter\",\n        countDownloads: `path:\"point_map_vae/diffusion_pytorch_model.safetensors\"`,\n    },\n    gliner: {\n        prettyLabel: \"GLiNER\",\n        repoName: \"GLiNER\",\n        repoUrl: \"https://github.com/urchade/GLiNER\",\n        snippets: snippets.gliner,\n        filter: false,\n        countDownloads: `path:\"gliner_config.json\"`,\n    },\n    \"glyph-byt5\": {\n        prettyLabel: \"Glyph-ByT5\",\n        repoName: \"Glyph-ByT5\",\n        repoUrl: \"https://github.com/AIGText/Glyph-ByT5\",\n        filter: false,\n        countDownloads: `path:\"checkpoints/byt5_model.pt\"`,\n    },\n    grok: {\n        prettyLabel: \"Grok\",\n        repoName: \"Grok\",\n        repoUrl: \"https://github.com/xai-org/grok-1\",\n        filter: false,\n        countDownloads: `path:\"ckpt/tensor00000_000\" OR path:\"ckpt-0/tensor00000_000\"`,\n    },\n    hallo: {\n        prettyLabel: \"Hallo\",\n        repoName: \"Hallo\",\n        repoUrl: \"https://github.com/fudan-generative-vision/hallo\",\n        countDownloads: `path:\"hallo/net.pth\"`,\n    },\n    hermes: {\n        prettyLabel: \"HERMES\",\n        repoName: \"HERMES\",\n        repoUrl: \"https://github.com/LMD0311/HERMES\",\n        filter: false,\n        countDownloads: `path:\"ckpt/hermes_final.pth\"`,\n    },\n    hezar: {\n        prettyLabel: \"Hezar\",\n        repoName: \"Hezar\",\n        repoUrl: \"https://github.com/hezarai/hezar\",\n        docsUrl: \"https://hezarai.github.io/hezar\",\n        countDownloads: `path:\"model_config.yaml\" OR path:\"embedding/embedding_config.yaml\"`,\n    },\n    htrflow: {\n        prettyLabel: \"HTRflow\",\n        repoName: \"HTRflow\",\n        repoUrl: \"https://github.com/AI-Riksarkivet/htrflow\",\n        docsUrl: \"https://ai-riksarkivet.github.io/htrflow\",\n        snippets: snippets.htrflow,\n    },\n    \"hunyuan-dit\": {\n        prettyLabel: \"HunyuanDiT\",\n        repoName: \"HunyuanDiT\",\n        repoUrl: \"https://github.com/Tencent/HunyuanDiT\",\n        countDownloads: `path:\"pytorch_model_ema.pt\" OR path:\"pytorch_model_distill.pt\"`,\n    },\n    \"hunyuan3d-2\": {\n        prettyLabel: \"Hunyuan3D-2\",\n        repoName: \"Hunyuan3D-2\",\n        repoUrl: \"https://github.com/Tencent/Hunyuan3D-2\",\n        countDownloads: `path_filename:\"model_index\" OR path_filename:\"config\"`,\n    },\n    \"hunyuanworld-voyager\": {\n        prettyLabel: \"HunyuanWorld-voyager\",\n        repoName: \"HunyuanWorld-voyager\",\n        repoUrl: \"https://github.com/Tencent-Hunyuan/HunyuanWorld-Voyager\",\n    },\n    imstoucan: {\n        prettyLabel: \"IMS Toucan\",\n        repoName: \"IMS-Toucan\",\n        repoUrl: \"https://github.com/DigitalPhonetics/IMS-Toucan\",\n        countDownloads: `path:\"embedding_gan.pt\" OR path:\"Vocoder.pt\" OR path:\"ToucanTTS.pt\"`,\n    },\n    \"index-tts\": {\n        prettyLabel: \"IndexTTS\",\n        repoName: \"IndexTTS\",\n        repoUrl: \"https://github.com/index-tts/index-tts\",\n        snippets: snippets.indextts,\n        filter: false,\n    },\n    infinitetalk: {\n        prettyLabel: \"InfiniteTalk\",\n        repoName: \"InfiniteTalk\",\n        repoUrl: \"https://github.com/MeiGen-AI/InfiniteTalk\",\n        filter: false,\n        countDownloads: `path_extension:\"safetensors\"`,\n    },\n    \"infinite-you\": {\n        prettyLabel: \"InfiniteYou\",\n        repoName: \"InfiniteYou\",\n        repoUrl: \"https://github.com/bytedance/InfiniteYou\",\n        filter: false,\n        countDownloads: `path:\"infu_flux_v1.0/sim_stage1/image_proj_model.bin\" OR path:\"infu_flux_v1.0/aes_stage2/image_proj_model.bin\"`,\n    },\n    keras: {\n        prettyLabel: \"Keras\",\n        repoName: \"Keras\",\n        repoUrl: \"https://github.com/keras-team/keras\",\n        docsUrl: \"https://huggingface.co/docs/hub/keras\",\n        snippets: snippets.keras,\n        filter: true,\n        countDownloads: `path:\"config.json\" OR path_extension:\"keras\"`,\n    },\n    \"tf-keras\": {\n        // Legacy \"Keras 2\" library (tensorflow-only)\n        prettyLabel: \"TF-Keras\",\n        repoName: \"TF-Keras\",\n        repoUrl: \"https://github.com/keras-team/tf-keras\",\n        docsUrl: \"https://huggingface.co/docs/hub/tf-keras\",\n        snippets: snippets.tf_keras,\n        countDownloads: `path:\"saved_model.pb\"`,\n    },\n    \"keras-hub\": {\n        prettyLabel: \"KerasHub\",\n        repoName: \"KerasHub\",\n        repoUrl: \"https://github.com/keras-team/keras-hub\",\n        docsUrl: \"https://keras.io/keras_hub/\",\n        snippets: snippets.keras_hub,\n        filter: true,\n    },\n    \"kimi-audio\": {\n        prettyLabel: \"KimiAudio\",\n        repoName: \"KimiAudio\",\n        repoUrl: \"https://github.com/MoonshotAI/Kimi-Audio\",\n        snippets: snippets.kimi_audio,\n        filter: false,\n    },\n    kittentts: {\n        prettyLabel: \"KittenTTS\",\n        repoName: \"KittenTTS\",\n        repoUrl: \"https://github.com/KittenML/KittenTTS\",\n        snippets: snippets.kittentts,\n    },\n    kronos: {\n        prettyLabel: \"KRONOS\",\n        repoName: \"KRONOS\",\n        repoUrl: \"https://github.com/mahmoodlab/KRONOS\",\n        filter: false,\n        countDownloads: `path_extension:\"pt\"`,\n    },\n    k2: {\n        prettyLabel: \"K2\",\n        repoName: \"k2\",\n        repoUrl: \"https://github.com/k2-fsa/k2\",\n    },\n    \"lightning-ir\": {\n        prettyLabel: \"Lightning IR\",\n        repoName: \"Lightning IR\",\n        repoUrl: \"https://github.com/webis-de/lightning-ir\",\n        snippets: snippets.lightning_ir,\n    },\n    \"litert-lm\": {\n        prettyLabel: \"LiteRT-LM\",\n        repoName: \"LiteRT-LM\",\n        repoUrl: \"https://github.com/google-ai-edge/LiteRT-LM\",\n        filter: false,\n        countDownloads: `path_extension:\"litertlm\"`,\n    },\n    lerobot: {\n        prettyLabel: \"LeRobot\",\n        repoName: \"LeRobot\",\n        repoUrl: \"https://github.com/huggingface/lerobot\",\n        docsUrl: \"https://huggingface.co/docs/lerobot\",\n        filter: false,\n        snippets: snippets.lerobot,\n    },\n    liveportrait: {\n        prettyLabel: \"LivePortrait\",\n        repoName: \"LivePortrait\",\n        repoUrl: \"https://github.com/KwaiVGI/LivePortrait\",\n        filter: false,\n        countDownloads: `path:\"liveportrait/landmark.onnx\"`,\n    },\n    \"llama-cpp-python\": {\n        prettyLabel: \"llama-cpp-python\",\n        repoName: \"llama-cpp-python\",\n        repoUrl: \"https://github.com/abetlen/llama-cpp-python\",\n        snippets: snippets.llama_cpp_python,\n    },\n    \"mini-omni2\": {\n        prettyLabel: \"Mini-Omni2\",\n        repoName: \"Mini-Omni2\",\n        repoUrl: \"https://github.com/gpt-omni/mini-omni2\",\n        countDownloads: `path:\"model_config.yaml\"`,\n    },\n    mindspore: {\n        prettyLabel: \"MindSpore\",\n        repoName: \"mindspore\",\n        repoUrl: \"https://github.com/mindspore-ai/mindspore\",\n    },\n    \"magi-1\": {\n        prettyLabel: \"MAGI-1\",\n        repoName: \"MAGI-1\",\n        repoUrl: \"https://github.com/SandAI-org/MAGI-1\",\n        countDownloads: `path:\"ckpt/vae/config.json\"`,\n    },\n    \"magenta-realtime\": {\n        prettyLabel: \"Magenta RT\",\n        repoName: \"Magenta RT\",\n        repoUrl: \"https://github.com/magenta/magenta-realtime\",\n        countDownloads: `path:\"checkpoints/llm_base_x4286_c1860k.tar\" OR path:\"checkpoints/llm_large_x3047_c1860k.tar\" OR path:\"checkpoints/llm_large_x3047_c1860k/checkpoint\"`,\n    },\n    \"mamba-ssm\": {\n        prettyLabel: \"MambaSSM\",\n        repoName: \"MambaSSM\",\n        repoUrl: \"https://github.com/state-spaces/mamba\",\n        filter: false,\n        snippets: snippets.mamba_ssm,\n    },\n    \"mars5-tts\": {\n        prettyLabel: \"MARS5-TTS\",\n        repoName: \"MARS5-TTS\",\n        repoUrl: \"https://github.com/Camb-ai/MARS5-TTS\",\n        filter: false,\n        countDownloads: `path:\"mars5_ar.safetensors\"`,\n        snippets: snippets.mars5_tts,\n    },\n    matanyone: {\n        prettyLabel: \"MatAnyone\",\n        repoName: \"MatAnyone\",\n        repoUrl: \"https://github.com/pq-yang/MatAnyone\",\n        snippets: snippets.matanyone,\n        filter: false,\n    },\n    \"mesh-anything\": {\n        prettyLabel: \"MeshAnything\",\n        repoName: \"MeshAnything\",\n        repoUrl: \"https://github.com/buaacyw/MeshAnything\",\n        filter: false,\n        countDownloads: `path:\"MeshAnything_350m.pth\"`,\n        snippets: snippets.mesh_anything,\n    },\n    merlin: {\n        prettyLabel: \"Merlin\",\n        repoName: \"Merlin\",\n        repoUrl: \"https://github.com/StanfordMIMI/Merlin\",\n        filter: false,\n        countDownloads: `path_extension:\"pt\"`,\n    },\n    medvae: {\n        prettyLabel: \"MedVAE\",\n        repoName: \"MedVAE\",\n        repoUrl: \"https://github.com/StanfordMIMI/MedVAE\",\n        filter: false,\n        countDownloads: `path_extension:\"ckpt\"`,\n    },\n    mitie: {\n        prettyLabel: \"MITIE\",\n        repoName: \"MITIE\",\n        repoUrl: \"https://github.com/mit-nlp/MITIE\",\n        countDownloads: `path_filename:\"total_word_feature_extractor\"`,\n    },\n    \"ml-agents\": {\n        prettyLabel: \"ml-agents\",\n        repoName: \"ml-agents\",\n        repoUrl: \"https://github.com/Unity-Technologies/ml-agents\",\n        docsUrl: \"https://huggingface.co/docs/hub/ml-agents\",\n        snippets: snippets.mlAgents,\n        filter: true,\n        countDownloads: `path_extension:\"onnx\"`,\n    },\n    mlx: {\n        prettyLabel: \"MLX\",\n        repoName: \"MLX\",\n        repoUrl: \"https://github.com/ml-explore/mlx-examples/tree/main\",\n        snippets: snippets.mlx,\n        filter: true,\n    },\n    \"mlx-image\": {\n        prettyLabel: \"mlx-image\",\n        repoName: \"mlx-image\",\n        repoUrl: \"https://github.com/riccardomusmeci/mlx-image\",\n        docsUrl: \"https://huggingface.co/docs/hub/mlx-image\",\n        snippets: snippets.mlxim,\n        filter: false,\n        countDownloads: `path:\"model.safetensors\"`,\n    },\n    \"mlc-llm\": {\n        prettyLabel: \"MLC-LLM\",\n        repoName: \"MLC-LLM\",\n        repoUrl: \"https://github.com/mlc-ai/mlc-llm\",\n        docsUrl: \"https://llm.mlc.ai/docs/\",\n        filter: false,\n        countDownloads: `path:\"mlc-chat-config.json\"`,\n    },\n    model2vec: {\n        prettyLabel: \"Model2Vec\",\n        repoName: \"model2vec\",\n        repoUrl: \"https://github.com/MinishLab/model2vec\",\n        snippets: snippets.model2vec,\n        filter: false,\n    },\n    moshi: {\n        prettyLabel: \"Moshi\",\n        repoName: \"Moshi\",\n        repoUrl: \"https://github.com/kyutai-labs/moshi\",\n        filter: false,\n        countDownloads: `path:\"tokenizer-e351c8d8-checkpoint125.safetensors\"`,\n    },\n    mtvcraft: {\n        prettyLabel: \"MTVCraft\",\n        repoName: \"MTVCraft\",\n        repoUrl: \"https://github.com/baaivision/MTVCraft\",\n        filter: false,\n        countDownloads: `path:\"vae/3d-vae.pt\"`,\n    },\n    nemo: {\n        prettyLabel: \"NeMo\",\n        repoName: \"NeMo\",\n        repoUrl: \"https://github.com/NVIDIA/NeMo\",\n        snippets: snippets.nemo,\n        filter: true,\n        countDownloads: `path_extension:\"nemo\" OR path:\"model_config.yaml\" OR path_extension:\"json\"`,\n    },\n    \"open-oasis\": {\n        prettyLabel: \"open-oasis\",\n        repoName: \"open-oasis\",\n        repoUrl: \"https://github.com/etched-ai/open-oasis\",\n        countDownloads: `path:\"oasis500m.safetensors\"`,\n    },\n    open_clip: {\n        prettyLabel: \"OpenCLIP\",\n        repoName: \"OpenCLIP\",\n        repoUrl: \"https://github.com/mlfoundations/open_clip\",\n        snippets: snippets.open_clip,\n        filter: true,\n        countDownloads: `path:\"open_clip_model.safetensors\"\n\t\t\tOR path:\"model.safetensors\"\n\t\t\tOR path:\"open_clip_pytorch_model.bin\"\n\t\t\tOR path:\"pytorch_model.bin\"`,\n    },\n    openpeerllm: {\n        prettyLabel: \"OpenPeerLLM\",\n        repoName: \"OpenPeerLLM\",\n        repoUrl: \"https://huggingface.co/openpeerai/openpeerllm\",\n        docsUrl: \"https://huggingface.co/OpenPeerAI/OpenPeerLLM/blob/main/README.md\",\n        countDownloads: `path:\".meta-huggingface.json\"`,\n        filter: false,\n    },\n    \"open-sora\": {\n        prettyLabel: \"Open-Sora\",\n        repoName: \"Open-Sora\",\n        repoUrl: \"https://github.com/hpcaitech/Open-Sora\",\n        filter: false,\n        countDownloads: `path:\"Open_Sora_v2.safetensors\"`,\n    },\n    outetts: {\n        prettyLabel: \"OuteTTS\",\n        repoName: \"OuteTTS\",\n        repoUrl: \"https://github.com/edwko/OuteTTS\",\n        snippets: snippets.outetts,\n        filter: false,\n    },\n    paddlenlp: {\n        prettyLabel: \"paddlenlp\",\n        repoName: \"PaddleNLP\",\n        repoUrl: \"https://github.com/PaddlePaddle/PaddleNLP\",\n        docsUrl: \"https://huggingface.co/docs/hub/paddlenlp\",\n        snippets: snippets.paddlenlp,\n        filter: true,\n        countDownloads: `path:\"model_config.json\"`,\n    },\n    PaddleOCR: {\n        prettyLabel: \"PaddleOCR\",\n        repoName: \"PaddleOCR\",\n        repoUrl: \"https://github.com/PaddlePaddle/PaddleOCR\",\n        snippets: snippets.paddleocr,\n        filter: true,\n    },\n    peft: {\n        prettyLabel: \"PEFT\",\n        repoName: \"PEFT\",\n        repoUrl: \"https://github.com/huggingface/peft\",\n        snippets: snippets.peft,\n        filter: true,\n        countDownloads: `path:\"adapter_config.json\"`,\n    },\n    \"perception-encoder\": {\n        prettyLabel: \"PerceptionEncoder\",\n        repoName: \"PerceptionModels\",\n        repoUrl: \"https://github.com/facebookresearch/perception_models\",\n        filter: false,\n        snippets: snippets.perception_encoder,\n        countDownloads: `path_extension:\"pt\"`,\n    },\n    \"phantom-wan\": {\n        prettyLabel: \"Phantom\",\n        repoName: \"Phantom\",\n        repoUrl: \"https://github.com/Phantom-video/Phantom\",\n        snippets: snippets.phantom_wan,\n        filter: false,\n        countDownloads: `path_extension:\"pth\"`,\n    },\n    \"pruna-ai\": {\n        prettyLabel: \"Pruna AI\",\n        repoName: \"Pruna AI\",\n        repoUrl: \"https://github.com/PrunaAI/pruna\",\n        snippets: snippets.pruna,\n        docsUrl: \"https://docs.pruna.ai\",\n    },\n    pxia: {\n        prettyLabel: \"pxia\",\n        repoName: \"pxia\",\n        repoUrl: \"https://github.com/not-lain/pxia\",\n        snippets: snippets.pxia,\n        filter: false,\n    },\n    \"pyannote-audio\": {\n        prettyLabel: \"pyannote.audio\",\n        repoName: \"pyannote-audio\",\n        repoUrl: \"https://github.com/pyannote/pyannote-audio\",\n        snippets: snippets.pyannote_audio,\n        filter: true,\n    },\n    \"py-feat\": {\n        prettyLabel: \"Py-Feat\",\n        repoName: \"Py-Feat\",\n        repoUrl: \"https://github.com/cosanlab/py-feat\",\n        docsUrl: \"https://py-feat.org/\",\n        filter: false,\n    },\n    pythae: {\n        prettyLabel: \"pythae\",\n        repoName: \"pythae\",\n        repoUrl: \"https://github.com/clementchadebec/benchmark_VAE\",\n        snippets: snippets.pythae,\n        filter: false,\n    },\n    recurrentgemma: {\n        prettyLabel: \"RecurrentGemma\",\n        repoName: \"recurrentgemma\",\n        repoUrl: \"https://github.com/google-deepmind/recurrentgemma\",\n        filter: false,\n        countDownloads: `path:\"tokenizer.model\"`,\n    },\n    relik: {\n        prettyLabel: \"Relik\",\n        repoName: \"Relik\",\n        repoUrl: \"https://github.com/SapienzaNLP/relik\",\n        snippets: snippets.relik,\n        filter: false,\n    },\n    refiners: {\n        prettyLabel: \"Refiners\",\n        repoName: \"Refiners\",\n        repoUrl: \"https://github.com/finegrain-ai/refiners\",\n        docsUrl: \"https://refine.rs/\",\n        filter: false,\n        countDownloads: `path:\"model.safetensors\"`,\n    },\n    renderformer: {\n        prettyLabel: \"RenderFormer\",\n        repoName: \"RenderFormer\",\n        repoUrl: \"https://github.com/microsoft/renderformer\",\n        snippets: snippets.renderformer,\n        filter: false,\n    },\n    reverb: {\n        prettyLabel: \"Reverb\",\n        repoName: \"Reverb\",\n        repoUrl: \"https://github.com/revdotcom/reverb\",\n        filter: false,\n    },\n    rkllm: {\n        prettyLabel: \"RKLLM\",\n        repoName: \"RKLLM\",\n        repoUrl: \"https://github.com/airockchip/rknn-llm\",\n        countDownloads: `path_extension:\"rkllm\"`,\n    },\n    saelens: {\n        prettyLabel: \"SAELens\",\n        repoName: \"SAELens\",\n        repoUrl: \"https://github.com/jbloomAus/SAELens\",\n        snippets: snippets.saelens,\n        filter: false,\n    },\n    sam2: {\n        prettyLabel: \"sam2\",\n        repoName: \"sam2\",\n        repoUrl: \"https://github.com/facebookresearch/segment-anything-2\",\n        filter: false,\n        snippets: snippets.sam2,\n        countDownloads: `path_extension:\"pt\"`,\n    },\n    \"sample-factory\": {\n        prettyLabel: \"sample-factory\",\n        repoName: \"sample-factory\",\n        repoUrl: \"https://github.com/alex-petrenko/sample-factory\",\n        docsUrl: \"https://huggingface.co/docs/hub/sample-factory\",\n        snippets: snippets.sampleFactory,\n        filter: true,\n        countDownloads: `path:\"cfg.json\"`,\n    },\n    sapiens: {\n        prettyLabel: \"sapiens\",\n        repoName: \"sapiens\",\n        repoUrl: \"https://github.com/facebookresearch/sapiens\",\n        filter: false,\n        countDownloads: `path_extension:\"pt2\" OR path_extension:\"pth\" OR path_extension:\"onnx\"`,\n    },\n    seedvr: {\n        prettyLabel: \"SeedVR\",\n        repoName: \"SeedVR\",\n        repoUrl: \"https://github.com/ByteDance-Seed/SeedVR\",\n        filter: false,\n        countDownloads: `path_extension:\"pth\"`,\n    },\n    \"self-forcing\": {\n        prettyLabel: \"SelfForcing\",\n        repoName: \"SelfForcing\",\n        repoUrl: \"https://github.com/guandeh17/Self-Forcing\",\n        filter: false,\n        countDownloads: `path_extension:\"pt\"`,\n    },\n    \"sentence-transformers\": {\n        prettyLabel: \"sentence-transformers\",\n        repoName: \"sentence-transformers\",\n        repoUrl: \"https://github.com/UKPLab/sentence-transformers\",\n        docsUrl: \"https://huggingface.co/docs/hub/sentence-transformers\",\n        snippets: snippets.sentenceTransformers,\n        filter: true,\n    },\n    setfit: {\n        prettyLabel: \"setfit\",\n        repoName: \"setfit\",\n        repoUrl: \"https://github.com/huggingface/setfit\",\n        docsUrl: \"https://huggingface.co/docs/hub/setfit\",\n        snippets: snippets.setfit,\n        filter: true,\n    },\n    sklearn: {\n        prettyLabel: \"Scikit-learn\",\n        repoName: \"Scikit-learn\",\n        repoUrl: \"https://github.com/scikit-learn/scikit-learn\",\n        snippets: snippets.sklearn,\n        filter: true,\n        countDownloads: `path:\"sklearn_model.joblib\"`,\n    },\n    spacy: {\n        prettyLabel: \"spaCy\",\n        repoName: \"spaCy\",\n        repoUrl: \"https://github.com/explosion/spaCy\",\n        docsUrl: \"https://huggingface.co/docs/hub/spacy\",\n        snippets: snippets.spacy,\n        filter: true,\n        countDownloads: `path_extension:\"whl\"`,\n    },\n    \"span-marker\": {\n        prettyLabel: \"SpanMarker\",\n        repoName: \"SpanMarkerNER\",\n        repoUrl: \"https://github.com/tomaarsen/SpanMarkerNER\",\n        docsUrl: \"https://huggingface.co/docs/hub/span_marker\",\n        snippets: snippets.span_marker,\n        filter: true,\n    },\n    speechbrain: {\n        prettyLabel: \"speechbrain\",\n        repoName: \"speechbrain\",\n        repoUrl: \"https://github.com/speechbrain/speechbrain\",\n        docsUrl: \"https://huggingface.co/docs/hub/speechbrain\",\n        snippets: snippets.speechbrain,\n        filter: true,\n        countDownloads: `path:\"hyperparams.yaml\"`,\n    },\n    \"ssr-speech\": {\n        prettyLabel: \"SSR-Speech\",\n        repoName: \"SSR-Speech\",\n        repoUrl: \"https://github.com/WangHelin1997/SSR-Speech\",\n        filter: false,\n        countDownloads: `path_extension:\".pth\"`,\n    },\n    \"stable-audio-tools\": {\n        prettyLabel: \"Stable Audio Tools\",\n        repoName: \"stable-audio-tools\",\n        repoUrl: \"https://github.com/Stability-AI/stable-audio-tools.git\",\n        filter: false,\n        countDownloads: `path:\"model.safetensors\"`,\n        snippets: snippets.stable_audio_tools,\n    },\n    monkeyocr: {\n        prettyLabel: \"MonkeyOCR\",\n        repoName: \"monkeyocr\",\n        repoUrl: \"https://github.com/Yuliang-Liu/MonkeyOCR\",\n        filter: false,\n        countDownloads: `path:\"Recognition/config.json\"`,\n    },\n    \"diffusion-single-file\": {\n        prettyLabel: \"Diffusion Single File\",\n        repoName: \"diffusion-single-file\",\n        repoUrl: \"https://github.com/comfyanonymous/ComfyUI\",\n        filter: false,\n        countDownloads: `path_extension:\"safetensors\"`,\n    },\n    \"seed-story\": {\n        prettyLabel: \"SEED-Story\",\n        repoName: \"SEED-Story\",\n        repoUrl: \"https://github.com/TencentARC/SEED-Story\",\n        filter: false,\n        countDownloads: `path:\"cvlm_llama2_tokenizer/tokenizer.model\"`,\n        snippets: snippets.seed_story,\n    },\n    soloaudio: {\n        prettyLabel: \"SoloAudio\",\n        repoName: \"SoloAudio\",\n        repoUrl: \"https://github.com/WangHelin1997/SoloAudio\",\n        filter: false,\n        countDownloads: `path:\"soloaudio_v2.pt\"`,\n    },\n    songbloom: {\n        prettyLabel: \"SongBloom\",\n        repoName: \"SongBloom\",\n        repoUrl: \"https://github.com/Cypress-Yang/SongBloom\",\n        filter: false,\n        countDownloads: `path_extension:\"pt\"`,\n    },\n    \"stable-baselines3\": {\n        prettyLabel: \"stable-baselines3\",\n        repoName: \"stable-baselines3\",\n        repoUrl: \"https://github.com/huggingface/huggingface_sb3\",\n        docsUrl: \"https://huggingface.co/docs/hub/stable-baselines3\",\n        snippets: snippets.stableBaselines3,\n        filter: true,\n        countDownloads: `path_extension:\"zip\"`,\n    },\n    stanza: {\n        prettyLabel: \"Stanza\",\n        repoName: \"stanza\",\n        repoUrl: \"https://github.com/stanfordnlp/stanza\",\n        docsUrl: \"https://huggingface.co/docs/hub/stanza\",\n        snippets: snippets.stanza,\n        filter: true,\n        countDownloads: `path:\"models/default.zip\"`,\n    },\n    swarmformer: {\n        prettyLabel: \"SwarmFormer\",\n        repoName: \"SwarmFormer\",\n        repoUrl: \"https://github.com/takara-ai/SwarmFormer\",\n        snippets: snippets.swarmformer,\n        filter: false,\n    },\n    \"f5-tts\": {\n        prettyLabel: \"F5-TTS\",\n        repoName: \"F5-TTS\",\n        repoUrl: \"https://github.com/SWivid/F5-TTS\",\n        filter: false,\n        countDownloads: `path_extension:\"safetensors\" OR path_extension:\"pt\"`,\n    },\n    genmo: {\n        prettyLabel: \"Genmo\",\n        repoName: \"Genmo\",\n        repoUrl: \"https://github.com/genmoai/models\",\n        filter: false,\n        countDownloads: `path:\"vae_stats.json\"`,\n    },\n    \"tencent-song-generation\": {\n        prettyLabel: \"SongGeneration\",\n        repoName: \"SongGeneration\",\n        repoUrl: \"https://github.com/tencent-ailab/songgeneration\",\n        filter: false,\n        countDownloads: `path:\"ckpt/songgeneration_base/model.pt\"`,\n    },\n    tensorflowtts: {\n        prettyLabel: \"TensorFlowTTS\",\n        repoName: \"TensorFlowTTS\",\n        repoUrl: \"https://github.com/TensorSpeech/TensorFlowTTS\",\n        snippets: snippets.tensorflowtts,\n    },\n    tabpfn: {\n        prettyLabel: \"TabPFN\",\n        repoName: \"TabPFN\",\n        repoUrl: \"https://github.com/PriorLabs/TabPFN\",\n    },\n    terratorch: {\n        prettyLabel: \"TerraTorch\",\n        repoName: \"TerraTorch\",\n        repoUrl: \"https://github.com/IBM/terratorch\",\n        docsUrl: \"https://ibm.github.io/terratorch/\",\n        filter: false,\n        countDownloads: `path_extension:\"pt\" OR path_extension:\"ckpt\"`,\n        snippets: snippets.terratorch,\n    },\n    \"tic-clip\": {\n        prettyLabel: \"TiC-CLIP\",\n        repoName: \"TiC-CLIP\",\n        repoUrl: \"https://github.com/apple/ml-tic-clip\",\n        filter: false,\n        countDownloads: `path_extension:\"pt\" AND path_prefix:\"checkpoints/\"`,\n    },\n    timesfm: {\n        prettyLabel: \"TimesFM\",\n        repoName: \"timesfm\",\n        repoUrl: \"https://github.com/google-research/timesfm\",\n        filter: false,\n        countDownloads: `path:\"checkpoints/checkpoint_1100000/state/checkpoint\" OR path:\"checkpoints/checkpoint_2150000/state/checkpoint\" OR path_extension:\"ckpt\"`,\n    },\n    timm: {\n        prettyLabel: \"timm\",\n        repoName: \"pytorch-image-models\",\n        repoUrl: \"https://github.com/rwightman/pytorch-image-models\",\n        docsUrl: \"https://huggingface.co/docs/hub/timm\",\n        snippets: snippets.timm,\n        filter: true,\n        countDownloads: `path:\"pytorch_model.bin\" OR path:\"model.safetensors\"`,\n    },\n    tirex: {\n        prettyLabel: \"TiRex\",\n        repoName: \"TiRex\",\n        repoUrl: \"https://github.com/NX-AI/tirex\",\n        countDownloads: `path_extension:\"ckpt\"`,\n    },\n    torchgeo: {\n        prettyLabel: \"TorchGeo\",\n        repoName: \"TorchGeo\",\n        repoUrl: \"https://github.com/microsoft/torchgeo\",\n        docsUrl: \"https://torchgeo.readthedocs.io/\",\n        filter: false,\n        countDownloads: `path_extension:\"pt\" OR path_extension:\"pth\"`,\n    },\n    transformers: {\n        prettyLabel: \"Transformers\",\n        repoName: \"🤗/transformers\",\n        repoUrl: \"https://github.com/huggingface/transformers\",\n        docsUrl: \"https://huggingface.co/docs/hub/transformers\",\n        snippets: snippets.transformers,\n        filter: true,\n    },\n    \"transformers.js\": {\n        prettyLabel: \"Transformers.js\",\n        repoName: \"transformers.js\",\n        repoUrl: \"https://github.com/huggingface/transformers.js\",\n        docsUrl: \"https://huggingface.co/docs/hub/transformers-js\",\n        snippets: snippets.transformersJS,\n        filter: true,\n    },\n    trellis: {\n        prettyLabel: \"Trellis\",\n        repoName: \"Trellis\",\n        repoUrl: \"https://github.com/microsoft/TRELLIS\",\n        countDownloads: `path_extension:\"safetensors\"`,\n    },\n    ultralytics: {\n        prettyLabel: \"ultralytics\",\n        repoName: \"ultralytics\",\n        repoUrl: \"https://github.com/ultralytics/ultralytics\",\n        docsUrl: \"https://github.com/ultralytics/ultralytics\",\n        filter: false,\n        countDownloads: `path_extension:\"pt\"`,\n        snippets: snippets.ultralytics,\n    },\n    univa: {\n        prettyLabel: \"univa\",\n        repoName: \"univa\",\n        repoUrl: \"https://github.com/PKU-YuanGroup/UniWorld-V1\",\n        snippets: snippets.univa,\n        filter: true,\n        countDownloads: `path:\"config.json\"`,\n    },\n    \"uni-3dar\": {\n        prettyLabel: \"Uni-3DAR\",\n        repoName: \"Uni-3DAR\",\n        repoUrl: \"https://github.com/dptech-corp/Uni-3DAR\",\n        docsUrl: \"https://github.com/dptech-corp/Uni-3DAR\",\n        countDownloads: `path_extension:\"pt\"`,\n    },\n    \"unity-sentis\": {\n        prettyLabel: \"unity-sentis\",\n        repoName: \"unity-sentis\",\n        repoUrl: \"https://github.com/Unity-Technologies/sentis-samples\",\n        snippets: snippets.sentis,\n        filter: true,\n        countDownloads: `path_extension:\"sentis\"`,\n    },\n    sana: {\n        prettyLabel: \"Sana\",\n        repoName: \"Sana\",\n        repoUrl: \"https://github.com/NVlabs/Sana\",\n        countDownloads: `path_extension:\"pth\"`,\n        snippets: snippets.sana,\n    },\n    videoprism: {\n        prettyLabel: \"VideoPrism\",\n        repoName: \"VideoPrism\",\n        repoUrl: \"https://github.com/google-deepmind/videoprism\",\n        countDownloads: `path_extension:\"npz\"`,\n        snippets: snippets.videoprism,\n    },\n    \"vfi-mamba\": {\n        prettyLabel: \"VFIMamba\",\n        repoName: \"VFIMamba\",\n        repoUrl: \"https://github.com/MCG-NJU/VFIMamba\",\n        countDownloads: `path_extension:\"pkl\"`,\n        snippets: snippets.vfimamba,\n    },\n    lvface: {\n        prettyLabel: \"LVFace\",\n        repoName: \"LVFace\",\n        repoUrl: \"https://github.com/bytedance/LVFace\",\n        countDownloads: `path_extension:\"pt\" OR path_extension:\"onnx\"`,\n        snippets: snippets.lvface,\n    },\n    voicecraft: {\n        prettyLabel: \"VoiceCraft\",\n        repoName: \"VoiceCraft\",\n        repoUrl: \"https://github.com/jasonppy/VoiceCraft\",\n        docsUrl: \"https://github.com/jasonppy/VoiceCraft\",\n        snippets: snippets.voicecraft,\n    },\n    voxcpm: {\n        prettyLabel: \"VoxCPM\",\n        repoName: \"VoxCPM\",\n        repoUrl: \"https://github.com/OpenBMB/VoxCPM\",\n        snippets: snippets.voxcpm,\n        filter: false,\n    },\n    vui: {\n        prettyLabel: \"Vui\",\n        repoName: \"Vui\",\n        repoUrl: \"https://github.com/vui-ai/vui\",\n        countDownloads: `path_extension:\"pt\"`,\n        snippets: snippets.vui,\n    },\n    \"wan2.2\": {\n        prettyLabel: \"Wan2.2\",\n        repoName: \"Wan2.2\",\n        repoUrl: \"https://github.com/Wan-Video/Wan2.2\",\n        countDownloads: `path_filename:\"config\" AND path_extension:\"json\"`,\n    },\n    wham: {\n        prettyLabel: \"WHAM\",\n        repoName: \"wham\",\n        repoUrl: \"https://huggingface.co/microsoft/wham\",\n        docsUrl: \"https://huggingface.co/microsoft/wham/blob/main/README.md\",\n        countDownloads: `path_extension:\"ckpt\"`,\n    },\n    whisperkit: {\n        prettyLabel: \"WhisperKit\",\n        repoName: \"WhisperKit\",\n        repoUrl: \"https://github.com/argmaxinc/WhisperKit\",\n        docsUrl: \"https://github.com/argmaxinc/WhisperKit?tab=readme-ov-file#homebrew\",\n        snippets: snippets.whisperkit,\n        countDownloads: `path_filename:\"model\" AND path_extension:\"mil\" AND _exists_:\"path_prefix\"`,\n    },\n    yolov10: {\n        // YOLOv10 is a fork of ultraLytics. Code snippets and download count are the same but the repo is different.\n        prettyLabel: \"YOLOv10\",\n        repoName: \"YOLOv10\",\n        repoUrl: \"https://github.com/THU-MIG/yolov10\",\n        docsUrl: \"https://github.com/THU-MIG/yolov10\",\n        countDownloads: `path_extension:\"pt\" OR path_extension:\"safetensors\"`,\n        snippets: snippets.ultralytics,\n    },\n    zonos: {\n        prettyLabel: \"Zonos\",\n        repoName: \"Zonos\",\n        repoUrl: \"https://github.com/Zyphra/Zonos\",\n        docsUrl: \"https://github.com/Zyphra/Zonos\",\n        snippets: snippets.zonos,\n        filter: false,\n    },\n    \"3dtopia-xl\": {\n        prettyLabel: \"3DTopia-XL\",\n        repoName: \"3DTopia-XL\",\n        repoUrl: \"https://github.com/3DTopia/3DTopia-XL\",\n        filter: false,\n        countDownloads: `path:\"model_vae_fp16.pt\"`,\n        snippets: snippets.threedtopia_xl,\n    },\n};\nexport const ALL_MODEL_LIBRARY_KEYS = Object.keys(MODEL_LIBRARIES_UI_ELEMENTS);\nexport const ALL_DISPLAY_MODEL_LIBRARY_KEYS = Object.entries(MODEL_LIBRARIES_UI_ELEMENTS)\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\n    .filter(([_, v]) => v.filter)\n    .map(([k]) => k);\n"],"names":[],"mappings":";;;;;;;;AAAA;;AAeO,MAAM,8BAA8B;IACvC,SAAS;QACL,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,uCAAuC,CAAC;IAC7D;IACA,wBAAwB;QACpB,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,qMAAiB;QAC3B,QAAQ;QACR,gBAAgB,CAAC,0BAA0B,CAAC;IAChD;IACA,UAAU;QACN,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,qMAAiB;QAC3B,QAAQ;IACZ;IACA,QAAQ;QACJ,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,qBAAqB,CAAC;QACvC,UAAU,mMAAe;IAC7B;IACA,SAAS;QACL,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,UAAU,oMAAgB;IAC9B;IACA,UAAU;QACN,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,qMAAiB;QAC3B,QAAQ;QACR,gBAAgB,CAAC,wBAAwB,CAAC;IAC9C;IACA,YAAY;QACR,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,uMAAmB;QAC7B,QAAQ;QACR,gBAAgB,CAAC,qBAAqB,CAAC;IAC3C;IACA,WAAW;QACP,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,oBAAoB,CAAC;QACtC,UAAU,sMAAkB;IAChC;IACA,aAAa;QACT,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,sBAAsB,CAAC;IAC5C;IACA,cAAc;QACV,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,oBAAoB,CAAC;IAC1C;IACA,MAAM;QACF,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,iMAAa;QACvB,QAAQ;IACZ;IACA,UAAU;QACN,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,qMAAiB;QAC3B,QAAQ;IACZ;IACA,YAAY;QACR,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,oBAAoB,CAAC;IAC1C;IACA,QAAQ;QACJ,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,mBAAmB,CAAC;IACzC;IACA,UAAU;QACN,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,qMAAiB;QAC3B,QAAQ;IACZ;IACA,OAAO;QACH,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,kMAAc;QACxB,QAAQ;QACR,gBAAgB,CAAC,wBAAwB,CAAC;IAC9C;IACA,OAAO;QACH,aAAa;QACb,UAAU;QACV,SAAS;QACT,gBAAgB,CAAC,8BAA8B,CAAC;IACpD;IACA,YAAY;QACR,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,uMAAmB;QAC7B,gBAAgB,CAAC,qBAAqB,CAAC;QACvC,QAAQ;IACZ;IACA,UAAU;QACN,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,oMAAgB;QAC1B,QAAQ;QACR,gBAAgB,CAAC,mBAAmB,CAAC;IACzC;IACA,gBAAgB;QACZ,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,eAAe,CAAC;IACrC;IACA,SAAS;QACL,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,0BAA0B,CAAC;IAChD;IACA,OAAO;QACH,aAAa;QACb,UAAU;QACV,SAAS;QACT,gBAAgB,CAAC,mBAAmB,CAAC;IACzC;IACA,YAAY;QACR,aAAa;QACb,UAAU;QACV,SAAS;QACT,gBAAgB,CAAC,mBAAmB,CAAC;QACrC,UAAU,uMAAmB;IACjC;IACA,QAAQ;QACJ,aAAa;QACb,UAAU;QACV,SAAS;QACT,gBAAgB,CAAC,yCAAyC,CAAC;IAC/D;IACA,kBAAkB;QACd,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,2MAAuB;QACjC,QAAQ;QACR,gBAAgB,CAAC,sFAAsF,CAAC;IAC5G;IACA,YAAY;QACR,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;IACb;IACA,qBAAqB;QACjB,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,8MAA0B;QACpC,QAAQ;QACR,gBAAgB,CAAC,oBAAoB,CAAC;IAC1C;IACA,aAAa;QACT,aAAa;QACb,UAAU;QACV,SAAS;QACT,gBAAgB,CAAC,mBAAmB,CAAC;QACrC,UAAU,sMAAkB;QAC5B,QAAQ;IACZ;IACA,mBAAmB;QACf,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,4MAAwB;QAClC,QAAQ;QACR,gBAAgB,CAAC,uEAAuE,CAAC;IAC7F;IACA,qBAAqB;QACjB,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,8MAA0B;QACpC,QAAQ;IACZ;IACA,WAAW;QACP,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,gMAAY;QACtB,QAAQ;IACZ;IACA,SAAS;QACL,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,kCAAkC,CAAC;IACxD;IACA,WAAW;QACP,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,sMAAkB;QAC5B,QAAQ;IAEZ;IACA,cAAc;QACV,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,yMAAqB;IACnC;IACA,OAAO;QACH,aAAa;QACb,UAAU;QACV,SAAS;IACb;IACA,kBAAkB;QACd,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,6MAAyB;IACvC;IACA,cAAc;QACV,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,yMAAqB;IACnC;IACA,WAAW;QACP,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,mBAAmB,CAAC;IACzC;IACA,WAAW;QACP,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,4CAA4C,CAAC;IAClE;IACA,WAAW;QACP,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,oBAAoB,CAAC;IAC1C;IACA,QAAQ;QACJ,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,QAAQ;QACR,UAAU,mMAAe;QACzB,gBAAgB,CAAC,+CAA+C,CAAC;IACrE;IACA,KAAK;QACD,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,2DAA2D,CAAC;IACjF;IACA,QAAQ;QACJ,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,mMAAe;QACzB,QAAQ;IACZ;IACA,SAAS;QACL,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,oMAAgB;QAC1B,QAAQ;IACZ;IACA,QAAQ;QACJ,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,mMAAe;QACzB,QAAQ;IACZ;IACA,WAAW;QACP,aAAa;QACb,UAAU;QACV,SAAS;QACT,gBAAgB,CAAC,mBAAmB,CAAC;IACzC;IACA,UAAU;QACN,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,qMAAiB;QAC3B,QAAQ;QACR,gBAAgB,CAAC,oBAAoB,CAAC;IAC1C;IACA,OAAO;QACH,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,kMAAc;QACxB,QAAQ;QACR,gBAAgB,CAAC,wBAAwB,CAAC;IAC9C;IACA,KAAK;QACD,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,oBAAoB,CAAC;IAC1C;IACA,aAAa;QACT,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,oBAAoB,CAAC;IAC1C;IACA,oBAAoB;QAChB,aAAa;QACb,UAAU;QACV,SAAS;QACT,gBAAgB,CAAC,wDAAwD,CAAC;IAC9E;IACA,QAAQ;QACJ,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,mMAAe;QACzB,QAAQ;QACR,gBAAgB,CAAC,yBAAyB,CAAC;IAC/C;IACA,cAAc;QACV,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,gCAAgC,CAAC;IACtD;IACA,MAAM;QACF,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,4DAA4D,CAAC;IAClF;IACA,OAAO;QACH,aAAa;QACb,UAAU;QACV,SAAS;QACT,gBAAgB,CAAC,oBAAoB,CAAC;IAC1C;IACA,QAAQ;QACJ,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,4BAA4B,CAAC;IAClD;IACA,OAAO;QACH,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,gBAAgB,CAAC,kEAAkE,CAAC;IACxF;IACA,SAAS;QACL,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,oMAAgB;IAC9B;IACA,eAAe;QACX,aAAa;QACb,UAAU;QACV,SAAS;QACT,gBAAgB,CAAC,8DAA8D,CAAC;IACpF;IACA,eAAe;QACX,aAAa;QACb,UAAU;QACV,SAAS;QACT,gBAAgB,CAAC,qDAAqD,CAAC;IAC3E;IACA,wBAAwB;QACpB,aAAa;QACb,UAAU;QACV,SAAS;IACb;IACA,WAAW;QACP,aAAa;QACb,UAAU;QACV,SAAS;QACT,gBAAgB,CAAC,mEAAmE,CAAC;IACzF;IACA,aAAa;QACT,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,qMAAiB;QAC3B,QAAQ;IACZ;IACA,cAAc;QACV,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,4BAA4B,CAAC;IAClD;IACA,gBAAgB;QACZ,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,8GAA8G,CAAC;IACpI;IACA,OAAO;QACH,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,kMAAc;QACxB,QAAQ;QACR,gBAAgB,CAAC,4CAA4C,CAAC;IAClE;IACA,YAAY;QACR,6CAA6C;QAC7C,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,qMAAiB;QAC3B,gBAAgB,CAAC,qBAAqB,CAAC;IAC3C;IACA,aAAa;QACT,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,sMAAkB;QAC5B,QAAQ;IACZ;IACA,cAAc;QACV,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,uMAAmB;QAC7B,QAAQ;IACZ;IACA,WAAW;QACP,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,sMAAkB;IAChC;IACA,QAAQ;QACJ,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,mBAAmB,CAAC;IACzC;IACA,IAAI;QACA,aAAa;QACb,UAAU;QACV,SAAS;IACb;IACA,gBAAgB;QACZ,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,yMAAqB;IACnC;IACA,aAAa;QACT,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,yBAAyB,CAAC;IAC/C;IACA,SAAS;QACL,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,QAAQ;QACR,UAAU,oMAAgB;IAC9B;IACA,cAAc;QACV,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,iCAAiC,CAAC;IACvD;IACA,oBAAoB;QAChB,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,6MAAyB;IACvC;IACA,cAAc;QACV,aAAa;QACb,UAAU;QACV,SAAS;QACT,gBAAgB,CAAC,wBAAwB,CAAC;IAC9C;IACA,WAAW;QACP,aAAa;QACb,UAAU;QACV,SAAS;IACb;IACA,UAAU;QACN,aAAa;QACb,UAAU;QACV,SAAS;QACT,gBAAgB,CAAC,2BAA2B,CAAC;IACjD;IACA,oBAAoB;QAChB,aAAa;QACb,UAAU;QACV,SAAS;QACT,gBAAgB,CAAC,qJAAqJ,CAAC;IAC3K;IACA,aAAa;QACT,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,UAAU,sMAAkB;IAChC;IACA,aAAa;QACT,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,2BAA2B,CAAC;QAC7C,UAAU,sMAAkB;IAChC;IACA,WAAW;QACP,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,sMAAkB;QAC5B,QAAQ;IACZ;IACA,iBAAiB;QACb,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,4BAA4B,CAAC;QAC9C,UAAU,0MAAsB;IACpC;IACA,QAAQ;QACJ,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,mBAAmB,CAAC;IACzC;IACA,QAAQ;QACJ,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,qBAAqB,CAAC;IAC3C;IACA,OAAO;QACH,aAAa;QACb,UAAU;QACV,SAAS;QACT,gBAAgB,CAAC,4CAA4C,CAAC;IAClE;IACA,aAAa;QACT,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,qMAAiB;QAC3B,QAAQ;QACR,gBAAgB,CAAC,qBAAqB,CAAC;IAC3C;IACA,KAAK;QACD,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,gMAAY;QACtB,QAAQ;IACZ;IACA,aAAa;QACT,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,kMAAc;QACxB,QAAQ;QACR,gBAAgB,CAAC,wBAAwB,CAAC;IAC9C;IACA,WAAW;QACP,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,2BAA2B,CAAC;IACjD;IACA,WAAW;QACP,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,sMAAkB;QAC5B,QAAQ;IACZ;IACA,OAAO;QACH,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,mDAAmD,CAAC;IACzE;IACA,UAAU;QACN,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,oBAAoB,CAAC;IAC1C;IACA,MAAM;QACF,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,iMAAa;QACvB,QAAQ;QACR,gBAAgB,CAAC,0EAA0E,CAAC;IAChG;IACA,cAAc;QACV,aAAa;QACb,UAAU;QACV,SAAS;QACT,gBAAgB,CAAC,4BAA4B,CAAC;IAClD;IACA,WAAW;QACP,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,sMAAkB;QAC5B,QAAQ;QACR,gBAAgB,CAAC;;;8BAGK,CAAC;IAC3B;IACA,aAAa;QACT,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,gBAAgB,CAAC,6BAA6B,CAAC;QAC/C,QAAQ;IACZ;IACA,aAAa;QACT,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,+BAA+B,CAAC;IACrD;IACA,SAAS;QACL,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,oMAAgB;QAC1B,QAAQ;IACZ;IACA,WAAW;QACP,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,sMAAkB;QAC5B,QAAQ;QACR,gBAAgB,CAAC,wBAAwB,CAAC;IAC9C;IACA,WAAW;QACP,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,sMAAkB;QAC5B,QAAQ;IACZ;IACA,MAAM;QACF,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,iMAAa;QACvB,QAAQ;QACR,gBAAgB,CAAC,0BAA0B,CAAC;IAChD;IACA,sBAAsB;QAClB,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,UAAU,+MAA2B;QACrC,gBAAgB,CAAC,mBAAmB,CAAC;IACzC;IACA,eAAe;QACX,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,wMAAoB;QAC9B,QAAQ;QACR,gBAAgB,CAAC,oBAAoB,CAAC;IAC1C;IACA,YAAY;QACR,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,kMAAc;QACxB,SAAS;IACb;IACA,MAAM;QACF,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,iMAAa;QACvB,QAAQ;IACZ;IACA,kBAAkB;QACd,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,2MAAuB;QACjC,QAAQ;IACZ;IACA,WAAW;QACP,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,QAAQ;IACZ;IACA,QAAQ;QACJ,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,mMAAe;QACzB,QAAQ;IACZ;IACA,gBAAgB;QACZ,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,sBAAsB,CAAC;IAC5C;IACA,OAAO;QACH,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,kMAAc;QACxB,QAAQ;IACZ;IACA,UAAU;QACN,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,wBAAwB,CAAC;IAC9C;IACA,cAAc;QACV,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,yMAAqB;QAC/B,QAAQ;IACZ;IACA,QAAQ;QACJ,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;IACZ;IACA,OAAO;QACH,aAAa;QACb,UAAU;QACV,SAAS;QACT,gBAAgB,CAAC,sBAAsB,CAAC;IAC5C;IACA,SAAS;QACL,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,oMAAgB;QAC1B,QAAQ;IACZ;IACA,MAAM;QACF,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,UAAU,iMAAa;QACvB,gBAAgB,CAAC,mBAAmB,CAAC;IACzC;IACA,kBAAkB;QACd,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,0MAAsB;QAChC,QAAQ;QACR,gBAAgB,CAAC,eAAe,CAAC;IACrC;IACA,SAAS;QACL,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,qEAAqE,CAAC;IAC3F;IACA,QAAQ;QACJ,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,oBAAoB,CAAC;IAC1C;IACA,gBAAgB;QACZ,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,mBAAmB,CAAC;IACzC;IACA,yBAAyB;QACrB,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,iNAA6B;QACvC,QAAQ;IACZ;IACA,QAAQ;QACJ,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,mMAAe;QACzB,QAAQ;IACZ;IACA,SAAS;QACL,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,oMAAgB;QAC1B,QAAQ;QACR,gBAAgB,CAAC,2BAA2B,CAAC;IACjD;IACA,OAAO;QACH,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,kMAAc;QACxB,QAAQ;QACR,gBAAgB,CAAC,oBAAoB,CAAC;IAC1C;IACA,eAAe;QACX,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,wMAAoB;QAC9B,QAAQ;IACZ;IACA,aAAa;QACT,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,wMAAoB;QAC9B,QAAQ;QACR,gBAAgB,CAAC,uBAAuB,CAAC;IAC7C;IACA,cAAc;QACV,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,qBAAqB,CAAC;IAC3C;IACA,sBAAsB;QAClB,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,wBAAwB,CAAC;QAC1C,UAAU,+MAA2B;IACzC;IACA,WAAW;QACP,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,8BAA8B,CAAC;IACpD;IACA,yBAAyB;QACrB,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,4BAA4B,CAAC;IAClD;IACA,cAAc;QACV,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,4CAA4C,CAAC;QAC9D,UAAU,uMAAmB;IACjC;IACA,WAAW;QACP,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,sBAAsB,CAAC;IAC5C;IACA,WAAW;QACP,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,mBAAmB,CAAC;IACzC;IACA,qBAAqB;QACjB,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,6MAAyB;QACnC,QAAQ;QACR,gBAAgB,CAAC,oBAAoB,CAAC;IAC1C;IACA,QAAQ;QACJ,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,mMAAe;QACzB,QAAQ;QACR,gBAAgB,CAAC,yBAAyB,CAAC;IAC/C;IACA,aAAa;QACT,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,wMAAoB;QAC9B,QAAQ;IACZ;IACA,UAAU;QACN,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,mDAAmD,CAAC;IACzE;IACA,OAAO;QACH,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,qBAAqB,CAAC;IAC3C;IACA,2BAA2B;QACvB,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,wCAAwC,CAAC;IAC9D;IACA,eAAe;QACX,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,0MAAsB;IACpC;IACA,QAAQ;QACJ,aAAa;QACb,UAAU;QACV,SAAS;IACb;IACA,YAAY;QACR,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,4CAA4C,CAAC;QAC9D,UAAU,uMAAmB;IACjC;IACA,YAAY;QACR,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,kDAAkD,CAAC;IACxE;IACA,SAAS;QACL,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,yIAAyI,CAAC;IAC/J;IACA,MAAM;QACF,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,iMAAa;QACvB,QAAQ;QACR,gBAAgB,CAAC,oDAAoD,CAAC;IAC1E;IACA,OAAO;QACH,aAAa;QACb,UAAU;QACV,SAAS;QACT,gBAAgB,CAAC,qBAAqB,CAAC;IAC3C;IACA,UAAU;QACN,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,2CAA2C,CAAC;IACjE;IACA,cAAc;QACV,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,yMAAqB;QAC/B,QAAQ;IACZ;IACA,mBAAmB;QACf,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,2MAAuB;QACjC,QAAQ;IACZ;IACA,SAAS;QACL,aAAa;QACb,UAAU;QACV,SAAS;QACT,gBAAgB,CAAC,4BAA4B,CAAC;IAClD;IACA,aAAa;QACT,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,mBAAmB,CAAC;QACrC,UAAU,wMAAoB;IAClC;IACA,OAAO;QACH,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,kMAAc;QACxB,QAAQ;QACR,gBAAgB,CAAC,kBAAkB,CAAC;IACxC;IACA,YAAY;QACR,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,gBAAgB,CAAC,mBAAmB,CAAC;IACzC;IACA,gBAAgB;QACZ,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,mMAAe;QACzB,QAAQ;QACR,gBAAgB,CAAC,uBAAuB,CAAC;IAC7C;IACA,MAAM;QACF,aAAa;QACb,UAAU;QACV,SAAS;QACT,gBAAgB,CAAC,oBAAoB,CAAC;QACtC,UAAU,iMAAa;IAC3B;IACA,YAAY;QACR,aAAa;QACb,UAAU;QACV,SAAS;QACT,gBAAgB,CAAC,oBAAoB,CAAC;QACtC,UAAU,uMAAmB;IACjC;IACA,aAAa;QACT,aAAa;QACb,UAAU;QACV,SAAS;QACT,gBAAgB,CAAC,oBAAoB,CAAC;QACtC,UAAU,qMAAiB;IAC/B;IACA,QAAQ;QACJ,aAAa;QACb,UAAU;QACV,SAAS;QACT,gBAAgB,CAAC,4CAA4C,CAAC;QAC9D,UAAU,mMAAe;IAC7B;IACA,YAAY;QACR,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,uMAAmB;IACjC;IACA,QAAQ;QACJ,aAAa;QACb,UAAU;QACV,SAAS;QACT,UAAU,mMAAe;QACzB,QAAQ;IACZ;IACA,KAAK;QACD,aAAa;QACb,UAAU;QACV,SAAS;QACT,gBAAgB,CAAC,mBAAmB,CAAC;QACrC,UAAU,gMAAY;IAC1B;IACA,UAAU;QACN,aAAa;QACb,UAAU;QACV,SAAS;QACT,gBAAgB,CAAC,gDAAgD,CAAC;IACtE;IACA,MAAM;QACF,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,gBAAgB,CAAC,qBAAqB,CAAC;IAC3C;IACA,YAAY;QACR,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,uMAAmB;QAC7B,gBAAgB,CAAC,yEAAyE,CAAC;IAC/F;IACA,SAAS;QACL,6GAA6G;QAC7G,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,gBAAgB,CAAC,mDAAmD,CAAC;QACrE,UAAU,wMAAoB;IAClC;IACA,OAAO;QACH,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;QACT,UAAU,kMAAc;QACxB,QAAQ;IACZ;IACA,cAAc;QACV,aAAa;QACb,UAAU;QACV,SAAS;QACT,QAAQ;QACR,gBAAgB,CAAC,wBAAwB,CAAC;QAC1C,UAAU,2MAAuB;IACrC;AACJ;AACO,MAAM,yBAAyB,OAAO,IAAI,CAAC;AAC3C,MAAM,iCAAiC,OAAO,OAAO,CAAC,4BACzD,6DAA6D;CAC5D,MAAM,CAAC,CAAC,CAAC,GAAG,EAAE,GAAK,EAAE,MAAM,EAC3B,GAAG,CAAC,CAAC,CAAC,EAAE,GAAK","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 9672, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/tokenizer-data.js"],"sourcesContent":["export const SPECIAL_TOKENS_ATTRIBUTES = [\n    \"bos_token\",\n    \"eos_token\",\n    \"unk_token\",\n    \"sep_token\",\n    \"pad_token\",\n    \"cls_token\",\n    \"mask_token\",\n    // additional_special_tokens (TODO)\n];\n"],"names":[],"mappings":";;;;AAAO,MAAM,4BAA4B;IACrC;IACA;IACA;IACA;IACA;IACA;IACA;CAEH","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 9689, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/gguf.js"],"sourcesContent":["// This list is copied from gguf/types.ts, but will all types available (for backward compatibility)\n// NOT to be confused with GGMLQuantizationType, a FileQuantization can contain multiple GGMLQuantizationType\n// For example, Q4_K_M model can contains Q4_K and Q6_K tensors\nexport var GGMLFileQuantizationType;\n(function (GGMLFileQuantizationType) {\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"F32\"] = 0] = \"F32\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"F16\"] = 1] = \"F16\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_0\"] = 2] = \"Q4_0\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_1\"] = 3] = \"Q4_1\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_1_SOME_F16\"] = 4] = \"Q4_1_SOME_F16\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_2\"] = 5] = \"Q4_2\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_3\"] = 6] = \"Q4_3\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q8_0\"] = 7] = \"Q8_0\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q5_0\"] = 8] = \"Q5_0\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q5_1\"] = 9] = \"Q5_1\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q2_K\"] = 10] = \"Q2_K\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q3_K_S\"] = 11] = \"Q3_K_S\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q3_K_M\"] = 12] = \"Q3_K_M\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q3_K_L\"] = 13] = \"Q3_K_L\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_K_S\"] = 14] = \"Q4_K_S\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_K_M\"] = 15] = \"Q4_K_M\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q5_K_S\"] = 16] = \"Q5_K_S\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q5_K_M\"] = 17] = \"Q5_K_M\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q6_K\"] = 18] = \"Q6_K\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ2_XXS\"] = 19] = \"IQ2_XXS\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ2_XS\"] = 20] = \"IQ2_XS\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q2_K_S\"] = 21] = \"Q2_K_S\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ3_XS\"] = 22] = \"IQ3_XS\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ3_XXS\"] = 23] = \"IQ3_XXS\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ1_S\"] = 24] = \"IQ1_S\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ4_NL\"] = 25] = \"IQ4_NL\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ3_S\"] = 26] = \"IQ3_S\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ3_M\"] = 27] = \"IQ3_M\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ2_S\"] = 28] = \"IQ2_S\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ2_M\"] = 29] = \"IQ2_M\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ4_XS\"] = 30] = \"IQ4_XS\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ1_M\"] = 31] = \"IQ1_M\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"BF16\"] = 32] = \"BF16\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_0_4_4\"] = 33] = \"Q4_0_4_4\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_0_4_8\"] = 34] = \"Q4_0_4_8\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_0_8_8\"] = 35] = \"Q4_0_8_8\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"TQ1_0\"] = 36] = \"TQ1_0\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"TQ2_0\"] = 37] = \"TQ2_0\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"MXFP4_MOE\"] = 38] = \"MXFP4_MOE\";\n    // custom quants used by unsloth\n    // they are not officially a scheme enum value in GGUF, but only here for naming\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q2_K_XL\"] = 1000] = \"Q2_K_XL\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q3_K_XL\"] = 1001] = \"Q3_K_XL\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_K_XL\"] = 1002] = \"Q4_K_XL\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q5_K_XL\"] = 1003] = \"Q5_K_XL\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q6_K_XL\"] = 1004] = \"Q6_K_XL\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q8_K_XL\"] = 1005] = \"Q8_K_XL\";\n})(GGMLFileQuantizationType || (GGMLFileQuantizationType = {}));\nconst ggufQuants = Object.values(GGMLFileQuantizationType).filter((v) => typeof v === \"string\");\nexport const GGUF_QUANT_RE = new RegExp(`(?<quant>${ggufQuants.join(\"|\")})` + \"(_(?<sizeVariation>[A-Z]+))?\");\nexport const GGUF_QUANT_RE_GLOBAL = new RegExp(GGUF_QUANT_RE, \"g\");\nexport function parseGGUFQuantLabel(fname) {\n    const quantLabel = fname.toUpperCase().match(GGUF_QUANT_RE_GLOBAL)?.at(-1); // if there is multiple quant substrings in a name, we prefer the last one\n    return quantLabel;\n}\n// order of quantization, from biggest to smallest\n// this list must be in sync with the order in GGMLFileQuantizationType\n// the gguf.spec.ts tests are using verify if the order is correct\nexport const GGUF_QUANT_ORDER = [\n    GGMLFileQuantizationType.F32,\n    GGMLFileQuantizationType.BF16,\n    GGMLFileQuantizationType.F16,\n    GGMLFileQuantizationType.Q8_K_XL,\n    GGMLFileQuantizationType.Q8_0,\n    // 6-bit quantizations\n    GGMLFileQuantizationType.Q6_K_XL,\n    GGMLFileQuantizationType.Q6_K,\n    // 5-bit quantizations\n    GGMLFileQuantizationType.Q5_K_XL,\n    GGMLFileQuantizationType.Q5_K_M,\n    GGMLFileQuantizationType.Q5_K_S,\n    GGMLFileQuantizationType.Q5_0,\n    GGMLFileQuantizationType.Q5_1,\n    // 4-bit quantizations\n    GGMLFileQuantizationType.Q4_K_XL,\n    GGMLFileQuantizationType.Q4_K_M,\n    GGMLFileQuantizationType.Q4_K_S,\n    GGMLFileQuantizationType.IQ4_NL,\n    GGMLFileQuantizationType.IQ4_XS,\n    GGMLFileQuantizationType.Q4_0_4_4,\n    GGMLFileQuantizationType.Q4_0_4_8,\n    GGMLFileQuantizationType.Q4_0_8_8,\n    GGMLFileQuantizationType.Q4_1_SOME_F16,\n    GGMLFileQuantizationType.Q4_0,\n    GGMLFileQuantizationType.Q4_1,\n    GGMLFileQuantizationType.Q4_2,\n    GGMLFileQuantizationType.Q4_3,\n    GGMLFileQuantizationType.MXFP4_MOE,\n    // 3-bit quantizations\n    GGMLFileQuantizationType.Q3_K_XL,\n    GGMLFileQuantizationType.Q3_K_L,\n    GGMLFileQuantizationType.Q3_K_M,\n    GGMLFileQuantizationType.Q3_K_S,\n    GGMLFileQuantizationType.IQ3_M,\n    GGMLFileQuantizationType.IQ3_S,\n    GGMLFileQuantizationType.IQ3_XS,\n    GGMLFileQuantizationType.IQ3_XXS,\n    // 2-bit quantizations\n    GGMLFileQuantizationType.Q2_K_XL,\n    GGMLFileQuantizationType.Q2_K,\n    GGMLFileQuantizationType.Q2_K_S,\n    GGMLFileQuantizationType.IQ2_M,\n    GGMLFileQuantizationType.IQ2_S,\n    GGMLFileQuantizationType.IQ2_XS,\n    GGMLFileQuantizationType.IQ2_XXS,\n    // 1-bit quantizations\n    GGMLFileQuantizationType.IQ1_S,\n    GGMLFileQuantizationType.IQ1_M,\n    GGMLFileQuantizationType.TQ1_0,\n    GGMLFileQuantizationType.TQ2_0,\n];\n// This function finds the nearest quantization type that is less than or equal to the given quantization type.\n// It returns undefined if no such quantization type is found.\nexport function findNearestQuantType(quant, availableQuants) {\n    // Create a map for quick index lookup from the defined order\n    const orderMap = new Map();\n    GGUF_QUANT_ORDER.forEach((q, index) => {\n        orderMap.set(q, index);\n    });\n    const targetIndex = orderMap.get(quant) ?? 0; // the 0 case should never happen\n    // Filter the available quantizations to include only those defined in the order map,\n    // then sort them according to the GGUF_QUANT_ORDER (from largest/index 0 to smallest/highest index).\n    const sortedAvailable = availableQuants\n        .filter((q) => orderMap.has(q))\n        .sort((a, b) => (orderMap.get(a) ?? Infinity) - (orderMap.get(b) ?? Infinity));\n    // If no valid quantizations are available after filtering\n    if (sortedAvailable.length === 0) {\n        return undefined;\n    }\n    // Iterate through the sorted available quantizations (largest to smallest).\n    // Find the first one whose order index is >= the target index.\n    // This means finding the largest quantization that is smaller than or equal to the target.\n    for (const availableQuant of sortedAvailable) {\n        // We know the key exists due to the filter above.\n        const availableIndex = orderMap.get(availableQuant) ?? 0;\n        if (availableIndex >= targetIndex) {\n            return availableQuant;\n        }\n    }\n    // If the loop completes, it means all available quantizations are larger (have a smaller index)\n    // than the target quantization. In this case, return the \"smallest\" available quantization,\n    // which is the last element in the sorted list (highest index among available).\n    return sortedAvailable[sortedAvailable.length - 1];\n}\n// This list is only used to calculate the size of the model, NOT to be confused with the quantization FILE type\nexport var GGMLQuantizationType;\n(function (GGMLQuantizationType) {\n    GGMLQuantizationType[GGMLQuantizationType[\"F32\"] = 0] = \"F32\";\n    GGMLQuantizationType[GGMLQuantizationType[\"F16\"] = 1] = \"F16\";\n    GGMLQuantizationType[GGMLQuantizationType[\"Q4_0\"] = 2] = \"Q4_0\";\n    GGMLQuantizationType[GGMLQuantizationType[\"Q4_1\"] = 3] = \"Q4_1\";\n    GGMLQuantizationType[GGMLQuantizationType[\"Q5_0\"] = 6] = \"Q5_0\";\n    GGMLQuantizationType[GGMLQuantizationType[\"Q5_1\"] = 7] = \"Q5_1\";\n    GGMLQuantizationType[GGMLQuantizationType[\"Q8_0\"] = 8] = \"Q8_0\";\n    GGMLQuantizationType[GGMLQuantizationType[\"Q8_1\"] = 9] = \"Q8_1\";\n    GGMLQuantizationType[GGMLQuantizationType[\"Q2_K\"] = 10] = \"Q2_K\";\n    GGMLQuantizationType[GGMLQuantizationType[\"Q3_K\"] = 11] = \"Q3_K\";\n    GGMLQuantizationType[GGMLQuantizationType[\"Q4_K\"] = 12] = \"Q4_K\";\n    GGMLQuantizationType[GGMLQuantizationType[\"Q5_K\"] = 13] = \"Q5_K\";\n    GGMLQuantizationType[GGMLQuantizationType[\"Q6_K\"] = 14] = \"Q6_K\";\n    GGMLQuantizationType[GGMLQuantizationType[\"Q8_K\"] = 15] = \"Q8_K\";\n    GGMLQuantizationType[GGMLQuantizationType[\"IQ2_XXS\"] = 16] = \"IQ2_XXS\";\n    GGMLQuantizationType[GGMLQuantizationType[\"IQ2_XS\"] = 17] = \"IQ2_XS\";\n    GGMLQuantizationType[GGMLQuantizationType[\"IQ3_XXS\"] = 18] = \"IQ3_XXS\";\n    GGMLQuantizationType[GGMLQuantizationType[\"IQ1_S\"] = 19] = \"IQ1_S\";\n    GGMLQuantizationType[GGMLQuantizationType[\"IQ4_NL\"] = 20] = \"IQ4_NL\";\n    GGMLQuantizationType[GGMLQuantizationType[\"IQ3_S\"] = 21] = \"IQ3_S\";\n    GGMLQuantizationType[GGMLQuantizationType[\"IQ2_S\"] = 22] = \"IQ2_S\";\n    GGMLQuantizationType[GGMLQuantizationType[\"IQ4_XS\"] = 23] = \"IQ4_XS\";\n    GGMLQuantizationType[GGMLQuantizationType[\"I8\"] = 24] = \"I8\";\n    GGMLQuantizationType[GGMLQuantizationType[\"I16\"] = 25] = \"I16\";\n    GGMLQuantizationType[GGMLQuantizationType[\"I32\"] = 26] = \"I32\";\n    GGMLQuantizationType[GGMLQuantizationType[\"I64\"] = 27] = \"I64\";\n    GGMLQuantizationType[GGMLQuantizationType[\"F64\"] = 28] = \"F64\";\n    GGMLQuantizationType[GGMLQuantizationType[\"IQ1_M\"] = 29] = \"IQ1_M\";\n    GGMLQuantizationType[GGMLQuantizationType[\"BF16\"] = 30] = \"BF16\";\n    GGMLQuantizationType[GGMLQuantizationType[\"TQ1_0\"] = 34] = \"TQ1_0\";\n    GGMLQuantizationType[GGMLQuantizationType[\"TQ2_0\"] = 35] = \"TQ2_0\";\n    GGMLQuantizationType[GGMLQuantizationType[\"MXFP4\"] = 39] = \"MXFP4\";\n})(GGMLQuantizationType || (GGMLQuantizationType = {}));\n"],"names":[],"mappings":"AAAA,oGAAoG;AACpG,6GAA6G;AAC7G,+DAA+D;;;;;;;;;;;;;;;;;AACxD,IAAI;AACX,CAAC,SAAU,wBAAwB;IAC/B,wBAAwB,CAAC,wBAAwB,CAAC,MAAM,GAAG,EAAE,GAAG;IAChE,wBAAwB,CAAC,wBAAwB,CAAC,MAAM,GAAG,EAAE,GAAG;IAChE,wBAAwB,CAAC,wBAAwB,CAAC,OAAO,GAAG,EAAE,GAAG;IACjE,wBAAwB,CAAC,wBAAwB,CAAC,OAAO,GAAG,EAAE,GAAG;IACjE,wBAAwB,CAAC,wBAAwB,CAAC,gBAAgB,GAAG,EAAE,GAAG;IAC1E,wBAAwB,CAAC,wBAAwB,CAAC,OAAO,GAAG,EAAE,GAAG;IACjE,wBAAwB,CAAC,wBAAwB,CAAC,OAAO,GAAG,EAAE,GAAG;IACjE,wBAAwB,CAAC,wBAAwB,CAAC,OAAO,GAAG,EAAE,GAAG;IACjE,wBAAwB,CAAC,wBAAwB,CAAC,OAAO,GAAG,EAAE,GAAG;IACjE,wBAAwB,CAAC,wBAAwB,CAAC,OAAO,GAAG,EAAE,GAAG;IACjE,wBAAwB,CAAC,wBAAwB,CAAC,OAAO,GAAG,GAAG,GAAG;IAClE,wBAAwB,CAAC,wBAAwB,CAAC,SAAS,GAAG,GAAG,GAAG;IACpE,wBAAwB,CAAC,wBAAwB,CAAC,SAAS,GAAG,GAAG,GAAG;IACpE,wBAAwB,CAAC,wBAAwB,CAAC,SAAS,GAAG,GAAG,GAAG;IACpE,wBAAwB,CAAC,wBAAwB,CAAC,SAAS,GAAG,GAAG,GAAG;IACpE,wBAAwB,CAAC,wBAAwB,CAAC,SAAS,GAAG,GAAG,GAAG;IACpE,wBAAwB,CAAC,wBAAwB,CAAC,SAAS,GAAG,GAAG,GAAG;IACpE,wBAAwB,CAAC,wBAAwB,CAAC,SAAS,GAAG,GAAG,GAAG;IACpE,wBAAwB,CAAC,wBAAwB,CAAC,OAAO,GAAG,GAAG,GAAG;IAClE,wBAAwB,CAAC,wBAAwB,CAAC,UAAU,GAAG,GAAG,GAAG;IACrE,wBAAwB,CAAC,wBAAwB,CAAC,SAAS,GAAG,GAAG,GAAG;IACpE,wBAAwB,CAAC,wBAAwB,CAAC,SAAS,GAAG,GAAG,GAAG;IACpE,wBAAwB,CAAC,wBAAwB,CAAC,SAAS,GAAG,GAAG,GAAG;IACpE,wBAAwB,CAAC,wBAAwB,CAAC,UAAU,GAAG,GAAG,GAAG;IACrE,wBAAwB,CAAC,wBAAwB,CAAC,QAAQ,GAAG,GAAG,GAAG;IACnE,wBAAwB,CAAC,wBAAwB,CAAC,SAAS,GAAG,GAAG,GAAG;IACpE,wBAAwB,CAAC,wBAAwB,CAAC,QAAQ,GAAG,GAAG,GAAG;IACnE,wBAAwB,CAAC,wBAAwB,CAAC,QAAQ,GAAG,GAAG,GAAG;IACnE,wBAAwB,CAAC,wBAAwB,CAAC,QAAQ,GAAG,GAAG,GAAG;IACnE,wBAAwB,CAAC,wBAAwB,CAAC,QAAQ,GAAG,GAAG,GAAG;IACnE,wBAAwB,CAAC,wBAAwB,CAAC,SAAS,GAAG,GAAG,GAAG;IACpE,wBAAwB,CAAC,wBAAwB,CAAC,QAAQ,GAAG,GAAG,GAAG;IACnE,wBAAwB,CAAC,wBAAwB,CAAC,OAAO,GAAG,GAAG,GAAG;IAClE,wBAAwB,CAAC,wBAAwB,CAAC,WAAW,GAAG,GAAG,GAAG;IACtE,wBAAwB,CAAC,wBAAwB,CAAC,WAAW,GAAG,GAAG,GAAG;IACtE,wBAAwB,CAAC,wBAAwB,CAAC,WAAW,GAAG,GAAG,GAAG;IACtE,wBAAwB,CAAC,wBAAwB,CAAC,QAAQ,GAAG,GAAG,GAAG;IACnE,wBAAwB,CAAC,wBAAwB,CAAC,QAAQ,GAAG,GAAG,GAAG;IACnE,wBAAwB,CAAC,wBAAwB,CAAC,YAAY,GAAG,GAAG,GAAG;IACvE,gCAAgC;IAChC,gFAAgF;IAChF,wBAAwB,CAAC,wBAAwB,CAAC,UAAU,GAAG,KAAK,GAAG;IACvE,wBAAwB,CAAC,wBAAwB,CAAC,UAAU,GAAG,KAAK,GAAG;IACvE,wBAAwB,CAAC,wBAAwB,CAAC,UAAU,GAAG,KAAK,GAAG;IACvE,wBAAwB,CAAC,wBAAwB,CAAC,UAAU,GAAG,KAAK,GAAG;IACvE,wBAAwB,CAAC,wBAAwB,CAAC,UAAU,GAAG,KAAK,GAAG;IACvE,wBAAwB,CAAC,wBAAwB,CAAC,UAAU,GAAG,KAAK,GAAG;AAC3E,CAAC,EAAE,4BAA4B,CAAC,2BAA2B,CAAC,CAAC;AAC7D,MAAM,aAAa,OAAO,MAAM,CAAC,0BAA0B,MAAM,CAAC,CAAC,IAAM,OAAO,MAAM;AAC/E,MAAM,gBAAgB,IAAI,OAAO,CAAC,SAAS,EAAE,WAAW,IAAI,CAAC,KAAK,CAAC,CAAC,GAAG;AACvE,MAAM,uBAAuB,IAAI,OAAO,eAAe;AACvD,SAAS,oBAAoB,KAAK;IACrC,MAAM,aAAa,MAAM,WAAW,GAAG,KAAK,CAAC,uBAAuB,GAAG,CAAC,IAAI,0EAA0E;IACtJ,OAAO;AACX;AAIO,MAAM,mBAAmB;IAC5B,yBAAyB,GAAG;IAC5B,yBAAyB,IAAI;IAC7B,yBAAyB,GAAG;IAC5B,yBAAyB,OAAO;IAChC,yBAAyB,IAAI;IAC7B,sBAAsB;IACtB,yBAAyB,OAAO;IAChC,yBAAyB,IAAI;IAC7B,sBAAsB;IACtB,yBAAyB,OAAO;IAChC,yBAAyB,MAAM;IAC/B,yBAAyB,MAAM;IAC/B,yBAAyB,IAAI;IAC7B,yBAAyB,IAAI;IAC7B,sBAAsB;IACtB,yBAAyB,OAAO;IAChC,yBAAyB,MAAM;IAC/B,yBAAyB,MAAM;IAC/B,yBAAyB,MAAM;IAC/B,yBAAyB,MAAM;IAC/B,yBAAyB,QAAQ;IACjC,yBAAyB,QAAQ;IACjC,yBAAyB,QAAQ;IACjC,yBAAyB,aAAa;IACtC,yBAAyB,IAAI;IAC7B,yBAAyB,IAAI;IAC7B,yBAAyB,IAAI;IAC7B,yBAAyB,IAAI;IAC7B,yBAAyB,SAAS;IAClC,sBAAsB;IACtB,yBAAyB,OAAO;IAChC,yBAAyB,MAAM;IAC/B,yBAAyB,MAAM;IAC/B,yBAAyB,MAAM;IAC/B,yBAAyB,KAAK;IAC9B,yBAAyB,KAAK;IAC9B,yBAAyB,MAAM;IAC/B,yBAAyB,OAAO;IAChC,sBAAsB;IACtB,yBAAyB,OAAO;IAChC,yBAAyB,IAAI;IAC7B,yBAAyB,MAAM;IAC/B,yBAAyB,KAAK;IAC9B,yBAAyB,KAAK;IAC9B,yBAAyB,MAAM;IAC/B,yBAAyB,OAAO;IAChC,sBAAsB;IACtB,yBAAyB,KAAK;IAC9B,yBAAyB,KAAK;IAC9B,yBAAyB,KAAK;IAC9B,yBAAyB,KAAK;CACjC;AAGM,SAAS,qBAAqB,KAAK,EAAE,eAAe;IACvD,6DAA6D;IAC7D,MAAM,WAAW,IAAI;IACrB,iBAAiB,OAAO,CAAC,CAAC,GAAG;QACzB,SAAS,GAAG,CAAC,GAAG;IACpB;IACA,MAAM,cAAc,SAAS,GAAG,CAAC,UAAU,GAAG,iCAAiC;IAC/E,qFAAqF;IACrF,qGAAqG;IACrG,MAAM,kBAAkB,gBACnB,MAAM,CAAC,CAAC,IAAM,SAAS,GAAG,CAAC,IAC3B,IAAI,CAAC,CAAC,GAAG,IAAM,CAAC,SAAS,GAAG,CAAC,MAAM,QAAQ,IAAI,CAAC,SAAS,GAAG,CAAC,MAAM,QAAQ;IAChF,0DAA0D;IAC1D,IAAI,gBAAgB,MAAM,KAAK,GAAG;QAC9B,OAAO;IACX;IACA,4EAA4E;IAC5E,+DAA+D;IAC/D,2FAA2F;IAC3F,KAAK,MAAM,kBAAkB,gBAAiB;QAC1C,kDAAkD;QAClD,MAAM,iBAAiB,SAAS,GAAG,CAAC,mBAAmB;QACvD,IAAI,kBAAkB,aAAa;YAC/B,OAAO;QACX;IACJ;IACA,gGAAgG;IAChG,4FAA4F;IAC5F,gFAAgF;IAChF,OAAO,eAAe,CAAC,gBAAgB,MAAM,GAAG,EAAE;AACtD;AAEO,IAAI;AACX,CAAC,SAAU,oBAAoB;IAC3B,oBAAoB,CAAC,oBAAoB,CAAC,MAAM,GAAG,EAAE,GAAG;IACxD,oBAAoB,CAAC,oBAAoB,CAAC,MAAM,GAAG,EAAE,GAAG;IACxD,oBAAoB,CAAC,oBAAoB,CAAC,OAAO,GAAG,EAAE,GAAG;IACzD,oBAAoB,CAAC,oBAAoB,CAAC,OAAO,GAAG,EAAE,GAAG;IACzD,oBAAoB,CAAC,oBAAoB,CAAC,OAAO,GAAG,EAAE,GAAG;IACzD,oBAAoB,CAAC,oBAAoB,CAAC,OAAO,GAAG,EAAE,GAAG;IACzD,oBAAoB,CAAC,oBAAoB,CAAC,OAAO,GAAG,EAAE,GAAG;IACzD,oBAAoB,CAAC,oBAAoB,CAAC,OAAO,GAAG,EAAE,GAAG;IACzD,oBAAoB,CAAC,oBAAoB,CAAC,OAAO,GAAG,GAAG,GAAG;IAC1D,oBAAoB,CAAC,oBAAoB,CAAC,OAAO,GAAG,GAAG,GAAG;IAC1D,oBAAoB,CAAC,oBAAoB,CAAC,OAAO,GAAG,GAAG,GAAG;IAC1D,oBAAoB,CAAC,oBAAoB,CAAC,OAAO,GAAG,GAAG,GAAG;IAC1D,oBAAoB,CAAC,oBAAoB,CAAC,OAAO,GAAG,GAAG,GAAG;IAC1D,oBAAoB,CAAC,oBAAoB,CAAC,OAAO,GAAG,GAAG,GAAG;IAC1D,oBAAoB,CAAC,oBAAoB,CAAC,UAAU,GAAG,GAAG,GAAG;IAC7D,oBAAoB,CAAC,oBAAoB,CAAC,SAAS,GAAG,GAAG,GAAG;IAC5D,oBAAoB,CAAC,oBAAoB,CAAC,UAAU,GAAG,GAAG,GAAG;IAC7D,oBAAoB,CAAC,oBAAoB,CAAC,QAAQ,GAAG,GAAG,GAAG;IAC3D,oBAAoB,CAAC,oBAAoB,CAAC,SAAS,GAAG,GAAG,GAAG;IAC5D,oBAAoB,CAAC,oBAAoB,CAAC,QAAQ,GAAG,GAAG,GAAG;IAC3D,oBAAoB,CAAC,oBAAoB,CAAC,QAAQ,GAAG,GAAG,GAAG;IAC3D,oBAAoB,CAAC,oBAAoB,CAAC,SAAS,GAAG,GAAG,GAAG;IAC5D,oBAAoB,CAAC,oBAAoB,CAAC,KAAK,GAAG,GAAG,GAAG;IACxD,oBAAoB,CAAC,oBAAoB,CAAC,MAAM,GAAG,GAAG,GAAG;IACzD,oBAAoB,CAAC,oBAAoB,CAAC,MAAM,GAAG,GAAG,GAAG;IACzD,oBAAoB,CAAC,oBAAoB,CAAC,MAAM,GAAG,GAAG,GAAG;IACzD,oBAAoB,CAAC,oBAAoB,CAAC,MAAM,GAAG,GAAG,GAAG;IACzD,oBAAoB,CAAC,oBAAoB,CAAC,QAAQ,GAAG,GAAG,GAAG;IAC3D,oBAAoB,CAAC,oBAAoB,CAAC,OAAO,GAAG,GAAG,GAAG;IAC1D,oBAAoB,CAAC,oBAAoB,CAAC,QAAQ,GAAG,GAAG,GAAG;IAC3D,oBAAoB,CAAC,oBAAoB,CAAC,QAAQ,GAAG,GAAG,GAAG;IAC3D,oBAAoB,CAAC,oBAAoB,CAAC,QAAQ,GAAG,GAAG,GAAG;AAC/D,CAAC,EAAE,wBAAwB,CAAC,uBAAuB,CAAC,CAAC","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 9886, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/snippets/types.js"],"sourcesContent":["// Order of the elements in InferenceModal.svelte is determined by this const\nexport const inferenceSnippetLanguages = [\"python\", \"js\", \"sh\"];\n"],"names":[],"mappings":"AAAA,6EAA6E;;;;;AACtE,MAAM,4BAA4B;IAAC;IAAU;IAAM;CAAK","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 9900, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/snippets/index.js"],"sourcesContent":["export * from \"./common.js\";\nexport * from \"./inputs.js\";\nexport * from \"./types.js\";\n"],"names":[],"mappings":";AAAA;AACA;AACA","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 9911, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/hardware.js"],"sourcesContent":["/**\n * Biden AI Executive Order (since revoked by President Trump):\n * https://web.archive.org/web/20250105222429/https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/\n */\nexport const TFLOPS_THRESHOLD_WHITE_HOUSE_MODEL_TRAINING_TOTAL = 10 ** 14;\nexport const TFLOPS_THRESHOLD_WHITE_HOUSE_MODEL_TRAINING_TOTAL_BIOLOGY = 10 ** 11;\nexport const TFLOPS_THRESHOLD_WHITE_HOUSE_CLUSTER = 10 ** 8;\n/**\n * EU AI Act\n * https://ec.europa.eu/commission/presscorner/detail/en/qanda_21_1683\n */\nexport const TFLOPS_THRESHOLD_EU_AI_ACT_MODEL_TRAINING_TOTAL = 10 ** 13;\nexport const DEFAULT_MEMORY_OPTIONS = [\n    8, 16, 24, 32, 40, 48, 64, 80, 96, 128, 192, 256, 384, 512, 768, 1024, 1536, 2048,\n];\nexport const SKUS = {\n    GPU: {\n        NVIDIA: {\n            B200: {\n                tflops: 496.6,\n                memory: [192],\n            },\n            H200: {\n                tflops: 241.3,\n                memory: [141],\n            },\n            H100: {\n                tflops: 267.6,\n                memory: [80],\n            },\n            L40s: {\n                tflops: 91.61,\n                memory: [48],\n            },\n            L40: {\n                tflops: 90.52,\n                memory: [48],\n            },\n            L20: {\n                tflops: 59.35,\n                memory: [48],\n            },\n            L4: {\n                tflops: 30.29,\n                memory: [24],\n            },\n            \"RTX PRO 6000 WS\": {\n                tflops: 126,\n                memory: [96],\n            },\n            \"RTX PRO 6000 Max-Q\": {\n                tflops: 116,\n                memory: [96],\n            },\n            \"RTX 6000 Ada\": {\n                tflops: 91.1,\n                memory: [48],\n            },\n            \"RTX 5880 Ada\": {\n                tflops: 69.3,\n                memory: [48],\n            },\n            \"RTX 5000 Ada\": {\n                tflops: 65.3,\n                memory: [32],\n            },\n            \"RTX 4500 Ada\": {\n                tflops: 39.6,\n                memory: [24],\n            },\n            \"RTX 4000 Ada\": {\n                tflops: 26.7,\n                memory: [20],\n            },\n            \"RTX 4000 SFF Ada\": {\n                tflops: 19.2,\n                memory: [20],\n            },\n            \"RTX 2000 Ada\": {\n                tflops: 12.0,\n                memory: [16],\n            },\n            \"RTX A6000\": {\n                tflops: 38.7,\n                memory: [48],\n            },\n            \"RTX A5000\": {\n                tflops: 34.1,\n                memory: [24],\n            },\n            \"RTX A4000\": {\n                tflops: 19.2,\n                memory: [16],\n            },\n            \"RTX A2000\": {\n                tflops: 7.987,\n                memory: [8, 12],\n            },\n            A100: {\n                tflops: 77.97,\n                memory: [80, 40],\n            },\n            A40: {\n                tflops: 37.42,\n                memory: [48],\n            },\n            A10: {\n                tflops: 31.24,\n                memory: [24],\n            },\n            A2: {\n                tflops: 4.531, // source: https://www.techpowerup.com/gpu-specs/a2.c3848\n                memory: [16],\n            },\n            \"RTX 5090\": {\n                tflops: 104.8,\n                memory: [32],\n            },\n            \"RTX 5090 D\": {\n                tflops: 104.8,\n                memory: [32],\n            },\n            \"RTX 5080\": {\n                tflops: 56.28,\n                memory: [16],\n            },\n            \"RTX 5080 Mobile\": {\n                tflops: 24.58,\n                memory: [16],\n            },\n            \"RTX 5070\": {\n                tflops: 30.84,\n                memory: [12],\n            },\n            \"RTX 5070 Mobile\": {\n                tflops: 23.22,\n                memory: [8],\n            },\n            \"RTX 5070 Ti\": {\n                tflops: 43.94,\n                memory: [16],\n            },\n            \"RTX 5060 Ti\": {\n                tflops: 23.7, // source https://www.techpowerup.com/gpu-specs/geforce-rtx-5060-ti.c4246\n                memory: [16, 8],\n            },\n            \"RTX 5060\": {\n                tflops: 19.18, // source https://www.techpowerup.com/gpu-specs/geforce-rtx-5060.c4219\n                memory: [8],\n            },\n            \"RTX 4090\": {\n                tflops: 82.58,\n                memory: [24],\n            },\n            \"RTX 4090D\": {\n                tflops: 79.49,\n                memory: [24],\n            },\n            \"RTX 4090 Mobile\": {\n                tflops: 32.98,\n                memory: [16],\n            },\n            \"RTX 4080 SUPER\": {\n                tflops: 52.2,\n                memory: [16],\n            },\n            \"RTX 4080\": {\n                tflops: 48.7,\n                memory: [16],\n            },\n            \"RTX 4080 Mobile\": {\n                tflops: 24.72,\n                memory: [12],\n            },\n            \"RTX 4070\": {\n                tflops: 29.15,\n                memory: [12],\n            },\n            \"RTX 4070 Mobile\": {\n                tflops: 15.62,\n                memory: [8],\n            },\n            \"RTX 4070 Ti\": {\n                tflops: 40.09,\n                memory: [12],\n            },\n            \"RTX 4070 Super\": {\n                tflops: 35.48,\n                memory: [12],\n            },\n            \"RTX 4070 Ti Super\": {\n                tflops: 44.1,\n                memory: [16],\n            },\n            \"RTX 4060\": {\n                tflops: 15.11,\n                memory: [8],\n            },\n            \"RTX 4060 Ti\": {\n                tflops: 22.06,\n                memory: [8, 16],\n            },\n            \"RTX 4090 Laptop\": {\n                tflops: 32.98,\n                memory: [16],\n            },\n            \"RTX 4080 Laptop\": {\n                tflops: 24.72,\n                memory: [12],\n            },\n            \"RTX 4070 Laptop\": {\n                tflops: 15.62,\n                memory: [8],\n            },\n            \"RTX 4060 Laptop\": {\n                tflops: 11.61,\n                memory: [8],\n            },\n            \"RTX 4050 Laptop\": {\n                tflops: 8.9,\n                memory: [6],\n            },\n            \"RTX 3090\": {\n                tflops: 35.58,\n                memory: [24],\n            },\n            \"RTX 3090 Ti\": {\n                tflops: 40,\n                memory: [24],\n            },\n            \"RTX 3080\": {\n                tflops: 30.6,\n                memory: [12, 10],\n            },\n            \"RTX 3080 Ti\": {\n                tflops: 34.1,\n                memory: [12],\n            },\n            \"RTX 3080 Mobile\": {\n                tflops: 18.98,\n                memory: [8],\n            },\n            \"RTX 3070\": {\n                tflops: 20.31,\n                memory: [8],\n            },\n            \"RTX 3070 Ti\": {\n                tflops: 21.75,\n                memory: [8],\n            },\n            \"RTX 3070 Ti Mobile\": {\n                tflops: 16.6,\n                memory: [8],\n            },\n            \"RTX 3060 Ti\": {\n                tflops: 16.2,\n                memory: [8],\n            },\n            \"RTX 3060\": {\n                tflops: 12.74,\n                memory: [12, 8],\n            },\n            \"RTX 2080 Ti\": {\n                tflops: 26.9,\n                memory: [11, 22], // 22GB: modded 2080ti\n            },\n            \"RTX 2080\": {\n                tflops: 20.14,\n                memory: [8],\n            },\n            \"RTX 2070\": {\n                tflops: 14.93,\n                memory: [8],\n            },\n            \"RTX 2070 SUPER Mobile\": {\n                tflops: 14.13,\n                memory: [8],\n            },\n            \"RTX 2070 SUPER\": {\n                tflops: 18.12,\n                memory: [8],\n            },\n            \"RTX 3060 Mobile\": {\n                tflops: 10.94,\n                memory: [6],\n            },\n            \"RTX 3050 Mobile\": {\n                tflops: 7.639,\n                memory: [6],\n            },\n            \"RTX 2060\": {\n                tflops: 12.9,\n                memory: [6],\n            },\n            \"RTX 2060 12GB\": {\n                tflops: 14.36,\n                memory: [12],\n            },\n            \"RTX 2060 Mobile\": {\n                tflops: 9.22,\n                memory: [6],\n            },\n            \"GTX 1080 Ti\": {\n                tflops: 11.34, // float32 (GPU does not support native float16)\n                memory: [11],\n            },\n            \"GTX 1070 Ti\": {\n                tflops: 8.2, // float32 (GPU does not support native float16)\n                memory: [8],\n            },\n            \"GTX 1060\": {\n                tflops: 3.9, // float32 (GPU does not support native float16)\n                memory: [3, 6],\n            },\n            \"GTX 1050 Ti\": {\n                tflops: 2.1, // float32 (GPU does not support native float16)\n                memory: [4],\n            },\n            \"RTX Titan\": {\n                tflops: 32.62,\n                memory: [24],\n            },\n            \"GTX 1660\": {\n                tflops: 10.05,\n                memory: [6],\n            },\n            \"GTX 1650 Mobile\": {\n                tflops: 6.39,\n                memory: [4],\n            },\n            T4: {\n                tflops: 65.13,\n                memory: [16],\n            },\n            T10: {\n                tflops: 20.0,\n                memory: [16],\n            },\n            V100: {\n                tflops: 28.26,\n                memory: [32, 16],\n            },\n            \"Quadro P6000\": {\n                tflops: 12.63, // float32 (GPU does not support native float16)\n                memory: [24],\n            },\n            P40: {\n                tflops: 11.76, // float32 (GPU does not support native float16)\n                memory: [24],\n            },\n            P100: {\n                tflops: 19.05,\n                memory: [16],\n            },\n            \"Jetson AGX Orin 64GB\": {\n                tflops: 10.65,\n                memory: [64],\n            },\n            \"Jetson AGX Orin 32GB\": {\n                tflops: 6.66,\n                memory: [32],\n            },\n            \"Jetson Orin NX 16GB\": {\n                tflops: 3.76,\n                memory: [16],\n            },\n            \"Jetson Orin NX 8GB\": {\n                tflops: 3.13,\n                memory: [8],\n            },\n            \"Jetson Orin Nano 8GB\": {\n                tflops: 2.56,\n                memory: [8],\n            },\n            \"Jetson Orin Nano 4GB\": {\n                tflops: 1.28,\n                memory: [4],\n            },\n            \"Jetson AGX Xavier\": {\n                tflops: 2.82,\n                memory: [32, 64],\n            },\n            \"Jetson Xavier NX\": {\n                tflops: 1.69,\n                memory: [8, 16],\n            },\n            \"Jetson TX2\": {\n                tflops: 1.33,\n                memory: [4, 8],\n            },\n            \"Jetson Nano\": {\n                tflops: 0.47,\n                memory: [4],\n            },\n        },\n        AMD: {\n            MI300: {\n                tflops: 383.0,\n                memory: [192],\n            },\n            MI250: {\n                tflops: 362.1,\n                memory: [128],\n            },\n            MI210: {\n                tflops: 181.0,\n                memory: [64],\n            },\n            MI100: {\n                tflops: 184.6,\n                memory: [32],\n            },\n            MI60: {\n                tflops: 29.5,\n                memory: [32],\n            },\n            MI50: {\n                tflops: 26.5,\n                memory: [16],\n            },\n            \"RX 9070 XT\": {\n                tflops: 97.32,\n                memory: [16],\n            },\n            \"RX 9070\": {\n                tflops: 72.25,\n                memory: [16],\n            },\n            \"RX 7900 XTX\": {\n                tflops: 122.8,\n                memory: [24],\n            },\n            \"RX 7900 XT\": {\n                tflops: 103.0,\n                memory: [20],\n            },\n            \"RX 7900 GRE\": {\n                tflops: 91.96,\n                memory: [16],\n            },\n            \"RX 7800 XT\": {\n                tflops: 74.65,\n                memory: [16],\n            },\n            \"RX 7700 XT\": {\n                tflops: 70.34,\n                memory: [12],\n            },\n            \"RX 7600 XT\": {\n                tflops: 45.14,\n                memory: [16, 8],\n            },\n            \"RX 6950 XT\": {\n                tflops: 47.31,\n                memory: [16],\n            },\n            \"RX 6800\": {\n                tflops: 32.33,\n                memory: [16],\n            },\n            \"RX 6700 XT\": {\n                tflops: 26.43,\n                memory: [12],\n            },\n            \"RX 6700\": {\n                tflops: 22.58,\n                memory: [10],\n            },\n            \"RX 6650 XT\": {\n                tflops: 21.59,\n                memory: [8],\n            },\n            \"RX 6600 XT\": {\n                tflops: 21.21,\n                memory: [8],\n            },\n            \"RX 6600\": {\n                tflops: 17.86,\n                memory: [8],\n            },\n            \"Radeon Pro VII\": {\n                tflops: 26.11,\n                memory: [16],\n            },\n        },\n        INTEL: {\n            \"Arc A750\": {\n                tflops: 34.41,\n                memory: [8],\n            },\n            \"Arc A770\": {\n                tflops: 39.32,\n                memory: [8, 16],\n            },\n            \"Arc B570\": {\n                tflops: 23.04,\n                memory: [10],\n            },\n            \"Arc B580\": {\n                tflops: 27.34,\n                memory: [12],\n            },\n        },\n        QUALCOMM: {\n            \"Snapdragon X Elite X1E-00-1DE\": {\n                tflops: 4.6,\n            },\n            \"Snapdragon X Elite X1E-84-100\": {\n                tflops: 4.6,\n            },\n            \"Snapdragon X Elite X1E-80-100\": {\n                tflops: 3.8,\n            },\n            \"Snapdragon X Elite X1E-78-100\": {\n                tflops: 3.8,\n            },\n            \"Snapdragon X Plus X1P-64-100\": {\n                tflops: 3.8,\n            },\n        },\n    },\n    CPU: {\n        Intel: {\n            \"Xeon 4th Generation (Sapphire Rapids)\": {\n                tflops: 1.3,\n            },\n            \"Xeon 3th Generation (Ice Lake)\": {\n                tflops: 0.8,\n            },\n            \"Xeon 2th Generation (Cascade Lake)\": {\n                tflops: 0.55,\n            },\n            \"Xeon E5v4 (Broadwell)\": {\n                tflops: 0.25,\n            },\n            \"Xeon E5v3 (Haswell)\": {\n                tflops: 0.2,\n            },\n            \"Xeon E5v2 (Ivy Bridge)\": {\n                tflops: 0.15,\n            },\n            \"Intel Core Ultra 7 265KF\": {\n                tflops: 1.53,\n            },\n            \"Intel Core 14th Generation (i7)\": {\n                tflops: 0.8,\n            },\n            \"Intel Core 13th Generation (i9)\": {\n                tflops: 0.85,\n            },\n            \"Intel Core 13th Generation (i7)\": {\n                tflops: 0.82,\n            },\n            \"Intel Core 13th Generation (i5)\": {\n                tflops: 0.68,\n            },\n            \"Intel Core 13th Generation (i3)\": {\n                tflops: 0.57,\n            },\n            \"Intel Core 12th Generation (i9)\": {\n                tflops: 0.79,\n            },\n            \"Intel Core 12th Generation (i7)\": {\n                tflops: 0.77,\n            },\n            \"Intel Core 12th Generation (i5)\": {\n                tflops: 0.65,\n            },\n            \"Intel Core 12th Generation (i3)\": {\n                tflops: 0.53,\n            },\n            \"Intel Core 11th Generation (i9)\": {\n                tflops: 0.7,\n            },\n            \"Intel Core 11th Generation (i7)\": {\n                tflops: 0.6,\n            },\n            \"Intel Core 11th Generation (i5)\": {\n                tflops: 0.5,\n            },\n            \"Intel Core 11th Generation (i3)\": {\n                tflops: 0.35,\n            },\n            \"Intel Core 10th Generation (i9)\": {\n                tflops: 0.46,\n            },\n            \"Intel Core 10th Generation (i7)\": {\n                tflops: 0.46,\n            },\n            \"Intel Core 10th Generation (i5)\": {\n                tflops: 0.46,\n            },\n            \"Intel Core 10th Generation (i3)\": {\n                tflops: 0.44,\n            },\n        },\n        AMD: {\n            \"EPYC 4th Generation (Genoa)\": {\n                tflops: 5,\n            },\n            \"EPYC 3th Generation (Milan)\": {\n                tflops: 2.4,\n            },\n            \"EPYC 2th Generation (Rome)\": {\n                tflops: 0.6,\n            },\n            \"EPYC 1st Generation (Naples)\": {\n                tflops: 0.6,\n            },\n            \"Ryzen Zen 4 7000 (Threadripper)\": {\n                tflops: 10.0,\n            },\n            \"Ryzen Zen5 9000 (Ryzen 9)\": {\n                tflops: 0.56,\n            },\n            \"Ryzen Zen5 9000 (Ryzen 7)\": {\n                tflops: 0.56,\n            },\n            \"Ryzen Zen5 9000 (Ryzen 5)\": {\n                tflops: 0.56,\n            },\n            \"Ryzen Zen4 7000 (Ryzen 9)\": {\n                tflops: 0.56,\n            },\n            \"Ryzen Zen4 7000 (Ryzen 7)\": {\n                tflops: 0.56,\n            },\n            \"Ryzen Zen4 7000 (Ryzen 5)\": {\n                tflops: 0.56,\n            },\n            \"Ryzen Zen3 5000 (Ryzen 9)\": {\n                tflops: 1.33,\n            },\n            \"Ryzen Zen3 5000 (Ryzen 7)\": {\n                tflops: 1.33,\n            },\n            \"Ryzen Zen3 5000 (Ryzen 5)\": {\n                tflops: 0.72,\n            },\n            \"Ryzen Zen 2  3000 (Threadripper)\": {\n                tflops: 0.72,\n            },\n            \"Ryzen Zen 2  3000 (Ryzen 9)\": {\n                tflops: 0.72,\n            },\n            \"Ryzen Zen 2  3000 (Ryzen 7)\": {\n                tflops: 0.72,\n            },\n            \"Ryzen Zen 2  3000 (Ryzen 5)\": {\n                tflops: 0.72,\n            },\n            \"Ryzen Zen 2  3000 (Ryzen 3)\": {\n                tflops: 0.72,\n            },\n        },\n    },\n    \"Apple Silicon\": {\n        \"-\": {\n            \"Apple M1\": {\n                tflops: 2.6,\n                memory: [8, 16],\n            },\n            \"Apple M1 Pro\": {\n                tflops: 5.2,\n                memory: [16, 24, 32],\n            },\n            \"Apple M1 Max\": {\n                tflops: 10.4,\n                memory: [16, 24, 32, 64],\n            },\n            \"Apple M1 Ultra\": {\n                tflops: 21,\n                memory: [16, 24, 32, 64, 96, 128],\n            },\n            \"Apple M2\": {\n                tflops: 3.6,\n                memory: [8, 16, 24],\n            },\n            \"Apple M2 Pro\": {\n                tflops: 6.8,\n                memory: [16, 24, 32],\n            },\n            \"Apple M2 Max\": {\n                tflops: 13.49,\n                memory: [32, 64, 96],\n            },\n            \"Apple M2 Ultra\": {\n                tflops: 27.2,\n                memory: [64, 96, 128, 192],\n            },\n            \"Apple M3\": {\n                tflops: 4.1,\n                memory: [8, 16, 24],\n            },\n            \"Apple M3 Pro\": {\n                tflops: 7.4,\n                memory: [18, 36],\n            },\n            \"Apple M3 Max\": {\n                tflops: 14.2,\n                memory: [36, 48, 64, 96, 128],\n            },\n            \"Apple M3 Ultra\": {\n                tflops: 28.4,\n                memory: [96, 256, 512],\n            },\n            \"Apple M4\": {\n                tflops: 4.6,\n                memory: [16, 24, 32],\n            },\n            \"Apple M4 Pro\": {\n                tflops: 9.2,\n                memory: [24, 48, 64],\n            },\n            \"Apple M4 Max\": {\n                tflops: 18.4,\n                memory: [36, 48, 64, 96, 128, 256, 512],\n            },\n        },\n    },\n};\n"],"names":[],"mappings":"AAAA;;;CAGC;;;;;;;;;;;;;;AACM,MAAM,oDAAoD,MAAM;AAChE,MAAM,4DAA4D,MAAM;AACxE,MAAM,uCAAuC,MAAM;AAKnD,MAAM,kDAAkD,MAAM;AAC9D,MAAM,yBAAyB;IAClC;IAAG;IAAI;IAAI;IAAI;IAAI;IAAI;IAAI;IAAI;IAAI;IAAK;IAAK;IAAK;IAAK;IAAK;IAAK;IAAM;IAAM;CAChF;AACM,MAAM,OAAO;IAChB,KAAK;QACD,QAAQ;YACJ,MAAM;gBACF,QAAQ;gBACR,QAAQ;oBAAC;iBAAI;YACjB;YACA,MAAM;gBACF,QAAQ;gBACR,QAAQ;oBAAC;iBAAI;YACjB;YACA,MAAM;gBACF,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,MAAM;gBACF,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,KAAK;gBACD,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,KAAK;gBACD,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,IAAI;gBACA,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,mBAAmB;gBACf,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,sBAAsB;gBAClB,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,gBAAgB;gBACZ,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,gBAAgB;gBACZ,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,gBAAgB;gBACZ,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,gBAAgB;gBACZ,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,gBAAgB;gBACZ,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,oBAAoB;gBAChB,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,gBAAgB;gBACZ,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,aAAa;gBACT,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,aAAa;gBACT,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,aAAa;gBACT,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,aAAa;gBACT,QAAQ;gBACR,QAAQ;oBAAC;oBAAG;iBAAG;YACnB;YACA,MAAM;gBACF,QAAQ;gBACR,QAAQ;oBAAC;oBAAI;iBAAG;YACpB;YACA,KAAK;gBACD,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,KAAK;gBACD,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,IAAI;gBACA,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,YAAY;gBACR,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,cAAc;gBACV,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,YAAY;gBACR,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,mBAAmB;gBACf,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,YAAY;gBACR,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,mBAAmB;gBACf,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,eAAe;gBACX,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,eAAe;gBACX,QAAQ;gBACR,QAAQ;oBAAC;oBAAI;iBAAE;YACnB;YACA,YAAY;gBACR,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,YAAY;gBACR,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,aAAa;gBACT,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,mBAAmB;gBACf,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,kBAAkB;gBACd,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,YAAY;gBACR,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,mBAAmB;gBACf,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,YAAY;gBACR,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,mBAAmB;gBACf,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,eAAe;gBACX,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,kBAAkB;gBACd,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,qBAAqB;gBACjB,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,YAAY;gBACR,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,eAAe;gBACX,QAAQ;gBACR,QAAQ;oBAAC;oBAAG;iBAAG;YACnB;YACA,mBAAmB;gBACf,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,mBAAmB;gBACf,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,mBAAmB;gBACf,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,mBAAmB;gBACf,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,mBAAmB;gBACf,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,YAAY;gBACR,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,eAAe;gBACX,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,YAAY;gBACR,QAAQ;gBACR,QAAQ;oBAAC;oBAAI;iBAAG;YACpB;YACA,eAAe;gBACX,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,mBAAmB;gBACf,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,YAAY;gBACR,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,eAAe;gBACX,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,sBAAsB;gBAClB,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,eAAe;gBACX,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,YAAY;gBACR,QAAQ;gBACR,QAAQ;oBAAC;oBAAI;iBAAE;YACnB;YACA,eAAe;gBACX,QAAQ;gBACR,QAAQ;oBAAC;oBAAI;iBAAG;YACpB;YACA,YAAY;gBACR,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,YAAY;gBACR,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,yBAAyB;gBACrB,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,kBAAkB;gBACd,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,mBAAmB;gBACf,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,mBAAmB;gBACf,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,YAAY;gBACR,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,iBAAiB;gBACb,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,mBAAmB;gBACf,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,eAAe;gBACX,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,eAAe;gBACX,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,YAAY;gBACR,QAAQ;gBACR,QAAQ;oBAAC;oBAAG;iBAAE;YAClB;YACA,eAAe;gBACX,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,aAAa;gBACT,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,YAAY;gBACR,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,mBAAmB;gBACf,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,IAAI;gBACA,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,KAAK;gBACD,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,MAAM;gBACF,QAAQ;gBACR,QAAQ;oBAAC;oBAAI;iBAAG;YACpB;YACA,gBAAgB;gBACZ,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,KAAK;gBACD,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,MAAM;gBACF,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,wBAAwB;gBACpB,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,wBAAwB;gBACpB,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,uBAAuB;gBACnB,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,sBAAsB;gBAClB,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,wBAAwB;gBACpB,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,wBAAwB;gBACpB,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,qBAAqB;gBACjB,QAAQ;gBACR,QAAQ;oBAAC;oBAAI;iBAAG;YACpB;YACA,oBAAoB;gBAChB,QAAQ;gBACR,QAAQ;oBAAC;oBAAG;iBAAG;YACnB;YACA,cAAc;gBACV,QAAQ;gBACR,QAAQ;oBAAC;oBAAG;iBAAE;YAClB;YACA,eAAe;gBACX,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;QACJ;QACA,KAAK;YACD,OAAO;gBACH,QAAQ;gBACR,QAAQ;oBAAC;iBAAI;YACjB;YACA,OAAO;gBACH,QAAQ;gBACR,QAAQ;oBAAC;iBAAI;YACjB;YACA,OAAO;gBACH,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,OAAO;gBACH,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,MAAM;gBACF,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,MAAM;gBACF,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,cAAc;gBACV,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,WAAW;gBACP,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,eAAe;gBACX,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,cAAc;gBACV,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,eAAe;gBACX,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,cAAc;gBACV,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,cAAc;gBACV,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,cAAc;gBACV,QAAQ;gBACR,QAAQ;oBAAC;oBAAI;iBAAE;YACnB;YACA,cAAc;gBACV,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,WAAW;gBACP,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,cAAc;gBACV,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,WAAW;gBACP,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,cAAc;gBACV,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,cAAc;gBACV,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,WAAW;gBACP,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,kBAAkB;gBACd,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;QACJ;QACA,OAAO;YACH,YAAY;gBACR,QAAQ;gBACR,QAAQ;oBAAC;iBAAE;YACf;YACA,YAAY;gBACR,QAAQ;gBACR,QAAQ;oBAAC;oBAAG;iBAAG;YACnB;YACA,YAAY;gBACR,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;YACA,YAAY;gBACR,QAAQ;gBACR,QAAQ;oBAAC;iBAAG;YAChB;QACJ;QACA,UAAU;YACN,iCAAiC;gBAC7B,QAAQ;YACZ;YACA,iCAAiC;gBAC7B,QAAQ;YACZ;YACA,iCAAiC;gBAC7B,QAAQ;YACZ;YACA,iCAAiC;gBAC7B,QAAQ;YACZ;YACA,gCAAgC;gBAC5B,QAAQ;YACZ;QACJ;IACJ;IACA,KAAK;QACD,OAAO;YACH,yCAAyC;gBACrC,QAAQ;YACZ;YACA,kCAAkC;gBAC9B,QAAQ;YACZ;YACA,sCAAsC;gBAClC,QAAQ;YACZ;YACA,yBAAyB;gBACrB,QAAQ;YACZ;YACA,uBAAuB;gBACnB,QAAQ;YACZ;YACA,0BAA0B;gBACtB,QAAQ;YACZ;YACA,4BAA4B;gBACxB,QAAQ;YACZ;YACA,mCAAmC;gBAC/B,QAAQ;YACZ;YACA,mCAAmC;gBAC/B,QAAQ;YACZ;YACA,mCAAmC;gBAC/B,QAAQ;YACZ;YACA,mCAAmC;gBAC/B,QAAQ;YACZ;YACA,mCAAmC;gBAC/B,QAAQ;YACZ;YACA,mCAAmC;gBAC/B,QAAQ;YACZ;YACA,mCAAmC;gBAC/B,QAAQ;YACZ;YACA,mCAAmC;gBAC/B,QAAQ;YACZ;YACA,mCAAmC;gBAC/B,QAAQ;YACZ;YACA,mCAAmC;gBAC/B,QAAQ;YACZ;YACA,mCAAmC;gBAC/B,QAAQ;YACZ;YACA,mCAAmC;gBAC/B,QAAQ;YACZ;YACA,mCAAmC;gBAC/B,QAAQ;YACZ;YACA,mCAAmC;gBAC/B,QAAQ;YACZ;YACA,mCAAmC;gBAC/B,QAAQ;YACZ;YACA,mCAAmC;gBAC/B,QAAQ;YACZ;YACA,mCAAmC;gBAC/B,QAAQ;YACZ;QACJ;QACA,KAAK;YACD,+BAA+B;gBAC3B,QAAQ;YACZ;YACA,+BAA+B;gBAC3B,QAAQ;YACZ;YACA,8BAA8B;gBAC1B,QAAQ;YACZ;YACA,gCAAgC;gBAC5B,QAAQ;YACZ;YACA,mCAAmC;gBAC/B,QAAQ;YACZ;YACA,6BAA6B;gBACzB,QAAQ;YACZ;YACA,6BAA6B;gBACzB,QAAQ;YACZ;YACA,6BAA6B;gBACzB,QAAQ;YACZ;YACA,6BAA6B;gBACzB,QAAQ;YACZ;YACA,6BAA6B;gBACzB,QAAQ;YACZ;YACA,6BAA6B;gBACzB,QAAQ;YACZ;YACA,6BAA6B;gBACzB,QAAQ;YACZ;YACA,6BAA6B;gBACzB,QAAQ;YACZ;YACA,6BAA6B;gBACzB,QAAQ;YACZ;YACA,oCAAoC;gBAChC,QAAQ;YACZ;YACA,+BAA+B;gBAC3B,QAAQ;YACZ;YACA,+BAA+B;gBAC3B,QAAQ;YACZ;YACA,+BAA+B;gBAC3B,QAAQ;YACZ;YACA,+BAA+B;gBAC3B,QAAQ;YACZ;QACJ;IACJ;IACA,iBAAiB;QACb,KAAK;YACD,YAAY;gBACR,QAAQ;gBACR,QAAQ;oBAAC;oBAAG;iBAAG;YACnB;YACA,gBAAgB;gBACZ,QAAQ;gBACR,QAAQ;oBAAC;oBAAI;oBAAI;iBAAG;YACxB;YACA,gBAAgB;gBACZ,QAAQ;gBACR,QAAQ;oBAAC;oBAAI;oBAAI;oBAAI;iBAAG;YAC5B;YACA,kBAAkB;gBACd,QAAQ;gBACR,QAAQ;oBAAC;oBAAI;oBAAI;oBAAI;oBAAI;oBAAI;iBAAI;YACrC;YACA,YAAY;gBACR,QAAQ;gBACR,QAAQ;oBAAC;oBAAG;oBAAI;iBAAG;YACvB;YACA,gBAAgB;gBACZ,QAAQ;gBACR,QAAQ;oBAAC;oBAAI;oBAAI;iBAAG;YACxB;YACA,gBAAgB;gBACZ,QAAQ;gBACR,QAAQ;oBAAC;oBAAI;oBAAI;iBAAG;YACxB;YACA,kBAAkB;gBACd,QAAQ;gBACR,QAAQ;oBAAC;oBAAI;oBAAI;oBAAK;iBAAI;YAC9B;YACA,YAAY;gBACR,QAAQ;gBACR,QAAQ;oBAAC;oBAAG;oBAAI;iBAAG;YACvB;YACA,gBAAgB;gBACZ,QAAQ;gBACR,QAAQ;oBAAC;oBAAI;iBAAG;YACpB;YACA,gBAAgB;gBACZ,QAAQ;gBACR,QAAQ;oBAAC;oBAAI;oBAAI;oBAAI;oBAAI;iBAAI;YACjC;YACA,kBAAkB;gBACd,QAAQ;gBACR,QAAQ;oBAAC;oBAAI;oBAAK;iBAAI;YAC1B;YACA,YAAY;gBACR,QAAQ;gBACR,QAAQ;oBAAC;oBAAI;oBAAI;iBAAG;YACxB;YACA,gBAAgB;gBACZ,QAAQ;gBACR,QAAQ;oBAAC;oBAAI;oBAAI;iBAAG;YACxB;YACA,gBAAgB;gBACZ,QAAQ;gBACR,QAAQ;oBAAC;oBAAI;oBAAI;oBAAI;oBAAI;oBAAK;oBAAK;iBAAI;YAC3C;QACJ;IACJ;AACJ","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 10985, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/local-apps.js"],"sourcesContent":["import { parseGGUFQuantLabel } from \"./gguf.js\";\nimport { stringifyMessages } from \"./snippets/common.js\";\nimport { getModelInputSnippet } from \"./snippets/inputs.js\";\nfunction isAwqModel(model) {\n    return model.config?.quantization_config?.quant_method === \"awq\";\n}\nfunction isGptqModel(model) {\n    return model.config?.quantization_config?.quant_method === \"gptq\";\n}\nfunction isAqlmModel(model) {\n    return model.config?.quantization_config?.quant_method === \"aqlm\";\n}\nfunction isMarlinModel(model) {\n    return model.config?.quantization_config?.quant_method === \"marlin\";\n}\nfunction isTransformersModel(model) {\n    return model.tags.includes(\"transformers\");\n}\nfunction isTgiModel(model) {\n    return model.tags.includes(\"text-generation-inference\");\n}\nfunction isLlamaCppGgufModel(model) {\n    return !!model.gguf?.context_length;\n}\nfunction isAmdRyzenModel(model) {\n    return model.tags.includes(\"ryzenai-hybrid\") || model.tags.includes(\"ryzenai-npu\");\n}\nfunction isMlxModel(model) {\n    return model.tags.includes(\"mlx\");\n}\nfunction getQuantTag(filepath) {\n    const defaultTag = \":{{QUANT_TAG}}\";\n    if (!filepath) {\n        return defaultTag;\n    }\n    const quantLabel = parseGGUFQuantLabel(filepath);\n    return quantLabel ? `:${quantLabel}` : defaultTag;\n}\nconst snippetLlamacpp = (model, filepath) => {\n    const command = (binary) => {\n        const snippet = [\"# Load and run the model:\", `${binary} -hf ${model.id}${getQuantTag(filepath)}`];\n        return snippet.join(\"\\n\");\n    };\n    return [\n        {\n            title: \"Install from brew\",\n            setup: \"brew install llama.cpp\",\n            content: command(\"llama-server\"),\n        },\n        {\n            title: \"Install from WinGet (Windows)\",\n            setup: \"winget install llama.cpp\",\n            content: command(\"llama-server\"),\n        },\n        {\n            title: \"Use pre-built binary\",\n            setup: [\n                // prettier-ignore\n                \"# Download pre-built binary from:\",\n                \"# https://github.com/ggerganov/llama.cpp/releases\",\n            ].join(\"\\n\"),\n            content: command(\"./llama-server\"),\n        },\n        {\n            title: \"Build from source code\",\n            setup: [\n                \"git clone https://github.com/ggerganov/llama.cpp.git\",\n                \"cd llama.cpp\",\n                \"cmake -B build\",\n                \"cmake --build build -j --target llama-server\",\n            ].join(\"\\n\"),\n            content: command(\"./build/bin/llama-server\"),\n        },\n    ];\n};\nconst snippetNodeLlamaCppCli = (model, filepath) => {\n    const tagName = getQuantTag(filepath);\n    return [\n        {\n            title: \"Chat with the model\",\n            content: `npx -y node-llama-cpp chat hf:${model.id}${tagName}`,\n        },\n        {\n            title: \"Estimate the model compatibility with your hardware\",\n            content: `npx -y node-llama-cpp inspect estimate hf:${model.id}${tagName}`,\n        },\n    ];\n};\nconst snippetOllama = (model, filepath) => {\n    return `ollama run hf.co/${model.id}${getQuantTag(filepath)}`;\n};\nconst snippetLocalAI = (model, filepath) => {\n    const command = (binary) => [\"# Load and run the model:\", `${binary} huggingface://${model.id}/${filepath ?? \"{{GGUF_FILE}}\"}`].join(\"\\n\");\n    return [\n        {\n            title: \"Install from binary\",\n            setup: \"curl https://localai.io/install.sh | sh\",\n            content: command(\"local-ai run\"),\n        },\n        {\n            title: \"Use Docker images\",\n            setup: [\n                // prettier-ignore\n                \"# Pull the image:\",\n                \"docker pull localai/localai:latest-cpu\",\n            ].join(\"\\n\"),\n            content: command(\"docker run -p 8080:8080 --name localai -v $PWD/models:/build/models localai/localai:latest-cpu\"),\n        },\n    ];\n};\nconst snippetVllm = (model) => {\n    const messages = getModelInputSnippet(model);\n    const runCommandInstruct = `# Call the server using curl:\ncurl -X POST \"http://localhost:8000/v1/chat/completions\" \\\\\n\t-H \"Content-Type: application/json\" \\\\\n\t--data '{\n\t\t\"model\": \"${model.id}\",\n\t\t\"messages\": ${stringifyMessages(messages, {\n        indent: \"\\t\\t\",\n        attributeKeyQuotes: true,\n        customContentEscaper: (str) => str.replace(/'/g, \"'\\\\''\"),\n    })}\n\t}'`;\n    const runCommandNonInstruct = `# Call the server using curl:\ncurl -X POST \"http://localhost:8000/v1/completions\" \\\\\n\t-H \"Content-Type: application/json\" \\\\\n\t--data '{\n\t\t\"model\": \"${model.id}\",\n\t\t\"prompt\": \"Once upon a time,\",\n\t\t\"max_tokens\": 512,\n\t\t\"temperature\": 0.5\n\t}'`;\n    const runCommand = model.tags.includes(\"conversational\") ? runCommandInstruct : runCommandNonInstruct;\n    let setup;\n    let dockerCommand;\n    if (model.tags.includes(\"mistral-common\")) {\n        setup = [\n            \"# Install vLLM from pip:\",\n            \"pip install vllm\",\n            \"# Make sure you have the latest version of mistral-common installed:\",\n            \"pip install --upgrade mistral-common\",\n        ].join(\"\\n\");\n        dockerCommand = `# Load and run the model:\\ndocker exec -it my_vllm_container bash -c \"vllm serve ${model.id} --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice\"`;\n    }\n    else {\n        setup = [\"# Install vLLM from pip:\", \"pip install vllm\"].join(\"\\n\");\n        dockerCommand = `# Load and run the model:\\ndocker exec -it my_vllm_container bash -c \"vllm serve ${model.id}\"`;\n    }\n    return [\n        {\n            title: \"Install from pip\",\n            setup: setup,\n            content: [`# Load and run the model:\\nvllm serve \"${model.id}\"`, runCommand],\n        },\n        {\n            title: \"Use Docker images\",\n            setup: [\n                \"# Deploy with docker on Linux:\",\n                `docker run --runtime nvidia --gpus all \\\\`,\n                `\t--name my_vllm_container \\\\`,\n                `\t-v ~/.cache/huggingface:/root/.cache/huggingface \\\\`,\n                ` \t--env \"HUGGING_FACE_HUB_TOKEN=<secret>\" \\\\`,\n                `\t-p 8000:8000 \\\\`,\n                `\t--ipc=host \\\\`,\n                `\tvllm/vllm-openai:latest \\\\`,\n                `\t--model ${model.id}`,\n            ].join(\"\\n\"),\n            content: [dockerCommand, runCommand],\n        },\n    ];\n};\nconst snippetTgi = (model) => {\n    const runCommand = [\n        \"# Call the server using curl:\",\n        `curl -X POST \"http://localhost:8000/v1/chat/completions\" \\\\`,\n        `\t-H \"Content-Type: application/json\" \\\\`,\n        `\t--data '{`,\n        `\t\t\"model\": \"${model.id}\",`,\n        `\t\t\"messages\": [`,\n        `\t\t\t{\"role\": \"user\", \"content\": \"What is the capital of France?\"}`,\n        `\t\t]`,\n        `\t}'`,\n    ];\n    return [\n        {\n            title: \"Use Docker images\",\n            setup: [\n                \"# Deploy with docker on Linux:\",\n                `docker run --gpus all \\\\`,\n                `\t-v ~/.cache/huggingface:/root/.cache/huggingface \\\\`,\n                ` \t-e HF_TOKEN=\"<secret>\" \\\\`,\n                `\t-p 8000:80 \\\\`,\n                `\tghcr.io/huggingface/text-generation-inference:latest \\\\`,\n                `\t--model-id ${model.id}`,\n            ].join(\"\\n\"),\n            content: [runCommand.join(\"\\n\")],\n        },\n    ];\n};\nconst snippetMlxLm = (model) => {\n    const openaiCurl = [\n        \"# Calling the OpenAI-compatible server with curl\",\n        `curl -X POST \"http://localhost:8000/v1/chat/completions\" \\\\`,\n        `   -H \"Content-Type: application/json\" \\\\`,\n        `   --data '{`,\n        `     \"model\": \"${model.id}\",`,\n        `     \"messages\": [`,\n        `       {\"role\": \"user\", \"content\": \"Hello\"}`,\n        `     ]`,\n        `   }'`,\n    ];\n    return [\n        {\n            title: \"Generate or start a chat session\",\n            setup: [\"# Install MLX LM\", \"uv tool install mlx-lm\"].join(\"\\n\"),\n            content: [\n                ...(model.tags.includes(\"conversational\")\n                    ? [\"# Interactive chat REPL\", `mlx_lm.chat --model \"${model.id}\"`]\n                    : [\"# Generate some text\", `mlx_lm.generate --model \"${model.id}\" --prompt \"Once upon a time\"`]),\n            ].join(\"\\n\"),\n        },\n        ...(model.tags.includes(\"conversational\")\n            ? [\n                {\n                    title: \"Run an OpenAI-compatible server\",\n                    setup: [\"# Install MLX LM\", \"uv tool install mlx-lm\"].join(\"\\n\"),\n                    content: [\"# Start the server\", `mlx_lm.server --model \"${model.id}\"`, ...openaiCurl].join(\"\\n\"),\n                },\n            ]\n            : []),\n    ];\n};\nconst snippetDockerModelRunner = (model, filepath) => {\n    return `docker model run hf.co/${model.id}${getQuantTag(filepath)}`;\n};\nconst snippetLemonade = (model, filepath) => {\n    const tagName = getQuantTag(filepath);\n    const modelName = model.id.includes(\"/\") ? model.id.split(\"/\")[1] : model.id;\n    // Get recipe according to model type\n    let simplifiedModelName;\n    let recipe;\n    let checkpoint;\n    let requirements;\n    if (model.tags.some((tag) => [\"ryzenai-npu\", \"ryzenai-hybrid\"].includes(tag))) {\n        recipe = model.tags.includes(\"ryzenai-npu\") ? \"oga-npu\" : \"oga-hybrid\";\n        checkpoint = model.id;\n        requirements = \" (requires RyzenAI 300 series)\";\n        simplifiedModelName = modelName.split(\"-awq-\")[0];\n        simplifiedModelName += recipe === \"oga-npu\" ? \"-NPU\" : \"-Hybrid\";\n    }\n    else {\n        recipe = \"llamacpp\";\n        checkpoint = `${model.id}${tagName}`;\n        requirements = \"\";\n        simplifiedModelName = modelName;\n    }\n    return [\n        {\n            title: \"Pull the model\",\n            setup: \"# Download Lemonade from https://lemonade-server.ai/\",\n            content: [\n                `lemonade-server pull user.${simplifiedModelName} --checkpoint ${checkpoint} --recipe ${recipe}`,\n                \"# Note: If you installed from source, use the lemonade-server-dev command instead.\",\n            ].join(\"\\n\"),\n        },\n        {\n            title: `Run and chat with the model${requirements}`,\n            content: `lemonade-server run user.${simplifiedModelName}`,\n        },\n        {\n            title: \"List all available models\",\n            content: \"lemonade-server list\",\n        },\n    ];\n};\n/**\n * Add your new local app here.\n *\n * This is open to new suggestions and awesome upcoming apps.\n *\n * /!\\ IMPORTANT\n *\n * If possible, you need to support deeplinks and be as cross-platform as possible.\n *\n * Ping the HF team if we can help with anything!\n */\nexport const LOCAL_APPS = {\n    \"llama.cpp\": {\n        prettyLabel: \"llama.cpp\",\n        docsUrl: \"https://github.com/ggerganov/llama.cpp\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: isLlamaCppGgufModel,\n        snippet: snippetLlamacpp,\n    },\n    \"node-llama-cpp\": {\n        prettyLabel: \"node-llama-cpp\",\n        docsUrl: \"https://node-llama-cpp.withcat.ai\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: isLlamaCppGgufModel,\n        snippet: snippetNodeLlamaCppCli,\n    },\n    vllm: {\n        prettyLabel: \"vLLM\",\n        docsUrl: \"https://docs.vllm.ai\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: (model) => (isAwqModel(model) ||\n            isGptqModel(model) ||\n            isAqlmModel(model) ||\n            isMarlinModel(model) ||\n            isLlamaCppGgufModel(model) ||\n            isTransformersModel(model)) &&\n            (model.pipeline_tag === \"text-generation\" || model.pipeline_tag === \"image-text-to-text\"),\n        snippet: snippetVllm,\n    },\n    \"mlx-lm\": {\n        prettyLabel: \"MLX LM\",\n        docsUrl: \"https://github.com/ml-explore/mlx-lm\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: (model) => model.pipeline_tag === \"text-generation\" && isMlxModel(model),\n        snippet: snippetMlxLm,\n    },\n    tgi: {\n        prettyLabel: \"TGI\",\n        docsUrl: \"https://huggingface.co/docs/text-generation-inference/\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: isTgiModel,\n        snippet: snippetTgi,\n    },\n    lmstudio: {\n        prettyLabel: \"LM Studio\",\n        docsUrl: \"https://lmstudio.ai\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: (model) => isLlamaCppGgufModel(model) || isMlxModel(model),\n        deeplink: (model, filepath) => new URL(`lmstudio://open_from_hf?model=${model.id}${filepath ? `&file=${filepath}` : \"\"}`),\n    },\n    localai: {\n        prettyLabel: \"LocalAI\",\n        docsUrl: \"https://github.com/mudler/LocalAI\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: isLlamaCppGgufModel,\n        snippet: snippetLocalAI,\n    },\n    jan: {\n        prettyLabel: \"Jan\",\n        docsUrl: \"https://jan.ai\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: isLlamaCppGgufModel,\n        deeplink: (model) => new URL(`jan://models/huggingface/${model.id}`),\n    },\n    backyard: {\n        prettyLabel: \"Backyard AI\",\n        docsUrl: \"https://backyard.ai\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: isLlamaCppGgufModel,\n        deeplink: (model) => new URL(`https://backyard.ai/hf/model/${model.id}`),\n    },\n    sanctum: {\n        prettyLabel: \"Sanctum\",\n        docsUrl: \"https://sanctum.ai\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: isLlamaCppGgufModel,\n        deeplink: (model) => new URL(`sanctum://open_from_hf?model=${model.id}`),\n    },\n    jellybox: {\n        prettyLabel: \"Jellybox\",\n        docsUrl: \"https://jellybox.com\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: (model) => isLlamaCppGgufModel(model) ||\n            (model.library_name === \"diffusers\" &&\n                model.tags.includes(\"safetensors\") &&\n                (model.pipeline_tag === \"text-to-image\" || model.tags.includes(\"lora\"))),\n        deeplink: (model) => {\n            if (isLlamaCppGgufModel(model)) {\n                return new URL(`jellybox://llm/models/huggingface/LLM/${model.id}`);\n            }\n            else if (model.tags.includes(\"lora\")) {\n                return new URL(`jellybox://image/models/huggingface/ImageLora/${model.id}`);\n            }\n            else {\n                return new URL(`jellybox://image/models/huggingface/Image/${model.id}`);\n            }\n        },\n    },\n    msty: {\n        prettyLabel: \"Msty\",\n        docsUrl: \"https://msty.app\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: isLlamaCppGgufModel,\n        deeplink: (model) => new URL(`msty://models/search/hf/${model.id}`),\n    },\n    recursechat: {\n        prettyLabel: \"RecurseChat\",\n        docsUrl: \"https://recurse.chat\",\n        mainTask: \"text-generation\",\n        macOSOnly: true,\n        displayOnModelPage: isLlamaCppGgufModel,\n        deeplink: (model) => new URL(`recursechat://new-hf-gguf-model?hf-model-id=${model.id}`),\n    },\n    drawthings: {\n        prettyLabel: \"Draw Things\",\n        docsUrl: \"https://drawthings.ai\",\n        mainTask: \"text-to-image\",\n        macOSOnly: true,\n        displayOnModelPage: (model) => model.library_name === \"diffusers\" && (model.pipeline_tag === \"text-to-image\" || model.tags.includes(\"lora\")),\n        deeplink: (model) => {\n            if (model.tags.includes(\"lora\")) {\n                return new URL(`https://drawthings.ai/import/diffusers/pipeline.load_lora_weights?repo_id=${model.id}`);\n            }\n            else {\n                return new URL(`https://drawthings.ai/import/diffusers/pipeline.from_pretrained?repo_id=${model.id}`);\n            }\n        },\n    },\n    diffusionbee: {\n        prettyLabel: \"DiffusionBee\",\n        docsUrl: \"https://diffusionbee.com\",\n        mainTask: \"text-to-image\",\n        macOSOnly: true,\n        displayOnModelPage: (model) => model.library_name === \"diffusers\" && model.pipeline_tag === \"text-to-image\",\n        deeplink: (model) => new URL(`https://diffusionbee.com/huggingface_import?model_id=${model.id}`),\n    },\n    joyfusion: {\n        prettyLabel: \"JoyFusion\",\n        docsUrl: \"https://joyfusion.app\",\n        mainTask: \"text-to-image\",\n        macOSOnly: true,\n        displayOnModelPage: (model) => model.tags.includes(\"coreml\") && model.tags.includes(\"joyfusion\") && model.pipeline_tag === \"text-to-image\",\n        deeplink: (model) => new URL(`https://joyfusion.app/import_from_hf?repo_id=${model.id}`),\n    },\n    invoke: {\n        prettyLabel: \"Invoke\",\n        docsUrl: \"https://github.com/invoke-ai/InvokeAI\",\n        mainTask: \"text-to-image\",\n        displayOnModelPage: (model) => model.library_name === \"diffusers\" && model.pipeline_tag === \"text-to-image\",\n        deeplink: (model) => new URL(`https://models.invoke.ai/huggingface/${model.id}`),\n    },\n    ollama: {\n        prettyLabel: \"Ollama\",\n        docsUrl: \"https://ollama.com\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: isLlamaCppGgufModel,\n        snippet: snippetOllama,\n    },\n    \"docker-model-runner\": {\n        prettyLabel: \"Docker Model Runner\",\n        docsUrl: \"https://docs.docker.com/ai/model-runner/\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: isLlamaCppGgufModel,\n        snippet: snippetDockerModelRunner,\n    },\n    lemonade: {\n        prettyLabel: \"Lemonade\",\n        docsUrl: \"https://lemonade-server.ai\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: (model) => isLlamaCppGgufModel(model) || isAmdRyzenModel(model),\n        snippet: snippetLemonade,\n    },\n};\n"],"names":[],"mappings":";;;;AAAA;AACA;AACA;;;;AACA,SAAS,WAAW,KAAK;IACrB,OAAO,MAAM,MAAM,EAAE,qBAAqB,iBAAiB;AAC/D;AACA,SAAS,YAAY,KAAK;IACtB,OAAO,MAAM,MAAM,EAAE,qBAAqB,iBAAiB;AAC/D;AACA,SAAS,YAAY,KAAK;IACtB,OAAO,MAAM,MAAM,EAAE,qBAAqB,iBAAiB;AAC/D;AACA,SAAS,cAAc,KAAK;IACxB,OAAO,MAAM,MAAM,EAAE,qBAAqB,iBAAiB;AAC/D;AACA,SAAS,oBAAoB,KAAK;IAC9B,OAAO,MAAM,IAAI,CAAC,QAAQ,CAAC;AAC/B;AACA,SAAS,WAAW,KAAK;IACrB,OAAO,MAAM,IAAI,CAAC,QAAQ,CAAC;AAC/B;AACA,SAAS,oBAAoB,KAAK;IAC9B,OAAO,CAAC,CAAC,MAAM,IAAI,EAAE;AACzB;AACA,SAAS,gBAAgB,KAAK;IAC1B,OAAO,MAAM,IAAI,CAAC,QAAQ,CAAC,qBAAqB,MAAM,IAAI,CAAC,QAAQ,CAAC;AACxE;AACA,SAAS,WAAW,KAAK;IACrB,OAAO,MAAM,IAAI,CAAC,QAAQ,CAAC;AAC/B;AACA,SAAS,YAAY,QAAQ;IACzB,MAAM,aAAa;IACnB,IAAI,CAAC,UAAU;QACX,OAAO;IACX;IACA,MAAM,aAAa,IAAA,sLAAmB,EAAC;IACvC,OAAO,aAAa,CAAC,CAAC,EAAE,YAAY,GAAG;AAC3C;AACA,MAAM,kBAAkB,CAAC,OAAO;IAC5B,MAAM,UAAU,CAAC;QACb,MAAM,UAAU;YAAC;YAA6B,GAAG,OAAO,KAAK,EAAE,MAAM,EAAE,GAAG,YAAY,WAAW;SAAC;QAClG,OAAO,QAAQ,IAAI,CAAC;IACxB;IACA,OAAO;QACH;YACI,OAAO;YACP,OAAO;YACP,SAAS,QAAQ;QACrB;QACA;YACI,OAAO;YACP,OAAO;YACP,SAAS,QAAQ;QACrB;QACA;YACI,OAAO;YACP,OAAO;gBACH,kBAAkB;gBAClB;gBACA;aACH,CAAC,IAAI,CAAC;YACP,SAAS,QAAQ;QACrB;QACA;YACI,OAAO;YACP,OAAO;gBACH;gBACA;gBACA;gBACA;aACH,CAAC,IAAI,CAAC;YACP,SAAS,QAAQ;QACrB;KACH;AACL;AACA,MAAM,yBAAyB,CAAC,OAAO;IACnC,MAAM,UAAU,YAAY;IAC5B,OAAO;QACH;YACI,OAAO;YACP,SAAS,CAAC,8BAA8B,EAAE,MAAM,EAAE,GAAG,SAAS;QAClE;QACA;YACI,OAAO;YACP,SAAS,CAAC,0CAA0C,EAAE,MAAM,EAAE,GAAG,SAAS;QAC9E;KACH;AACL;AACA,MAAM,gBAAgB,CAAC,OAAO;IAC1B,OAAO,CAAC,iBAAiB,EAAE,MAAM,EAAE,GAAG,YAAY,WAAW;AACjE;AACA,MAAM,iBAAiB,CAAC,OAAO;IAC3B,MAAM,UAAU,CAAC,SAAW;YAAC;YAA6B,GAAG,OAAO,eAAe,EAAE,MAAM,EAAE,CAAC,CAAC,EAAE,YAAY,iBAAiB;SAAC,CAAC,IAAI,CAAC;IACrI,OAAO;QACH;YACI,OAAO;YACP,OAAO;YACP,SAAS,QAAQ;QACrB;QACA;YACI,OAAO;YACP,OAAO;gBACH,kBAAkB;gBAClB;gBACA;aACH,CAAC,IAAI,CAAC;YACP,SAAS,QAAQ;QACrB;KACH;AACL;AACA,MAAM,cAAc,CAAC;IACjB,MAAM,WAAW,IAAA,qMAAoB,EAAC;IACtC,MAAM,qBAAqB,CAAC;;;;YAIpB,EAAE,MAAM,EAAE,CAAC;cACT,EAAE,IAAA,kMAAiB,EAAC,UAAU;QACpC,QAAQ;QACR,oBAAoB;QACpB,sBAAsB,CAAC,MAAQ,IAAI,OAAO,CAAC,MAAM;IACrD,GAAG;GACJ,CAAC;IACA,MAAM,wBAAwB,CAAC;;;;YAIvB,EAAE,MAAM,EAAE,CAAC;;;;GAIpB,CAAC;IACA,MAAM,aAAa,MAAM,IAAI,CAAC,QAAQ,CAAC,oBAAoB,qBAAqB;IAChF,IAAI;IACJ,IAAI;IACJ,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,mBAAmB;QACvC,QAAQ;YACJ;YACA;YACA;YACA;SACH,CAAC,IAAI,CAAC;QACP,gBAAgB,CAAC,iFAAiF,EAAE,MAAM,EAAE,CAAC,6HAA6H,CAAC;IAC/O,OACK;QACD,QAAQ;YAAC;YAA4B;SAAmB,CAAC,IAAI,CAAC;QAC9D,gBAAgB,CAAC,iFAAiF,EAAE,MAAM,EAAE,CAAC,CAAC,CAAC;IACnH;IACA,OAAO;QACH;YACI,OAAO;YACP,OAAO;YACP,SAAS;gBAAC,CAAC,uCAAuC,EAAE,MAAM,EAAE,CAAC,CAAC,CAAC;gBAAE;aAAW;QAChF;QACA;YACI,OAAO;YACP,OAAO;gBACH;gBACA,CAAC,yCAAyC,CAAC;gBAC3C,CAAC,4BAA4B,CAAC;gBAC9B,CAAC,oDAAoD,CAAC;gBACtD,CAAC,4CAA4C,CAAC;gBAC9C,CAAC,gBAAgB,CAAC;gBAClB,CAAC,cAAc,CAAC;gBAChB,CAAC,2BAA2B,CAAC;gBAC7B,CAAC,SAAS,EAAE,MAAM,EAAE,EAAE;aACzB,CAAC,IAAI,CAAC;YACP,SAAS;gBAAC;gBAAe;aAAW;QACxC;KACH;AACL;AACA,MAAM,aAAa,CAAC;IAChB,MAAM,aAAa;QACf;QACA,CAAC,2DAA2D,CAAC;QAC7D,CAAC,uCAAuC,CAAC;QACzC,CAAC,UAAU,CAAC;QACZ,CAAC,YAAY,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;QAC3B,CAAC,eAAe,CAAC;QACjB,CAAC,gEAAgE,CAAC;QAClE,CAAC,GAAG,CAAC;QACL,CAAC,GAAG,CAAC;KACR;IACD,OAAO;QACH;YACI,OAAO;YACP,OAAO;gBACH;gBACA,CAAC,wBAAwB,CAAC;gBAC1B,CAAC,oDAAoD,CAAC;gBACtD,CAAC,2BAA2B,CAAC;gBAC7B,CAAC,cAAc,CAAC;gBAChB,CAAC,wDAAwD,CAAC;gBAC1D,CAAC,YAAY,EAAE,MAAM,EAAE,EAAE;aAC5B,CAAC,IAAI,CAAC;YACP,SAAS;gBAAC,WAAW,IAAI,CAAC;aAAM;QACpC;KACH;AACL;AACA,MAAM,eAAe,CAAC;IAClB,MAAM,aAAa;QACf;QACA,CAAC,2DAA2D,CAAC;QAC7D,CAAC,yCAAyC,CAAC;QAC3C,CAAC,YAAY,CAAC;QACd,CAAC,eAAe,EAAE,MAAM,EAAE,CAAC,EAAE,CAAC;QAC9B,CAAC,kBAAkB,CAAC;QACpB,CAAC,2CAA2C,CAAC;QAC7C,CAAC,MAAM,CAAC;QACR,CAAC,KAAK,CAAC;KACV;IACD,OAAO;QACH;YACI,OAAO;YACP,OAAO;gBAAC;gBAAoB;aAAyB,CAAC,IAAI,CAAC;YAC3D,SAAS;mBACD,MAAM,IAAI,CAAC,QAAQ,CAAC,oBAClB;oBAAC;oBAA2B,CAAC,qBAAqB,EAAE,MAAM,EAAE,CAAC,CAAC,CAAC;iBAAC,GAChE;oBAAC;oBAAwB,CAAC,yBAAyB,EAAE,MAAM,EAAE,CAAC,6BAA6B,CAAC;iBAAC;aACtG,CAAC,IAAI,CAAC;QACX;WACI,MAAM,IAAI,CAAC,QAAQ,CAAC,oBAClB;YACE;gBACI,OAAO;gBACP,OAAO;oBAAC;oBAAoB;iBAAyB,CAAC,IAAI,CAAC;gBAC3D,SAAS;oBAAC;oBAAsB,CAAC,uBAAuB,EAAE,MAAM,EAAE,CAAC,CAAC,CAAC;uBAAK;iBAAW,CAAC,IAAI,CAAC;YAC/F;SACH,GACC,EAAE;KACX;AACL;AACA,MAAM,2BAA2B,CAAC,OAAO;IACrC,OAAO,CAAC,uBAAuB,EAAE,MAAM,EAAE,GAAG,YAAY,WAAW;AACvE;AACA,MAAM,kBAAkB,CAAC,OAAO;IAC5B,MAAM,UAAU,YAAY;IAC5B,MAAM,YAAY,MAAM,EAAE,CAAC,QAAQ,CAAC,OAAO,MAAM,EAAE,CAAC,KAAK,CAAC,IAAI,CAAC,EAAE,GAAG,MAAM,EAAE;IAC5E,qCAAqC;IACrC,IAAI;IACJ,IAAI;IACJ,IAAI;IACJ,IAAI;IACJ,IAAI,MAAM,IAAI,CAAC,IAAI,CAAC,CAAC,MAAQ;YAAC;YAAe;SAAiB,CAAC,QAAQ,CAAC,OAAO;QAC3E,SAAS,MAAM,IAAI,CAAC,QAAQ,CAAC,iBAAiB,YAAY;QAC1D,aAAa,MAAM,EAAE;QACrB,eAAe;QACf,sBAAsB,UAAU,KAAK,CAAC,QAAQ,CAAC,EAAE;QACjD,uBAAuB,WAAW,YAAY,SAAS;IAC3D,OACK;QACD,SAAS;QACT,aAAa,GAAG,MAAM,EAAE,GAAG,SAAS;QACpC,eAAe;QACf,sBAAsB;IAC1B;IACA,OAAO;QACH;YACI,OAAO;YACP,OAAO;YACP,SAAS;gBACL,CAAC,0BAA0B,EAAE,oBAAoB,cAAc,EAAE,WAAW,UAAU,EAAE,QAAQ;gBAChG;aACH,CAAC,IAAI,CAAC;QACX;QACA;YACI,OAAO,CAAC,2BAA2B,EAAE,cAAc;YACnD,SAAS,CAAC,yBAAyB,EAAE,qBAAqB;QAC9D;QACA;YACI,OAAO;YACP,SAAS;QACb;KACH;AACL;AAYO,MAAM,aAAa;IACtB,aAAa;QACT,aAAa;QACb,SAAS;QACT,UAAU;QACV,oBAAoB;QACpB,SAAS;IACb;IACA,kBAAkB;QACd,aAAa;QACb,SAAS;QACT,UAAU;QACV,oBAAoB;QACpB,SAAS;IACb;IACA,MAAM;QACF,aAAa;QACb,SAAS;QACT,UAAU;QACV,oBAAoB,CAAC,QAAU,CAAC,WAAW,UACvC,YAAY,UACZ,YAAY,UACZ,cAAc,UACd,oBAAoB,UACpB,oBAAoB,MAAM,KAC1B,CAAC,MAAM,YAAY,KAAK,qBAAqB,MAAM,YAAY,KAAK,oBAAoB;QAC5F,SAAS;IACb;IACA,UAAU;QACN,aAAa;QACb,SAAS;QACT,UAAU;QACV,oBAAoB,CAAC,QAAU,MAAM,YAAY,KAAK,qBAAqB,WAAW;QACtF,SAAS;IACb;IACA,KAAK;QACD,aAAa;QACb,SAAS;QACT,UAAU;QACV,oBAAoB;QACpB,SAAS;IACb;IACA,UAAU;QACN,aAAa;QACb,SAAS;QACT,UAAU;QACV,oBAAoB,CAAC,QAAU,oBAAoB,UAAU,WAAW;QACxE,UAAU,CAAC,OAAO,WAAa,IAAI,IAAI,CAAC,8BAA8B,EAAE,MAAM,EAAE,GAAG,WAAW,CAAC,MAAM,EAAE,UAAU,GAAG,IAAI;IAC5H;IACA,SAAS;QACL,aAAa;QACb,SAAS;QACT,UAAU;QACV,oBAAoB;QACpB,SAAS;IACb;IACA,KAAK;QACD,aAAa;QACb,SAAS;QACT,UAAU;QACV,oBAAoB;QACpB,UAAU,CAAC,QAAU,IAAI,IAAI,CAAC,yBAAyB,EAAE,MAAM,EAAE,EAAE;IACvE;IACA,UAAU;QACN,aAAa;QACb,SAAS;QACT,UAAU;QACV,oBAAoB;QACpB,UAAU,CAAC,QAAU,IAAI,IAAI,CAAC,6BAA6B,EAAE,MAAM,EAAE,EAAE;IAC3E;IACA,SAAS;QACL,aAAa;QACb,SAAS;QACT,UAAU;QACV,oBAAoB;QACpB,UAAU,CAAC,QAAU,IAAI,IAAI,CAAC,6BAA6B,EAAE,MAAM,EAAE,EAAE;IAC3E;IACA,UAAU;QACN,aAAa;QACb,SAAS;QACT,UAAU;QACV,oBAAoB,CAAC,QAAU,oBAAoB,UAC9C,MAAM,YAAY,KAAK,eACpB,MAAM,IAAI,CAAC,QAAQ,CAAC,kBACpB,CAAC,MAAM,YAAY,KAAK,mBAAmB,MAAM,IAAI,CAAC,QAAQ,CAAC,OAAO;QAC9E,UAAU,CAAC;YACP,IAAI,oBAAoB,QAAQ;gBAC5B,OAAO,IAAI,IAAI,CAAC,sCAAsC,EAAE,MAAM,EAAE,EAAE;YACtE,OACK,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,SAAS;gBAClC,OAAO,IAAI,IAAI,CAAC,8CAA8C,EAAE,MAAM,EAAE,EAAE;YAC9E,OACK;gBACD,OAAO,IAAI,IAAI,CAAC,0CAA0C,EAAE,MAAM,EAAE,EAAE;YAC1E;QACJ;IACJ;IACA,MAAM;QACF,aAAa;QACb,SAAS;QACT,UAAU;QACV,oBAAoB;QACpB,UAAU,CAAC,QAAU,IAAI,IAAI,CAAC,wBAAwB,EAAE,MAAM,EAAE,EAAE;IACtE;IACA,aAAa;QACT,aAAa;QACb,SAAS;QACT,UAAU;QACV,WAAW;QACX,oBAAoB;QACpB,UAAU,CAAC,QAAU,IAAI,IAAI,CAAC,4CAA4C,EAAE,MAAM,EAAE,EAAE;IAC1F;IACA,YAAY;QACR,aAAa;QACb,SAAS;QACT,UAAU;QACV,WAAW;QACX,oBAAoB,CAAC,QAAU,MAAM,YAAY,KAAK,eAAe,CAAC,MAAM,YAAY,KAAK,mBAAmB,MAAM,IAAI,CAAC,QAAQ,CAAC,OAAO;QAC3I,UAAU,CAAC;YACP,IAAI,MAAM,IAAI,CAAC,QAAQ,CAAC,SAAS;gBAC7B,OAAO,IAAI,IAAI,CAAC,0EAA0E,EAAE,MAAM,EAAE,EAAE;YAC1G,OACK;gBACD,OAAO,IAAI,IAAI,CAAC,wEAAwE,EAAE,MAAM,EAAE,EAAE;YACxG;QACJ;IACJ;IACA,cAAc;QACV,aAAa;QACb,SAAS;QACT,UAAU;QACV,WAAW;QACX,oBAAoB,CAAC,QAAU,MAAM,YAAY,KAAK,eAAe,MAAM,YAAY,KAAK;QAC5F,UAAU,CAAC,QAAU,IAAI,IAAI,CAAC,qDAAqD,EAAE,MAAM,EAAE,EAAE;IACnG;IACA,WAAW;QACP,aAAa;QACb,SAAS;QACT,UAAU;QACV,WAAW;QACX,oBAAoB,CAAC,QAAU,MAAM,IAAI,CAAC,QAAQ,CAAC,aAAa,MAAM,IAAI,CAAC,QAAQ,CAAC,gBAAgB,MAAM,YAAY,KAAK;QAC3H,UAAU,CAAC,QAAU,IAAI,IAAI,CAAC,6CAA6C,EAAE,MAAM,EAAE,EAAE;IAC3F;IACA,QAAQ;QACJ,aAAa;QACb,SAAS;QACT,UAAU;QACV,oBAAoB,CAAC,QAAU,MAAM,YAAY,KAAK,eAAe,MAAM,YAAY,KAAK;QAC5F,UAAU,CAAC,QAAU,IAAI,IAAI,CAAC,qCAAqC,EAAE,MAAM,EAAE,EAAE;IACnF;IACA,QAAQ;QACJ,aAAa;QACb,SAAS;QACT,UAAU;QACV,oBAAoB;QACpB,SAAS;IACb;IACA,uBAAuB;QACnB,aAAa;QACb,SAAS;QACT,UAAU;QACV,oBAAoB;QACpB,SAAS;IACb;IACA,UAAU;QACN,aAAa;QACb,SAAS;QACT,UAAU;QACV,oBAAoB,CAAC,QAAU,oBAAoB,UAAU,gBAAgB;QAC7E,SAAS;IACb;AACJ","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 11461, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/dataset-libraries.js"],"sourcesContent":["export const DATASET_LIBRARIES_UI_ELEMENTS = {\n    mlcroissant: {\n        prettyLabel: \"Croissant\",\n        repoName: \"croissant\",\n        repoUrl: \"https://github.com/mlcommons/croissant/tree/main/python/mlcroissant\",\n        docsUrl: \"https://huggingface.co/docs/dataset-viewer/mlcroissant\",\n    },\n    webdataset: {\n        prettyLabel: \"WebDataset\",\n        repoName: \"webdataset\",\n        repoUrl: \"https://github.com/webdataset/webdataset\",\n        docsUrl: \"https://huggingface.co/docs/hub/datasets-webdataset\",\n    },\n    datasets: {\n        prettyLabel: \"Datasets\",\n        repoName: \"datasets\",\n        repoUrl: \"https://github.com/huggingface/datasets\",\n        docsUrl: \"https://huggingface.co/docs/hub/datasets-usage\",\n    },\n    pandas: {\n        prettyLabel: \"pandas\",\n        repoName: \"pandas\",\n        repoUrl: \"https://github.com/pandas-dev/pandas\",\n        docsUrl: \"https://huggingface.co/docs/hub/datasets-pandas\",\n    },\n    dask: {\n        prettyLabel: \"Dask\",\n        repoName: \"dask\",\n        repoUrl: \"https://github.com/dask/dask\",\n        docsUrl: \"https://huggingface.co/docs/hub/datasets-dask\",\n    },\n    distilabel: {\n        prettyLabel: \"Distilabel\",\n        repoName: \"distilabel\",\n        repoUrl: \"https://github.com/argilla-io/distilabel\",\n        docsUrl: \"https://huggingface.co/docs/hub/datasets-distilabel\",\n    },\n    fiftyone: {\n        prettyLabel: \"FiftyOne\",\n        repoName: \"fiftyone\",\n        repoUrl: \"https://github.com/voxel51/fiftyone\",\n        docsUrl: \"https://huggingface.co/docs/hub/datasets-fiftyone\",\n    },\n    argilla: {\n        prettyLabel: \"Argilla\",\n        repoName: \"argilla\",\n        repoUrl: \"https://github.com/argilla-io/argilla\",\n        docsUrl: \"https://huggingface.co/docs/hub/datasets-argilla\",\n    },\n    polars: {\n        prettyLabel: \"Polars\",\n        repoName: \"polars\",\n        repoUrl: \"https://github.com/pola-rs/polars\",\n        docsUrl: \"https://huggingface.co/docs/hub/datasets-polars\",\n    },\n    duckdb: {\n        prettyLabel: \"DuckDB\",\n        repoName: \"duckdb\",\n        repoUrl: \"https://github.com/duckdb/duckdb\",\n        docsUrl: \"https://huggingface.co/docs/hub/datasets-duckdb\",\n    },\n};\n"],"names":[],"mappings":";;;;AAAO,MAAM,gCAAgC;IACzC,aAAa;QACT,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;IACb;IACA,YAAY;QACR,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;IACb;IACA,UAAU;QACN,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;IACb;IACA,QAAQ;QACJ,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;IACb;IACA,MAAM;QACF,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;IACb;IACA,YAAY;QACR,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;IACb;IACA,UAAU;QACN,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;IACb;IACA,SAAS;QACL,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;IACb;IACA,QAAQ;QACJ,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;IACb;IACA,QAAQ;QACJ,aAAa;QACb,UAAU;QACV,SAAS;QACT,SAAS;IACb;AACJ","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 11531, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/inference-providers.js"],"sourcesContent":["/// This list is for illustration purposes only.\n/// in the `tasks` sub-package, we do not need actual strong typing of the inference providers.\nconst INFERENCE_PROVIDERS = [\n    \"cerebras\",\n    \"cohere\",\n    \"fal-ai\",\n    \"fireworks-ai\",\n    \"hf-inference\",\n    \"hyperbolic\",\n    \"ovhcloud\",\n    \"replicate\",\n    \"sambanova\",\n    \"together\",\n];\nexport const HF_HUB_INFERENCE_PROXY_TEMPLATE = `https://router.huggingface.co/{{PROVIDER}}`;\n/**\n * URL to set as baseUrl in the OpenAI SDK.\n *\n * TODO(Expose this from InferenceClient in the future?)\n */\nexport function openAIbaseUrl(provider) {\n    const url = HF_HUB_INFERENCE_PROXY_TEMPLATE.replace(\"{{PROVIDER}}\", provider);\n    return provider === \"hf-inference\" ? `${url}/v1` : url;\n}\n"],"names":[],"mappings":"AAAA,gDAAgD;AAChD,+FAA+F;;;;;;;AAC/F,MAAM,sBAAsB;IACxB;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;CACH;AACM,MAAM,kCAAkC,CAAC,0CAA0C,CAAC;AAMpF,SAAS,cAAc,QAAQ;IAClC,MAAM,MAAM,gCAAgC,OAAO,CAAC,gBAAgB;IACpE,OAAO,aAAa,iBAAiB,GAAG,IAAI,GAAG,CAAC,GAAG;AACvD","ignoreList":[0],"debugId":null}},
    {"offset": {"line": 11560, "column": 0}, "map": {"version":3,"sources":["file:///Users/nikhil/Downloads/App/node_modules/%40huggingface/tasks/dist/esm/index.js"],"sourcesContent":["export { LIBRARY_TASK_MAPPING } from \"./library-to-tasks.js\";\nexport { MAPPING_DEFAULT_WIDGET } from \"./default-widget-inputs.js\";\nexport * from \"./tasks/index.js\";\nexport { PIPELINE_DATA, PIPELINE_TYPES, MODALITIES, MODALITY_LABELS, SUBTASK_TYPES, PIPELINE_TYPES_SET, } from \"./pipelines.js\";\nexport { ALL_DISPLAY_MODEL_LIBRARY_KEYS, ALL_MODEL_LIBRARY_KEYS, MODEL_LIBRARIES_UI_ELEMENTS, } from \"./model-libraries.js\";\nexport { SPECIAL_TOKENS_ATTRIBUTES } from \"./tokenizer-data.js\";\nexport * from \"./gguf.js\";\nexport { inferenceSnippetLanguages, stringifyGenerationConfig, stringifyMessages, getModelInputSnippet, } from \"./snippets/index.js\";\nexport { SKUS, DEFAULT_MEMORY_OPTIONS } from \"./hardware.js\";\nexport { LOCAL_APPS } from \"./local-apps.js\";\nexport { DATASET_LIBRARIES_UI_ELEMENTS } from \"./dataset-libraries.js\";\nexport * from \"./inference-providers.js\";\n"],"names":[],"mappings":";AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA","ignoreList":[0],"debugId":null}}]
}